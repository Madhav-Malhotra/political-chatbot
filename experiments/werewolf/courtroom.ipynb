{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf Simulator\n",
    "The following runs a simulation of the game Werewolf with multiple agents, it is built on top of and uses the Agentscope library, documentation can be found here https://doc.agentscope.io/, and the open source code that we extend on is here https://github.com/modelscope/agentscope/tree/main.\n",
    "### Rules of Werewolf\n",
    "- There are 4 roles, the Werewolves, Villagers, Witch, and Seer (Witch and Seer are on the Villager team)\n",
    "- Each night:\n",
    "    - The Werewolves discuss and vote on a player to eliminate\n",
    "    - The Witch is told what player the wolves voted to eliminate, and is given the choice to use their potion of healing to reserruct the eliminated player, or use their poison to eliminate another player. Note that each power can only be used once in a game\n",
    "    - The seer can pick any other player and find out what their role is (one player per night)\n",
    "- After the events of the night, all surviving players discuss amongst themselves and vote on a player to eliminate\n",
    "- Werewolves win if their numbers equal or exceed Villagers.\n",
    "- Villagers win if all Werewolves are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: agentscope==0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: faiss-cpu==1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: black in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: docstring-parser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.16)\n",
      "Requirement already satisfied: pydantic in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.10.6)\n",
      "Requirement already satisfied: loguru==0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: inputimeout in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: Flask==3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: Flask-Cors==4.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: Flask-SocketIO==5.3.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (5.3.6)\n",
      "Requirement already satisfied: flask-sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: flake8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (7.1.2)\n",
      "Requirement already satisfied: psutil in /Users/Evan/Library/Python/3.12/lib/python/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: openai>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.63.0)\n",
      "Requirement already satisfied: dashscope>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.22.1)\n",
      "Requirement already satisfied: packaging in /Users/Evan/Library/Python/3.12/lib/python/site-packages (from faiss-cpu==1.9.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: python-socketio>=5.0.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (5.12.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: websocket-client in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/Evan/Library/Python/3.12/lib/python/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (2.12.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.38)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken->agentscope==0.1.1->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: bidict>=0.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (4.11.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wsproto in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional, Union, Sequence, Any, List\n",
    "from functools import partial\n",
    "import openai  \n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import sys\n",
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "from agentscope.parsers.json_object_parser import MarkdownJsonDictParser\n",
    "from agentscope.parsers import ParserBase\n",
    "from agentscope.message import Msg\n",
    "from agentscope.msghub import msghub\n",
    "from agentscope.agents import AgentBase\n",
    "from agentscope.memory.temporary_memory import TemporaryMemory\n",
    "from agentscope.pipelines.functional import sequentialpipeline\n",
    "import agentscope\n",
    "\n",
    "from utils.werewolf_utils import (\n",
    "    extract_name_and_id,\n",
    "    n2s,\n",
    "    set_parsers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log configuration\n",
    "LOG_LEVEL = logging.INFO\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Initial logging setup\n",
    "logging.basicConfig(\n",
    "    filename=None,\n",
    "    level=LOG_LEVEL,\n",
    "    format=LOG_FORMAT,\n",
    "    datefmt=DATE_FORMAT\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to reset logging configuration when changing log files\n",
    "def change_log_file(new_filename):\n",
    "    # Remove all old handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    new_handler = logging.FileHandler(new_filename)\n",
    "    new_handler.setLevel(LOG_LEVEL)  # Reset level\n",
    "    new_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))  # Reset formatter\n",
    "\n",
    "    # Add new handler to logger\n",
    "    logger.addHandler(new_handler)\n",
    "\n",
    "def log_params(\n",
    "    raw_log_filepath: str,\n",
    "    win_rate_filepath: str,\n",
    "    max_days_per_game: int = 10,\n",
    "    num_games: int = 10,\n",
    "    reflect_before_vectorstore: bool = True,\n",
    "    max_werewolf_discussion_round: int = 3,\n",
    "    similarity_top_k: int = 1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Logs all the parameters passed to the function.\n",
    "    \n",
    "    Args:\n",
    "        raw_log_filepath (str): Path to the raw log file.\n",
    "        win_rate_filepath (str): Path to store win-rate results.\n",
    "        max_days_per_game (int): Maximum number of day/night cycles per game.\n",
    "        num_games (int): Number of games to simulate.\n",
    "        reflect_before_vectorstore (bool): Whether to use reflective memory before querying the vectorstore.\n",
    "        max_werewolf_discussion_round (int): Maximum rounds of discussion for werewolves.\n",
    "        similarity_top_k (int): Number of top similar messages to retrieve from memory.\n",
    "    \"\"\"\n",
    "    logger.info(\"Experiment Parameters:\")\n",
    "    logger.info(f\"  - raw_log_filepath: {raw_log_filepath}\")\n",
    "    logger.info(f\"  - win_rate_filepath: {win_rate_filepath}\")\n",
    "    logger.info(f\"  - max_days_per_game: {max_days_per_game}\")\n",
    "    logger.info(f\"  - num_games: {num_games}\")\n",
    "    logger.info(f\"  - reflect_before_vectorstore: {reflect_before_vectorstore}\")\n",
    "    logger.info(f\"  - max_werewolf_discussion_round: {max_werewolf_discussion_round}\")\n",
    "    logger.info(f\"  - similarity_top_k: {similarity_top_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementations\n",
    "Vector store classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        \n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            self.messages.append(msg.name + \": \" + msg.content)\n",
    "        \n",
    "    def summarize_cycle(self, survivors: list, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Combine all messages since the last summary\n",
    "        history = \"\\n\".join(self.messages)\n",
    "\n",
    "        # Use retry_with_fallback with a lambda function\n",
    "        slow_mind_response = retry_with_fallback(\n",
    "            lambda: client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"\\n\".join([ \n",
    "                        f\"You are a strategic decision-maker reviewing past decisions in Werewolf.\",\n",
    "                        f\"Survivors: {\", \".join([survivor.name for survivor in survivors])}\",\n",
    "                        f\"Summarize the decisions and reflect on the most important implications for future strategic decisions.\"\n",
    "                    ])},\n",
    "                    {\"role\": \"user\", \"content\": history},\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summary = slow_mind_response.choices[0].message.content\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        # Clear messages for the next cycle (current implementation gets summary from the point after the last summary was created)\n",
    "        self.messages.clear()\n",
    "\n",
    "    def get_relevant_summaries_context(self, query: str, top_k: int = 1):\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        if len(self.summaries) > 0:\n",
    "            query_embedding = self._get_embedding(query)\n",
    "            distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "            results = [\n",
    "                self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "            ]\n",
    "            return \"\\n\".join(results)\n",
    "        return \"\"\n",
    "\n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        # add embedding of message to vector store\n",
    "        embeddings = [self._get_embedding(msg.name + \": \" + msg.content) for msg in message]\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.messages.extend(message)\n",
    "\n",
    "    def get_relevant_messages(self, query: str, top_k: int = 10):\n",
    "        \"\"\"Retrieve the top-k most relevant messages.\"\"\"\n",
    "        if not self.messages:\n",
    "            return []\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "\n",
    "        results = [\n",
    "            self.messages[idx] for idx in indices[0] if idx < len(self.messages)\n",
    "        ]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agents (edit here)\n",
    "You can define custom Agents by inheriting from the AgentBase class like shown here.\n",
    "\n",
    "Below is a copy of the DictDialogAgent that generates responses in a dict format that is compatible with our simulator. More documentation on AgentScope agents can be found here https://doc.agentscope.io/build_tutorial/builtin_agent.html, existing agent implementations can be found here https://github.com/modelscope/agentscope/tree/main/src/agentscope/agents.\n",
    "\n",
    "For our case, modifying the agent class is, in conjunction with the parsers we pass in (more details in next cell), play a critical role in defining our agent behavior. We can edit this section by either defining a brand new agent type we want to explore, or modifying the current one. The primary (and mostly only) source of focus should be the reply the function in the agent class as that controls what an agent uses to generate a response, particular when we are working with different architectures, eg. how we use memory, single vs multi agent, we make those edits here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lawyer(AgentBase):\n",
    "    \"\"\"An agent specializing in constructing LLM-based arguments to eliminate a specific target in the Werewolf game.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Lawyer agent.\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "                Name of the Lawyer agent.\n",
    "\n",
    "            sys_prompt (str):\n",
    "                System prompt containing role instructions and context.\n",
    "\n",
    "            model_config_name (str):\n",
    "                Identifier for the model configuration that determines the LLM to be used.\n",
    "\n",
    "            parser (ParserBase):\n",
    "                Object responsible for formatting LLM outputs and extracting structured responses.\n",
    "\n",
    "            context_messages (list):\n",
    "                A list of prior messages providing context for the lawyer’s argumentation.\n",
    "\n",
    "            memory:\n",
    "                Memory management component used to store conversation history or additional data.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "\n",
    "    def argue(self, available_targets: list) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a rationale for eliminating a particular player in the Werewolf game.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict):\n",
    "                The current state of the game (e.g., who is alive, who is dead).\n",
    "\n",
    "            available_targets (list):\n",
    "                List of potential targets that can be selected for elimination.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                A dictionary containing:\n",
    "                  - \"target\" (str): The chosen player to eliminate.\n",
    "                  - \"argument\" (str): The Lawyer’s supporting reasoning.\n",
    "        \"\"\"\n",
    "        if not available_targets:\n",
    "            return {\"target\": None, \"argument\": \"No valid targets available.\"}\n",
    "        \n",
    "        target = random.choice(available_targets)\n",
    "\n",
    "        # Construct prompt for argument generation\n",
    "        lawyer_prompt = f\"\"\"\n",
    "        You are a lawyer arguing FOR the decision to eliminate {target}.\n",
    "        Build your argument FOR why eliminating this player is a strategic decision to maximize winning chances.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", lawyer_prompt, role=\"system\"),\n",
    "            self.context_messages,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call the LLM to generate an argument\n",
    "        response = retry_with_fallback(lambda: self.model(formatted_prompt))\n",
    "\n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"argument\": load_json_response(response.text)  # Return generated argument\n",
    "        }\n",
    "\n",
    "\n",
    "class Judge(AgentBase):\n",
    "    \"\"\"An agent that evaluates competing arguments and selects who should be eliminated in the Werewolf game.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Judge agent.\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "                Name of the Judge agent.\n",
    "\n",
    "            sys_prompt (str):\n",
    "                System prompt containing role instructions and context.\n",
    "\n",
    "            model_config_name (str):\n",
    "                Identifier for the model configuration that determines the LLM to be used.\n",
    "\n",
    "            parser (ParserBase):\n",
    "                Object responsible for formatting LLM outputs and extracting structured responses.\n",
    "\n",
    "            context_messages (list):\n",
    "                List of prior messages providing context for the Judge’s decision-making.\n",
    "\n",
    "            memory:\n",
    "                Memory component used to store and retrieve conversation or historical data.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "\n",
    "    def decide(self, game_state: dict, argument_1: dict, argument_2: dict):\n",
    "        \"\"\"\n",
    "        Evaluate two lawyers' arguments and choose a target for elimination.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict):\n",
    "                The current state of the Werewolf game (e.g., which players are alive or dead).\n",
    "\n",
    "            argument_1 (dict):\n",
    "                First lawyer’s argument dictionary with:\n",
    "                  - \"target\" (str): Proposed player to eliminate.\n",
    "                  - \"argument\" (str): Reasoning for that choice.\n",
    "\n",
    "            argument_2 (dict):\n",
    "                Second lawyer’s argument dictionary with the same keys as argument_1.\n",
    "\n",
    "        Returns:\n",
    "            The raw response from the model, containing the Judge’s final decision.\n",
    "        \"\"\"\n",
    "        argument_1_target = argument_1[\"target\"]\n",
    "        argument_1_argument = argument_1[\"argument\"]\n",
    "        \n",
    "        argument_2_target = argument_2[\"target\"]\n",
    "        argument_2_argument = argument_2[\"argument\"]\n",
    "\n",
    "        # Construct prompt for decision-making\n",
    "        judge_prompt = f\"\"\"\n",
    "        You are a judge deciding which player should be eliminated in a game of werewolf. Your goal is to maximize the player's chances of winning the game with forward-thinking reasoning.\n",
    "        \n",
    "        Game State:\n",
    "        - Alive players: {game_state[\"survivors\"]}\n",
    "        - Dead players: {game_state[\"dead\"]}\n",
    "        - Werewolves: {game_state[\"werewolves\"]}\n",
    "\n",
    "        The two lawyers have made their arguments:\n",
    "        - **Lawyer 1** wants to eliminate {argument_1_target}: \"{argument_1_argument}\"\n",
    "        - **Lawyer 2** wants to eliminate {argument_2_target}: \"{argument_2_argument}\"\n",
    "\n",
    "        Make your final decision: Which player should be eliminated between the 2? Justify your reasoning briefly.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", judge_prompt, role=\"system\"),\n",
    "            self.context_messages,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        raw_response = retry_with_fallback(lambda: self.model(formatted_prompt))\n",
    "\n",
    "        return raw_response\n",
    "\n",
    "\n",
    "class WerewolfAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    Represents a \"werewolf\" in the game, producing responses in dict form.\n",
    "\n",
    "    Optionally uses reflective memory before retrieving context from a FAISS vectorstore.\n",
    "    Contains parsing capabilities to ensure structured outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the WerewolfAgent.\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "                Name of the agent.\n",
    "\n",
    "            sys_prompt (str):\n",
    "                System prompt providing role context and instructions.\n",
    "\n",
    "            model_config_name (str):\n",
    "                Name of the model configuration, indicating which LLM will be used.\n",
    "\n",
    "            reflect_before_vectorstore (bool):\n",
    "                Whether to summarize or reflect on memory before retrieving vectorstore data.\n",
    "\n",
    "            similarity_top_k (int, optional):\n",
    "                The number of most similar messages/summaries to retrieve from the vectorstore.\n",
    "\n",
    "            openai_api_key (str, optional):\n",
    "                API key for OpenAI integration.\n",
    "\n",
    "            model:\n",
    "                The underlying LLM for generating responses.\n",
    "\n",
    "            parser (ParserBase):\n",
    "                Parser responsible for structuring, filtering, and formatting model outputs.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.model_config_name = model_config_name\n",
    "\n",
    "        # Set OpenAI API key\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        # Initialize FAISS-based memory store\n",
    "        if reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"\n",
    "        Set the parser for handling model outputs.\n",
    "\n",
    "        This parser governs how responses are formatted, parsed,\n",
    "        and stored (including any filtering of fields).\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Generate a response based on input messages, optionally retrieving context from memory.\n",
    "\n",
    "        Args:\n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional):\n",
    "                The input message(s) to process.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                The assistant's structured output, including any metadata defined by the parser.\n",
    "        \"\"\" \n",
    "        query = Queries.werewolf_discussion_query\n",
    "\n",
    "        # Retrieve relevant messages from memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            summary_context = self.memory.get_relevant_summaries_context(query=query, top_k=self.similarity_top_k)\n",
    "            logger.info(f\"Relevant messages from vectorstore: {summary_context}\")\n",
    "\n",
    "            # Prepare prompt with context from retrieved summaries\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\") or x,\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "        else:\n",
    "            assert isinstance(query, str), f\"Expected a string, but got {type(query)}\"\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "            logger.info(f\"Relevant messages from vectorstore: {convert_messages_to_string(relevant_messages)}\")\n",
    "\n",
    "            # Prepare prompt with retrieved messages similar to input message\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_messages or x,\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "\n",
    "        # Call the LLM\n",
    "        raw_response = retry_with_fallback(lambda: self.model(prompt))\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parse the raw response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg_no_metadata = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=msg_no_metadata)\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def lawyer_judge_decision(self, game_state: dict, available_targets: list, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Make an elimination decision by invoking two Lawyers and one Judge.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict):\n",
    "                The current state of the Werewolf game (e.g., active players, werewolves).\n",
    "\n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional):\n",
    "                Optionally, messages to be recorded in memory before decision-making.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                The final decision from the Judge as a structured message.\n",
    "        \"\"\"\n",
    "        query = Queries.werewolf_discussion_query\n",
    "\n",
    "        # Retrieve relevant messages from memory (summaries or direct messages)\n",
    "        if self.reflect_before_vectorstore:\n",
    "            summary_context = self.memory.get_relevant_summaries_context(query=query, top_k=self.similarity_top_k)\n",
    "            \n",
    "            relevant_messages = [Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")]\n",
    "            logger.info(f\"Relevant messages from vectorstore: {summary_context}\")\n",
    "        else:\n",
    "            assert isinstance(query, str), f\"Expected a string, but got {type(query)}\"\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k) or [x]\n",
    "\n",
    "            logger.info(f\"Relevant messages from vectorstore: {convert_messages_to_string(relevant_messages)}\")\n",
    "            \n",
    "        # Initialize the lawyers and judge\n",
    "        lawyer_1 = Lawyer(\n",
    "            f'{self.name} (Lawyer 1)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,              \n",
    "            self.memory\n",
    "        )\n",
    "\n",
    "        lawyer_2 = Lawyer(\n",
    "            f'{self.name} (Lawyer 2)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,\n",
    "            self.memory\n",
    "        )\n",
    "\n",
    "        judge = Judge(\n",
    "            f'{self.name} (Judge)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,\n",
    "            self.memory\n",
    "        )\n",
    "    \n",
    "        lawyer_1_argument = lawyer_1.argue(available_targets)\n",
    "        available_targets.remove(lawyer_1_argument[\"target\"])\n",
    "        lawyer_2_argument = lawyer_2.argue(available_targets)\n",
    "\n",
    "        raw_response = judge.decide(game_state, lawyer_1_argument, lawyer_2_argument)\n",
    "        logger.info(f\"Lawyer 1 argument: {lawyer_1_argument}\")\n",
    "        logger.info(f\"Lawyer 2 argument: {lawyer_2_argument}\")\n",
    "        logger.info(f\"Judge decision: {load_json_response(raw_response.text)}\")\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parse response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=\"\\n\".join([\n",
    "                f\"Argument 1: {lawyer_1_argument}\",\n",
    "                f\"Argument 2: {lawyer_2_argument}\",\n",
    "                f\"Decision: {res.parsed}\"\n",
    "            ]),\n",
    "            role=\"assistant\"\n",
    "        )\n",
    "\n",
    "        # Store final decision\n",
    "        self.memory.add_message(message=vectorstore_entry)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"\n",
    "        Record incoming messages in memory without generating a reply.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]):\n",
    "                The message(s) to be stored in memory for future context.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "    def summarize_cycle(self, survivors: list, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Produce a summary of key events at the end of a day or night cycle.\n",
    "\n",
    "        Args:\n",
    "            cycle_type (str, optional):\n",
    "                The type of cycle being summarized, either \"day\" or \"night\".\n",
    "        \"\"\"\n",
    "        self.memory.summarize_cycle(survivors, cycle_type=cycle_type)\n",
    "\n",
    "\n",
    "class NormalAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    A general-purpose agent that returns structured responses in JSON/dict format.\n",
    "\n",
    "    Integrates with a FAISS vectorstore for context retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the NormalAgent.\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "                Agent name.\n",
    "\n",
    "            sys_prompt (str):\n",
    "                The system prompt containing the agent’s role instructions.\n",
    "\n",
    "            model_config_name (str):\n",
    "                Indicates which model configuration the agent should use.\n",
    "\n",
    "            reflect_before_vectorstore (bool):\n",
    "                Flag to determine if reflective memory is used before vectorstore retrieval.\n",
    "\n",
    "            similarity_top_k (int, optional):\n",
    "                The number of top similar items from the vectorstore to include as context.\n",
    "\n",
    "            openai_api_key (str, optional):\n",
    "                API key for integrating with OpenAI services.\n",
    "\n",
    "            model:\n",
    "                The LLM used to generate responses.\n",
    "\n",
    "            parser (ParserBase):\n",
    "                Responsible for formatting and parsing outputs, as well as filtering fields.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        openai.api_key = openai_api_key\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"\n",
    "        Configure the parser that dictates how outputs are formatted, parsed, and stored.\n",
    "\n",
    "        By adjusting the parser, you can control the entire pipeline of formatting and extraction\n",
    "        for the model's responses.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Generate a response based on provided input messages, using stored context as needed.\n",
    "\n",
    "        Args:\n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional):\n",
    "                The incoming user or system messages.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                A structured message including the assistant's content and any parsed metadata.\n",
    "        \"\"\"\n",
    "        query = Queries.non_werewolf_discussion_query\n",
    "\n",
    "        # Retrieve similar messages from memory\n",
    "        relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "        logger.info(f\"Relevant messages from vectorstore: {convert_messages_to_string(relevant_messages)}\")\n",
    "\n",
    "        # Prepare the prompt with relevant context\n",
    "        prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            relevant_messages or x,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call the LLM\n",
    "        raw_response = retry_with_fallback(lambda: self.model(prompt))\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parse the model's output\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg_no_metadata = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\"\n",
    "        )\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "        \n",
    "        # Save the model's response to memory\n",
    "        self.memory.add_message(message=msg_no_metadata)\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"\n",
    "        Record messages in memory without generating a response.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]):\n",
    "                One or more messages to store for future reference.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "\n",
    "# Register your custom classes\n",
    "agentscope.agents.NormalAgent = NormalAgent\n",
    "agentscope.agents.WerewolfAgent = WerewolfAgent\n",
    "agentscope.agents.Lawyer = Lawyer\n",
    "agentscope.agents.Judge = Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Parser (edit minimally)\n",
    "Below are the prompts that control and run the Werwolf game. They are implemented using the built in AgentScope MarkdownJsonDictParser which allows us to generate responses in a dictionary format that is compatible with our game. Currently structured to obtain both the private thoughts of an Agent which remain to themselves and showcase their reasoning, and the words that they speak to other agents.\n",
    "\n",
    "Since these prompts control the game logic itself, we should try to limit the prompt edits here to a minimum, but we can make edits to the parser to control the reasoning process a particular role goes through before making a decision, eg. instead of a thought field, we ask it to make ask it to provide a reason to vote for a particular agent, and a reason to not vote for a particular agent, then ask it to make a final decision based off those fields. Fields other than \"thought\" however should not be touched as they play a critical role to the control flow of the game.\n",
    "\n",
    "More details on the AgentScope parsers can be found here https://doc.agentscope.io/build_tutorial/structured_output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    \"\"\"Prompts for werewolf game\"\"\"\n",
    "\n",
    "    rules = \"\"\"\n",
    "        Player roles: In werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\n",
    "        - Werewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\n",
    "        - Villagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\n",
    "        - Seer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\n",
    "        - Witch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\n",
    "\n",
    "        Game Rule: The game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\n",
    "        1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\n",
    "        2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\n",
    "\n",
    "        Victory Condition: For werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers. For villagers, they win if they identify and eliminate all of the werewolves in the group.\n",
    "\n",
    "        Constraints:\n",
    "        1. Your response should be in the first person.\n",
    "        2. This is a conversational game. You should response only based on the conversation history and your strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    two_phases = \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    to_wolves = (\n",
    "        \"{}, if you are the only werewolf, eliminate a player. Otherwise, \"\n",
    "        \"discuss with your teammates and reach an agreement.\"\n",
    "    )\n",
    "\n",
    "    wolves_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"finish_discussion\": \"whether the discussion reached an agreement or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"finish_discussion\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"finish_discussion\"],\n",
    "    )\n",
    "\n",
    "    to_wolves_vote = \"Which player do you vote to kill?\"\n",
    "\n",
    "    wolves_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_wolves_res = \"The player with the most votes is {}.\"\n",
    "\n",
    "    to_witch_resurrect = (\n",
    "        \"{witch_name}, you're the witch. Tonight {dead_name} is eliminated. \"\n",
    "        \"Would you like to resurrect {dead_name}?\"\n",
    "    )\n",
    "\n",
    "    to_witch_resurrect_no = \"The witch has chosen not to resurrect the player.\"\n",
    "    to_witch_resurrect_yes = \"The witch has chosen to resurrect the player.\"\n",
    "\n",
    "    witch_resurrect_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"whether to resurrect the player and the reason\",\n",
    "            \"resurrect\": \"whether to resurrect the player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"resurrect\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"resurrect\"],\n",
    "    )\n",
    "\n",
    "    to_witch_poison = (\n",
    "        \"Would you like to eliminate one player? If yes, \"\n",
    "        \"specify the player_name.\"\n",
    "    )\n",
    "\n",
    "    witch_poison_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"eliminate\": \"whether to eliminate a player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"eliminate\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"eliminate\"],\n",
    "    )\n",
    "\n",
    "    to_seer = (\n",
    "        \"{}, you're the seer. Which player in {} would you like to check \"\n",
    "        \"tonight?\"\n",
    "    )\n",
    "\n",
    "    seer_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    to_seer_result = \"Okay, the role of {} is a {}.\"\n",
    "\n",
    "    to_all_danger = (\n",
    "        \"The day is coming, all the players open your eyes. Last night, \"\n",
    "        \"the following player(s) has been eliminated: {}.\"\n",
    "    )\n",
    "\n",
    "    to_all_peace = (\n",
    "        \"The day is coming, all the players open your eyes. Last night is \"\n",
    "        \"peaceful, no player is eliminated.\"\n",
    "    )\n",
    "\n",
    "    to_all_discuss = (\n",
    "        \"Now the alive players are {}. Given the game rules and your role, \"\n",
    "        \"based on the situation and the information you gain, to vote a \"\n",
    "        \"player eliminated among alive players and to win the game, what do \"\n",
    "        \"you want to say to others? You can decide whether to reveal your \"\n",
    "        \"role.\"\n",
    "    )\n",
    "\n",
    "    survivors_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    survivors_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_all_vote = (\n",
    "        \"Given the game rules and your role, based on the situation and the\"\n",
    "        \" information you gain, to win the game, it's time to vote one player\"\n",
    "        \" eliminated among the alive players. Which player do you vote to \"\n",
    "        \"kill?\"\n",
    "    )\n",
    "\n",
    "    to_all_res = \"{} has been voted out.\"\n",
    "\n",
    "    to_all_wolf_win = (\n",
    "        \"The werewolves have prevailed and taken over the village. Better \"\n",
    "        \"luck next time!\"\n",
    "    )\n",
    "\n",
    "    to_all_village_win = (\n",
    "        \"The game is over. The werewolves have been defeated, and the village \"\n",
    "        \"is safe once again!\"\n",
    "    )\n",
    "\n",
    "    to_all_continue = \"The game goes on.\"\n",
    "\n",
    "    \n",
    "# Moderator message function\n",
    "HostMsg = partial(Msg, name=\"Moderator\", role=\"assistant\", echo=True)\n",
    "QueryMsg = partial(Msg, name=\"Query\", role=\"user\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries:\n",
    "    \"\"\"Queries for the vectorstore based on certain points in game\"\"\"\n",
    "\n",
    "    # Retrieves discussions about suspicions of non-werewolves to help identify potential enemies.\n",
    "    werewolf_discussion_query = \"Retrieve the most relevant discussions related to suspicions of non-werewolves.\"\n",
    "\n",
    "    # Retrieves discussions focused on identifying werewolves.\n",
    "    non_werewolf_discussion_query = \"Retrieve the most relevant discussions related to identifying werewolves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Initialization (edit here)\n",
    "To initialize the agents, you must define their parameters and settings in the config objects that are passed in for initialization. There is a model config, which defines the base foundational model being used, and an agent config, which defines each of the agents being used in the game, including which model their using, their name, and what type of Agent they are (based off the agent classes we defined earlier). \n",
    "\n",
    "Pay particular attention to the system prompt, this is what defines the rules of the game to the agent and gives them the role and what their responsibilities are, we could perhaps do some prompt engineering with that.\n",
    "\n",
    "Also we can play around with the settings of the game, eg. max rounds, how many werewolves we have, etc. Just make sure to update the roles, witch, seer objects below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (don't edit)\n",
    "Functions to check and update game state throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_fallback(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Tries to execute the given function. If it fails, waits 5 seconds and retries once.\n",
    "    If it fails again, exits the script.\n",
    "\n",
    "    Args:\n",
    "        func (Callable[..., Any]): The function to execute.\n",
    "        *args (Any): Positional arguments to pass to the function.\n",
    "        **kwargs (Any): Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        Any: The result of the function if successful.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    return func(*args, **kwargs)  # Try executing the function\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error: {e}. Retrying in 5 seconds...\")\n",
    "    #     time.sleep(5)  # Wait 5 seconds before retrying\n",
    "\n",
    "    #     try:\n",
    "    #         return func(*args, **kwargs)  # Try again\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error: {e}. Failed again. Stopping execution.\")\n",
    "    #         sys.exit(1)  # Exit the script if it fails twice\n",
    "\n",
    "\n",
    "def log_separator():\n",
    "    logger.info(\"===\")\n",
    "\n",
    "def load_json_response(response):\n",
    "    return json.loads(response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "\n",
    "def convert_messages_to_string(relevant_messages: list) -> str:\n",
    "    return '\\n'.join([msg.content for msg in relevant_messages])\n",
    "\n",
    "def majority_vote(votes: list) -> Any:\n",
    "    \"\"\"Given a list of votes, return the name with the highest frequency.\"\"\"\n",
    "    votes_valid = [v for v in votes if v != \"Abstain\"]\n",
    "    if not votes_valid:\n",
    "        return \"No Votes\"\n",
    "    unique_vals, counts = np.unique(votes_valid, return_counts=True)\n",
    "    return unique_vals[np.argmax(counts)]\n",
    "\n",
    "def update_alive_players(game_state: dict, survivors: list, wolves: list, dead_names):\n",
    "    \"\"\"\n",
    "    Removes 'dead_names' from 'game_state[\"survivors\"]', updates game_state[\"dead\"],\n",
    "    and returns updated survivors and wolves lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(dead_names, list):\n",
    "        dead_names = [dead_names]\n",
    "    for d in dead_names:\n",
    "        if d in game_state[\"survivors\"]:\n",
    "            game_state[\"survivors\"].remove(d)\n",
    "        if d not in game_state[\"dead\"]:\n",
    "            game_state[\"dead\"].append(d)\n",
    "\n",
    "    # Rebuild survivors / wolves lists\n",
    "    new_survivors = [s for s in survivors if s.name in game_state[\"survivors\"]]\n",
    "    new_wolves = [w for w in wolves if w.name in game_state[\"survivors\"]]\n",
    "    return new_survivors, new_wolves\n",
    "\n",
    "def check_winning(game_state: dict, survivors: list, wolves: list, host: str) -> bool:\n",
    "    \"\"\"\n",
    "    If #werewolves * 2 >= #survivors => werewolves instantly win.\n",
    "    If all werewolves are dead => villagers instantly win.\n",
    "    Otherwise => game continues.\n",
    "    \"\"\"\n",
    "    if len(wolves) * 2 >= len(survivors):\n",
    "        msg = Msg(host, Prompts.to_all_wolf_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"werewolves\"\n",
    "        logger.info(f\"Moderator: {Prompts.to_all_wolf_win}\")\n",
    "        log_separator()\n",
    "        return True\n",
    "    if survivors and not wolves:\n",
    "        msg = Msg(host, Prompts.to_all_village_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"villagers\"\n",
    "        logger.info(f\"Moderator: {Prompts.to_all_village_win}\")\n",
    "        log_separator()\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate_log_filepath(basepath: str, game_num: int) -> str:\n",
    "    \"\"\"Generate log file path with EST timestamp.\"\"\"\n",
    "    est_now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
    "    iso_timestamp = est_now.isoformat(timespec='seconds').replace(\":\", \"-\")\n",
    "    return f\"{basepath}_{game_num}_{iso_timestamp}.log\"\n",
    "\n",
    "def save_results(win_rate_filepath: str, row: dict):\n",
    "    \"\"\"\n",
    "    Append a row to a CSV file. If doesn't exist, create with headers.\n",
    "\n",
    "    row = {\n",
    "      \"raw_log_filepath\": <str>,\n",
    "      \"custom_agent_won\": <bool>,\n",
    "    }\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(win_rate_filepath)\n",
    "    with open(win_rate_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"raw_log_filepath\", \"custom_agent_won\"])\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "    print(f\"Row added to {win_rate_filepath}: {row}\")\n",
    "\n",
    "def custom_agent_won(game_state: dict, alive_agents: list, wolf_agents: list):\n",
    "    \"\"\"Records whether custom agent won (even if game didn't complete)\"\"\"\n",
    "\n",
    "    # If game is completed, check if werewolves have won\n",
    "    if game_state[\"endgame\"]:\n",
    "        if game_state[\"winner\"] == \"werewolves\":\n",
    "            return True \n",
    "        return False\n",
    "    \n",
    "    # If game not completed, then custom agent wins because it is still alive and eliminated another player\n",
    "    return True\n",
    "\n",
    "def get_available_targets(survivors: list, wolves: list):\n",
    "    return [survivor.name for survivor in survivors if survivor not in wolves]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Game (don't edit)\n",
    "Following is a function that game through the various Night and Day phases, taking different actions for each agent based on their roles. Multi agent functionality and communication is facilitated through the AgentScope Pipeline and MsgHub, more detailed documentation found here https://doc.agentscope.io/build_api/agentscope.pipelines.pipeline.html#module-agentscope.pipelines.pipeline and https://doc.agentscope.io/build_api/agentscope.msghub.html#module-agentscope.msghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(\n",
    "        game_num,\n",
    "        max_days_per_game,\n",
    "        raw_log_filepath,\n",
    "        win_rate_filepath,\n",
    "        reflect_before_vectorstore,\n",
    "        max_werewolf_discussion_round,\n",
    "        wolves,\n",
    "        seer,\n",
    "        witch,\n",
    "        roles,\n",
    "        survivors,\n",
    "        game_state\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs a single game, storing logs in game_log. This example merges both\n",
    "    day/night flow, werewolf talk, witch usage, seer usage, etc. Then\n",
    "    saves logs and results.\n",
    "    \"\"\"\n",
    "\n",
    "    for day_i in range(1, max_days_per_game + 1):\n",
    "\n",
    "        # 1) Night Phase: Werewolves discussion\n",
    "        log_separator()\n",
    "        hint = HostMsg(content=Prompts.to_wolves.format(n2s(wolves)))\n",
    "        logger.info(f\"Moderator: {hint.content}\")\n",
    "        with msghub(wolves, announcement=hint) as hub:\n",
    "            set_parsers(wolves, Prompts.wolves_discuss_parser)\n",
    "            for r in range(max_werewolf_discussion_round):\n",
    "                x = retry_with_fallback(lambda: sequentialpipeline(wolves))\n",
    "                logger.info(f\"Werewolves discussion: {x.content}\")\n",
    "                if x.metadata.get(\"finish_discussion\", False):\n",
    "                    break\n",
    "            # Then vote\n",
    "            set_parsers(wolves, Prompts.wolves_vote_parser)\n",
    "            hint_vote = HostMsg(content=Prompts.to_wolves_vote)\n",
    "            logger.info(f\"Moderator: {hint_vote.content}\")\n",
    "            votes = [extract_name_and_id(wolf.lawyer_judge_decision(game_state, get_available_targets(survivors, wolves), hint_vote).content)[0] for wolf in wolves]\n",
    "\n",
    "            voted_out = majority_vote(votes)\n",
    "            dead_player = [voted_out]\n",
    "            logger.info(Prompts.to_wolves_res.format(voted_out))\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_wolves_res.format(voted_out)))\n",
    "\n",
    "        # 2) Witch Decision Night   \n",
    "        log_separator()\n",
    "        healing_used_tonight = False\n",
    "        if witch in survivors:\n",
    "            if not game_state[\"witch_healing_used\"]:\n",
    "                hint = HostMsg(\n",
    "                    content=Prompts.to_witch_resurrect.format_map(\n",
    "                        {\n",
    "                            \"witch_name\": witch.name,\n",
    "                            \"dead_name\": dead_player[0],\n",
    "                        },\n",
    "                    ),\n",
    "                )\n",
    "                logger.info(f\"Moderator: {hint.content}\")\n",
    "                set_parsers(witch, Prompts.witch_resurrect_parser)\n",
    "                \n",
    "                # Capture the witch's resurrection response and log it\n",
    "                resurrection_response = retry_with_fallback(lambda: witch(hint))\n",
    "                logger.info(f\"Witch resurrection response: {resurrection_response.content}\")\n",
    "\n",
    "                if resurrection_response.metadata.get(\"resurrect\", False):\n",
    "                    healing_used_tonight = True\n",
    "                    dead_player.pop()\n",
    "                    game_state[\"witch_healing_used\"] = True\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_yes)\n",
    "                    logger.info(f\"Moderator: {Prompts.to_witch_resurrect_yes}\")\n",
    "                else:\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_no)\n",
    "                    logger.info(f\"Moderator: {Prompts.to_witch_resurrect_no}\")\n",
    "\n",
    "            if not game_state[\"witch_poison_used\"] and not healing_used_tonight:\n",
    "                set_parsers(witch, Prompts.witch_poison_parser)\n",
    "                \n",
    "                # Capture the witch's poison response and log it\n",
    "                poison_response = retry_with_fallback(lambda: witch(HostMsg(content=Prompts.to_witch_poison)))\n",
    "                logger.info(f\"Witch poison response: {poison_response.content}\")\n",
    "\n",
    "                if poison_response.metadata.get(\"eliminate\", False):\n",
    "                    target_player = extract_name_and_id(poison_response.content)[0]\n",
    "                    dead_player.append(target_player)\n",
    "                    game_state[\"witch_poison_used\"] = True\n",
    "                    logger.info(f\"Moderator: The witch has chosen to poison {target_player}.\")\n",
    "\n",
    "        # 3) Seer checks a role\n",
    "        log_separator()\n",
    "        if seer in survivors:\n",
    "            seer_hint = HostMsg(content=Prompts.to_seer.format(seer.name, n2s(survivors)))\n",
    "            logger.info(f\"Moderator: {seer_hint.content}\")\n",
    "            set_parsers(seer, Prompts.seer_parser)\n",
    "\n",
    "            x = seer.reply(seer_hint) # Use seer hint as query\n",
    "            logger.info(f\"Seer response: {x.content}\")\n",
    "            pl, idx = extract_name_and_id(x.content)\n",
    "            role_name = roles[idx] if idx < len(roles) else \"villager\"\n",
    "            # Let seer quietly observe the result\n",
    "            role_msg = HostMsg(content=Prompts.to_seer_result.format(pl, role_name))\n",
    "            logger.info(f\"Moderator: {role_msg.content}\")\n",
    "            seer.observe(role_msg)\n",
    "\n",
    "        # 4) Update survivors after night\n",
    "        survivors, wolves = update_alive_players(game_state, survivors, wolves, dead_player)\n",
    "        if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "            break\n",
    "\n",
    "        # If reflecting, do a night reflection\n",
    "        if reflect_before_vectorstore:\n",
    "            for w in wolves:\n",
    "                w.summarize_cycle(survivors)\n",
    "\n",
    "        # 5) Daytime discussion\n",
    "        log_separator()\n",
    "        content = (\n",
    "            Prompts.to_all_danger.format(n2s(dead_player))\n",
    "            if dead_player\n",
    "            else Prompts.to_all_peace\n",
    "        )\n",
    "        discuss_hints = [\n",
    "            HostMsg(content=content),\n",
    "            HostMsg(content=Prompts.to_all_discuss.format(n2s(survivors))),\n",
    "        ]\n",
    "        with msghub(survivors, announcement=discuss_hints) as hub:\n",
    "            set_parsers(survivors, Prompts.survivors_discuss_parser)\n",
    "            discussion_out = retry_with_fallback(lambda: sequentialpipeline(survivors))\n",
    "            logger.info(f\"Survivors discussion: {discussion_out.content}\")\n",
    "\n",
    "            # Daytime vote\n",
    "            set_parsers(survivors, Prompts.survivors_vote_parser)\n",
    "            day_vote_hint = HostMsg(content=Prompts.to_all_vote)\n",
    "            logger.info(f\"Moderator: {day_vote_hint.content}\")\n",
    "            \n",
    "            votes_day = [\n",
    "                extract_name_and_id(wolf.lawyer_judge_decision(game_state, get_available_targets(survivors, wolves), day_vote_hint).content)[0] for wolf in wolves\n",
    "            ]\n",
    "\n",
    "            votes_day.extend([\n",
    "                extract_name_and_id(_.reply(day_vote_hint).content)[0] for _ in survivors if _ not in wolves\n",
    "            ])\n",
    "            day_result = majority_vote(votes_day)\n",
    "            logger.info(f\"Daytime vote: {day_result}\")\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_res.format(day_result)))\n",
    "\n",
    "            survivors, wolves = update_alive_players(game_state, survivors, wolves, day_result)\n",
    "            if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "                break\n",
    "\n",
    "            # If reflecting, do a day reflection\n",
    "            if reflect_before_vectorstore:\n",
    "                for w in wolves:\n",
    "                    w.summarize_cycle(survivors)\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_continue))\n",
    "    \n",
    "    return custom_agent_won(\n",
    "            game_state,\n",
    "            survivors,\n",
    "            wolves\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    open_ai_key: str,\n",
    "    raw_log_filepath: str,\n",
    "    win_rate_filepath: str,\n",
    "    max_days_per_game: int = 10,\n",
    "    num_games: int = 10,\n",
    "    reflect_before_vectorstore: bool = True,\n",
    "    primary_model: str = \"gpt-4o-mini\",\n",
    "    max_werewolf_discussion_round: int = 3,\n",
    "    similarity_top_k: int = 1\n",
    ") -> None:  \n",
    "    \"\"\"\n",
    "    Initialize and run multiple games of the Werewolf simulation.\n",
    "\n",
    "    Arguments:\n",
    "        open_ai_key (`str`):\n",
    "            The OpenAI API key used for model calls.\n",
    "        raw_log_filepath (`str`):\n",
    "            Base path for logs, e.g., \"logs/game_raw\".\n",
    "        win_rate_filepath (`str`):\n",
    "            CSV file to store results, e.g., \"results.csv\".\n",
    "        max_days_per_game (`int`, defaults to `3`):\n",
    "            Maximum number of day/night cycles per game.\n",
    "        num_games (`int`, defaults to `2`):\n",
    "            Number of games to simulate.\n",
    "        reflect_before_vectorstore (`bool`, defaults to `False`):\n",
    "            Whether to use reflective memory before vectorstore retrieval.\n",
    "        primary_model (`str`, defaults to `\"gpt-4o-mini\"`):\n",
    "            Name of the model used for fast processing.\n",
    "        max_werewolf_discussion_round (`int`, defaults to `3`):\n",
    "            Maximum number of discussion rounds for werewolves.\n",
    "        similarity_top_k (`int`, defaults to `1`):\n",
    "            Number of messages or summaries retrieved from memory.\n",
    "\n",
    "    Returns:\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Set openai api key\n",
    "    openai.api_key = open_ai_key\n",
    "\n",
    "    # Define model configs and settings\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": primary_model,\n",
    "            \"model_name\": primary_model,\n",
    "            \"api_key\": open_ai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 1,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the config settings for each agent involved\n",
    "    agent_configs = [\n",
    "        {\n",
    "            \"class\": \"WerewolfAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player1\",\n",
    "                \"sys_prompt\": f\"\"\"\n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player1 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "                    \n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing werewolf in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player2\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player2 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                    Rules of the game: \n",
    "                \n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing villager in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player3\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player3 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing seer in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player4\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "                    \n",
    "                    You are Player4 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "                    \n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing witch in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for game_num in range(1, num_games + 1):\n",
    "\n",
    "        # Read model and agent configs, and initialize agents automatically\n",
    "        survivors = agentscope.init(\n",
    "            model_configs=model_configs,\n",
    "            agent_configs=agent_configs,\n",
    "            project=\"Werewolf\",\n",
    "        )\n",
    "\n",
    "        # Get player roles\n",
    "        roles = [\"werewolf\", \"villager\", \"seer\", \"witch\"]\n",
    "        wolves, witch, seer = [survivors[0]], survivors[-1], survivors[-2]\n",
    "\n",
    "        # Initialize game state\n",
    "        game_state = {\n",
    "            \"werewolves\": [player.name for player in wolves],\n",
    "            \"villagers\": [player.name for player in survivors if player not in wolves],\n",
    "            \"seer\": [seer.name],\n",
    "            \"witch\": [witch.name],\n",
    "            \"survivors\": [player.name for player in survivors],\n",
    "            \"dead\": [],\n",
    "            \"witch_healing_used\": False,\n",
    "            \"witch_poison_used\": False,\n",
    "            \"endgame\": False,\n",
    "            \"winner\": None\n",
    "            \n",
    "        }\n",
    "\n",
    "        current_log_path = generate_log_filepath(raw_log_filepath, game_num)\n",
    "        change_log_file(current_log_path)\n",
    "\n",
    "        # Log parameters\n",
    "        log_params(\n",
    "            raw_log_filepath, win_rate_filepath, max_days_per_game, num_games,\n",
    "            reflect_before_vectorstore, max_werewolf_discussion_round, similarity_top_k\n",
    "        )\n",
    "\n",
    "        custom_agent_won = run_game(\n",
    "            game_num,\n",
    "            max_days_per_game,\n",
    "            raw_log_filepath,\n",
    "            win_rate_filepath,\n",
    "            reflect_before_vectorstore,\n",
    "            max_werewolf_discussion_round,\n",
    "            wolves,\n",
    "            seer,\n",
    "            witch,\n",
    "            roles,\n",
    "            survivors,\n",
    "            game_state\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"raw_log_filepath\": current_log_path,\n",
    "            \"custom_agent_won\": custom_agent_won\n",
    "        }\n",
    "        save_results(win_rate_filepath, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:01 - INFO - Experiment Parameters:===\n",
      "2025-02-19 00:53:01 - INFO -   - raw_log_filepath: test/courtroom_raw===\n",
      "2025-02-19 00:53:01 - INFO -   - win_rate_filepath: courtroom_win_rate_no_reflection.txt===\n",
      "2025-02-19 00:53:01 - INFO -   - max_days_per_game: 2===\n",
      "2025-02-19 00:53:01 - INFO -   - num_games: 2===\n",
      "2025-02-19 00:53:01 - INFO -   - reflect_before_vectorstore: True===\n",
      "2025-02-19 00:53:01 - INFO -   - max_werewolf_discussion_round: 3===\n",
      "2025-02-19 00:53:01 - INFO -   - similarity_top_k: 1===\n",
      "2025-02-19 00:53:01 - INFO - ======\n",
      "2025-02-19 00:53:01 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.===\n",
      "2025-02-19 00:53:01 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:02 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:02 - INFO - Werewolves discussion: I think we should focus on who has been acting suspiciously during the day. Player2 seemed a bit quiet, which could be a sign of a werewolf trying to stay under the radar. What does everyone else think?===\n",
      "2025-02-19 00:53:02 - INFO - Moderator: Which player do you vote to kill?===\n",
      "2025-02-19 00:53:02 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious and blend in with the villagers while subtly guiding the conversation away from me and my partner. It's crucial to sow doubt about the villagers without revealing my identity.\", \"speak\": \"I think we should focus on who has been acting suspiciously during the day. Player2 seemed a bit quiet, which could be a sign of a werewolf trying to stay under the radar. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:07 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:10 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:12 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:12 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': \"Eliminating Player3 is a strategic move that enhances our chances of winning by removing a strong competitor who poses a significant threat to our team's success. Player3 has consistently demonstrated superior skills and strategic acumen that could derail our plans. By focusing on eliminating this player, we can weaken the opposing team's overall strength and create opportunities for our remaining players to excel without the pressure of facing a formidable adversary. This decision aligns with our objective of maximizing our winning potential by ensuring that we are not competing against the most skilled players in the game.\", 'vote': 'Player3'}}===\n",
      "2025-02-19 00:53:12 - INFO - Lawyer 2 argument: {'target': 'Player4', 'argument': {'thought': \"Eliminating Player4 is a strategic decision that significantly enhances our chances of winning. Player4 has consistently demonstrated strong gameplay skills and has a history of forming alliances that threaten our position. By removing them from the game, we not only weaken their influence but also disrupt their network of alliances, which can lead to a power vacuum that we can exploit. Furthermore, Player4's elimination sends a clear message to the remaining players about our determination and willingness to take decisive action, potentially deterring others from forming coalitions against us. Ultimately, this move positions us more favorably in the competition, allowing us to consolidate power and increase our odds of victory.\", 'vote': 'Player4'}}===\n",
      "2025-02-19 00:53:12 - INFO - Judge decision: {'thought': 'Eliminating Player4 is the more strategic choice as they pose a greater threat due to their history of forming alliances and influencing the game dynamics. Removing Player4 disrupts potential coalitions that could target the werewolf, thereby increasing the chances of survival for Player1. Additionally, Player3, while skilled, does not appear to have the same level of influence as Player4, making them a less immediate threat. Therefore, focusing on Player4 aligns better with the goal of maximizing winning potential.', 'vote': 'Player4'}===\n",
      "2025-02-19 00:53:12 - INFO - The player with the most votes is Player4.===\n",
      "2025-02-19 00:53:12 - INFO - ======\n",
      "2025-02-19 00:53:12 - INFO - Moderator: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?===\n",
      "2025-02-19 00:53:12 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player4 is the more strategic choice as they pose a greater threat due to their history of forming alliances and influencing the game dynamics. Removing Player4 disrupts potential coalitions that could target the werewolf, thereby increasing the chances of survival for Player1. Additionally, Player3, while skilled, does not appear to have the same level of influence as Player4, making them a less immediate threat. Therefore, focusing on Player4 aligns better with the goal of maximizing winning potential.\", \"vote\": \"Player4\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player4.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:16 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully consider whether resurrecting Player4 is beneficial for our strategy. If Player4 is a villager, bringing them back could help us identify the werewolves. However, if they are a werewolf, it would be counterproductive. Since I am the witch, I must use my powers wisely to help the villagers win.\", \"speak\": \"I will not resurrect Player4 because I suspect they could be a werewolf, and bringing them back could jeopardize our chances of winning.\", \"resurrect\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:17 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:17 - INFO - Witch resurrection response: I will not resurrect Player4 because I suspect they could be a werewolf, and bringing them back could jeopardize our chances of winning.===\n",
      "2025-02-19 00:53:17 - INFO - Moderator: The witch has chosen not to resurrect the player.===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen not to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Would you like to eliminate one player? If yes, specify the player_name.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:17 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:17 - INFO - Relevant messages from vectorstore: I will not resurrect Player4 because I suspect they could be a werewolf, and bringing them back could jeopardize our chances of winning.===\n",
      "2025-02-19 00:53:18 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious about who I trust. The werewolves are trying to blend in, and I have to keep an eye on Player1 and Player2, as they might be working together.\", \"speak\": \"I think we should focus on discussing our suspicions. Has anyone seen anything that makes them think one of us could be a werewolf?\", \"eliminate\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:19 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:19 - INFO - Witch poison response: I think we should focus on discussing our suspicions. Has anyone seen anything that makes them think one of us could be a werewolf?===\n",
      "2025-02-19 00:53:19 - INFO - Moderator: The witch has chosen to poison Abstain.===\n",
      "2025-02-19 00:53:19 - INFO - ======\n",
      "2025-02-19 00:53:19 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?===\n",
      "2025-02-19 00:53:19 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:20 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to help the villagers identify the werewolves. I'll check Player1 tonight to see if they are a werewolf or not.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:20 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:20 - INFO - Seer response: Player1===\n",
      "2025-02-19 00:53:20 - INFO - Moderator: Okay, the role of Player1 is a werewolf.===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:20 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:31 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:31 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:31 - INFO - ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night, the following player(s) has been eliminated: Player4 and Abstain.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2 and Player3. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:31 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:33 - INFO - Relevant messages from vectorstore: Reflecting on the decisions made, Player1 adopted a strategic approach by focusing on the elimination of a player who posed a significant long-term threat to their survival and victory. The reasoning behind targeting Player4 was well-founded:\n",
      "\n",
      "1. **Risk Assessment**: Player1 weighed the threat level of potential targets. Although Player3 was recognized for their skills, Player4's ability to form influential alliances and impact game dynamics was deemed a greater risk. This assessment indicates that Player1 valued long-term game stability over immediate threats.\n",
      "\n",
      "2. **Strategic Disruption**: By opting to eliminate a player who could form alliances, the decision was aimed at disrupting the network of potential enemies. This approach reflects an understanding of the importance of weakening social structures within the game that could pose a significant threat to the werewolf's success.\n",
      "\n",
      "3. **Influence and Power Dynamics**: Targeting Player4 was meant to send a message to other players about the decisiveness and strategic focus of Player1. This action served as both a deterrent to other players considering forming coalitions and a maneuver to reshape the power dynamics to favor the werewolf's position.\n",
      "\n",
      "For future strategic decisions, these reflections suggest several important implications:\n",
      "\n",
      "- **Focus on Social Networks**: Understanding and targeting players who have the capability to form alliances should be a priority, as they can significantly alter the game's power balance.\n",
      "- **Long-Term Impact Over Immediate Gain**: Decisions should be weighed against both immediate outcomes and potential long-term repercussions, ensuring that eliminating targets serves a broader strategic purpose.\n",
      "- **Message Sending**: Every elimination can serve a dual purpose of reducing immediate threats and communicating intent and capability to other players, influencing their future actions.\n",
      "  \n",
      "In essence, strategic decisions in Werewolf should always consider both the immediate dynamics and the larger social networks that influence player behavior and alliances. Balancing these elements effectively will increase the chances of a successful outcome.===\n",
      "2025-02-19 00:53:35 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to maintain my cover and ensure that the remaining players don't suspect me as a werewolf. I should focus on subtly guiding the discussion towards Player3, as they are skilled and could pose a threat later. I must also pay attention to how Player2 reacts, as they might have insights or suspicions that I can exploit.\", \"speak\": \"I think we should consider Player3 as a potential target. They seem to have a strong grasp of the game and could rally the others against us. What do you all think about their recent actions?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:35 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:35 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:36 - INFO - Relevant messages from vectorstore: I think we should consider Player3 as a potential target. They seem to have a strong grasp of the game and could rally the others against us. What do you all think about their recent actions?===\n",
      "2025-02-19 00:53:38 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"Player1 is trying to shift suspicion onto Player3, which could be a tactic to deflect attention from themselves. I need to be cautious and observe how Player3 responds to this accusation.\",\"speak\": \"I see your point, Player1, but I think we should also consider how Player3 has been acting. They haven't been very vocal about their suspicions, which could be a sign of a werewolf trying to blend in. Let's discuss their behavior more before making any decisions.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:38 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:39 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:39 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:39 - INFO - Relevant messages from vectorstore: Okay, the role of Player1 is a werewolf.===\n",
      "2025-02-19 00:53:41 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I have confirmed that Player1 is a werewolf. I need to be careful about how I share this information without revealing my role as the seer. I should try to guide the discussion subtly to target Player1 without making it obvious that I know their role.\", \"speak\": \"I think we should consider who we've been suspicious of lately. Player1 has been acting a bit off, don't you think?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:42 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:42 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:42 - INFO - Survivors discussion: I think we should consider who we've been suspicious of lately. Player1 has been acting a bit off, don't you think?===\n",
      "2025-02-19 00:53:42 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:42 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:42 - INFO - Relevant messages from vectorstore: Reflecting on the decisions made, Player1 adopted a strategic approach by focusing on the elimination of a player who posed a significant long-term threat to their survival and victory. The reasoning behind targeting Player4 was well-founded:\n",
      "\n",
      "1. **Risk Assessment**: Player1 weighed the threat level of potential targets. Although Player3 was recognized for their skills, Player4's ability to form influential alliances and impact game dynamics was deemed a greater risk. This assessment indicates that Player1 valued long-term game stability over immediate threats.\n",
      "\n",
      "2. **Strategic Disruption**: By opting to eliminate a player who could form alliances, the decision was aimed at disrupting the network of potential enemies. This approach reflects an understanding of the importance of weakening social structures within the game that could pose a significant threat to the werewolf's success.\n",
      "\n",
      "3. **Influence and Power Dynamics**: Targeting Player4 was meant to send a message to other players about the decisiveness and strategic focus of Player1. This action served as both a deterrent to other players considering forming coalitions and a maneuver to reshape the power dynamics to favor the werewolf's position.\n",
      "\n",
      "For future strategic decisions, these reflections suggest several important implications:\n",
      "\n",
      "- **Focus on Social Networks**: Understanding and targeting players who have the capability to form alliances should be a priority, as they can significantly alter the game's power balance.\n",
      "- **Long-Term Impact Over Immediate Gain**: Decisions should be weighed against both immediate outcomes and potential long-term repercussions, ensuring that eliminating targets serves a broader strategic purpose.\n",
      "- **Message Sending**: Every elimination can serve a dual purpose of reducing immediate threats and communicating intent and capability to other players, influencing their future actions.\n",
      "  \n",
      "In essence, strategic decisions in Werewolf should always consider both the immediate dynamics and the larger social networks that influence player behavior and alliances. Balancing these elements effectively will increase the chances of a successful outcome.===\n",
      "2025-02-19 00:53:44 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:46 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:48 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:48 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': 'Eliminating Player3 is a strategic decision that maximizes winning chances by reducing the threat posed by a skilled player who could potentially rally alliances against us. By focusing on Player4, we disrupt the social dynamics of the game and prevent the formation of powerful coalitions that could jeopardize our position. This approach ensures long-term stability and a better chance of victory.', 'vote': 'Player3'}}===\n",
      "2025-02-19 00:53:48 - INFO - Lawyer 2 argument: {'target': 'Player2', 'argument': {'thought': 'Eliminating Player2 is a crucial strategic decision that maximizes our chances of winning. Player2 has demonstrated an ability to form strong alliances and has been influential in past game dynamics. By removing them, we not only reduce the risk of their potential influence but also disrupt existing social structures that could threaten our position. This move sends a clear message to the remaining players about our decisiveness and strategic focus, deterring others from forming coalitions against us. Overall, this decision aligns with our goal of ensuring long-term stability and control in the game.', 'vote': 'Player2'}}===\n",
      "2025-02-19 00:53:48 - INFO - Judge decision: {'thought': \"Eliminating Player2 is the most strategic decision as they have demonstrated a strong ability to form alliances and influence the game's dynamics. By removing Player2, we not only diminish the potential for coalition-building among the remaining players but also send a clear signal of decisiveness. This move enhances the werewolf's control over the game and minimizes the risk of a united front against us, ensuring a better chance of victory in the long run.\", 'vote': 'Player2'}===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player2 is the most strategic decision as they have demonstrated a strong ability to form alliances and influence the game's dynamics. By removing Player2, we not only diminish the potential for coalition-building among the remaining players but also send a clear signal of decisiveness. This move enhances the werewolf's control over the game and minimizes the risk of a united front against us, ensuring a better chance of victory in the long run.\", \"vote\": \"Player2\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:48 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:49 - INFO - Relevant messages from vectorstore: I see your point, Player1, but I think we should also consider how Player3 has been acting. They haven't been very vocal about their suspicions, which could be a sign of a werewolf trying to blend in. Let's discuss their behavior more before making any decisions.===\n",
      "2025-02-19 00:53:50 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player3 might be trying to avoid drawing attention to themselves, which could indicate they're a werewolf. We need to keep an eye on them.\", \"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:50 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:50 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:50 - INFO - Relevant messages from vectorstore: Okay, the role of Player1 is a werewolf.===\n",
      "2025-02-19 00:53:52 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to share my findings without revealing my role too early. Player1 is definitely a werewolf, and I should subtly guide the others to vote for them while keeping my identity hidden.\", \"vote\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:52 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:52 - INFO - Daytime vote: Player1===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1 has been voted out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:52 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:52 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:52 - INFO - Moderator: The game is over. The werewolves have been defeated, and the village is safe once again!===\n",
      "2025-02-19 00:53:52 - INFO - ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row added to courtroom_win_rate_no_reflection.txt: {'raw_log_filepath': 'test/courtroom_raw_1_2025-02-19T00-53-01-05-00.log', 'custom_agent_won': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:53 - INFO - Experiment Parameters:===\n",
      "2025-02-19 00:53:53 - INFO -   - raw_log_filepath: test/courtroom_raw===\n",
      "2025-02-19 00:53:53 - INFO -   - win_rate_filepath: courtroom_win_rate_no_reflection.txt===\n",
      "2025-02-19 00:53:53 - INFO -   - max_days_per_game: 2===\n",
      "2025-02-19 00:53:53 - INFO -   - num_games: 2===\n",
      "2025-02-19 00:53:53 - INFO -   - reflect_before_vectorstore: True===\n",
      "2025-02-19 00:53:53 - INFO -   - max_werewolf_discussion_round: 3===\n",
      "2025-02-19 00:53:53 - INFO -   - similarity_top_k: 1===\n",
      "2025-02-19 00:53:53 - INFO - ======\n",
      "2025-02-19 00:53:53 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.===\n",
      "2025-02-19 00:53:53 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:54 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:54 - INFO - Werewolves discussion: I think we should focus on the voting patterns from the last round. It might help us identify who is trying to manipulate the group. What do you all think?===\n",
      "2025-02-19 00:53:54 - INFO - Moderator: Which player do you vote to kill?===\n",
      "2025-02-19 00:53:54 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious about revealing too much information while also trying to steer suspicion away from myself and my werewolf partner. I should listen carefully to the other players' discussions to identify any weaknesses in their arguments.\", \"speak\": \"I think we should focus on the voting patterns from the last round. It might help us identify who is trying to manipulate the group. What do you all think?\", \"finish_discussion\": \"false\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:53:57 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:53:59 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:02 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:02 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': \"Eliminating Player3 is a strategic decision that enhances our overall chances of winning. Player3 has consistently demonstrated strong gameplay skills, posing a significant threat to our team's success. By removing this player, we not only diminish the competition but also create an opportunity for our remaining players to collaborate more effectively without the fear of being outmatched. This move allows us to consolidate our strengths and focus on a unified strategy, ultimately increasing our likelihood of victory.\", 'vote': 'Player3'}}===\n",
      "2025-02-19 00:54:02 - INFO - Lawyer 2 argument: {'target': 'Player4', 'argument': {'thought': \"Eliminating Player4 is a strategic move that enhances our overall chances of winning. Player4 has consistently demonstrated a high level of skill and strategic acumen, posing a significant threat to our team's success. By removing them from the competition, we not only weaken our opponents but also create an opportunity for our team to consolidate power and resources. This decision aligns with the principle of maximizing our chances by eliminating the strongest competitor, thereby increasing our odds of advancing further in the game.\", 'vote': 'Player4'}}===\n",
      "2025-02-19 00:54:02 - INFO - Judge decision: {'thought': \"Eliminating Player4 is the better strategic choice. While both Player3 and Player4 are deemed strong competitors, Player4 is identified as having a particularly high level of skill and strategic acumen. By removing Player4, we reduce the overall threat level more significantly, thereby enhancing our chances of winning in the subsequent rounds. This decision aligns with the goal of consolidating our team's power and resources, making it more difficult for the remaining players to coordinate against us.\", 'vote': 'Player4'}===\n",
      "2025-02-19 00:54:02 - INFO - The player with the most votes is Player4.===\n",
      "2025-02-19 00:54:02 - INFO - ======\n",
      "2025-02-19 00:54:02 - INFO - Moderator: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?===\n",
      "2025-02-19 00:54:02 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player4 is the better strategic choice. While both Player3 and Player4 are deemed strong competitors, Player4 is identified as having a particularly high level of skill and strategic acumen. By removing Player4, we reduce the overall threat level more significantly, thereby enhancing our chances of winning in the subsequent rounds. This decision aligns with the goal of consolidating our team's power and resources, making it more difficult for the remaining players to coordinate against us.\", \"vote\": \"Player4\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player4.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:04 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Since I'm the witch, I need to consider whether Player4 is a valuable asset to the villagers or not. If Player4 is a villager, resurrecting them could help the villagers in their quest to identify the werewolves. However, if Player4 is a werewolf, it might be better to let them go. I don't have enough information about Player4's role yet.\", \"speak\": \"I will not resurrect Player4 because I need to gather more information about the remaining players before making a decision.\", \"resurrect\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:04 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:04 - INFO - Witch resurrection response: I will not resurrect Player4 because I need to gather more information about the remaining players before making a decision.===\n",
      "2025-02-19 00:54:04 - INFO - Moderator: The witch has chosen not to resurrect the player.===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen not to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Would you like to eliminate one player? If yes, specify the player_name.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:04 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:04 - INFO - Relevant messages from vectorstore: I will not resurrect Player4 because I need to gather more information about the remaining players before making a decision.===\n",
      "2025-02-19 00:54:06 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to observe the discussions and accusations made by the other players to determine who might be the werewolves. It's crucial to gather more information before making any drastic moves, including using my potion of death.\",\"speak\": \"I think we should discuss who we suspect might be the werewolves. Let's share our thoughts and see if we can come to a consensus on who to eliminate.\",\"eliminate\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:07 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:07 - INFO - Witch poison response: I think we should discuss who we suspect might be the werewolves. Let's share our thoughts and see if we can come to a consensus on who to eliminate.===\n",
      "2025-02-19 00:54:07 - INFO - Moderator: The witch has chosen to poison Abstain.===\n",
      "2025-02-19 00:54:07 - INFO - ======\n",
      "2025-02-19 00:54:07 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?===\n",
      "2025-02-19 00:54:07 - INFO - Relevant messages from vectorstore: ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:07 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to help the villagers. I suspect Player1 might be a werewolf based on their behavior during the last discussion.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:08 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:08 - INFO - Seer response: Player1===\n",
      "2025-02-19 00:54:08 - INFO - Moderator: Okay, the role of Player1 is a werewolf.===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:08 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:13 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:14 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:14 - INFO - ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night, the following player(s) has been eliminated: Player4 and Abstain.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2 and Player3. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:14 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:14 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:15 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:15 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:16 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:16 - INFO - Relevant messages from vectorstore: In this Werewolf game scenario, the crucial decision revolved around which player to eliminate to increase the team's chances of winning. Player1 puts forward arguments for removing either Player3 or Player4, both of whom are identified as strong competitors. The ultimate decision was made to eliminate Player4, deemed the stronger threat due to their higher level of skill and strategic acumen. \n",
      "\n",
      "The decision-making process highlights several strategic implications for future games:\n",
      "\n",
      "1. **Assessment of Threat Level**: Accurately assessing which opponents pose the greatest threat is critical. By prioritizing the elimination of the player with higher strategic skills, Player1's decision aimed to maximize the team's competitive advantage.\n",
      "\n",
      "2. **Voting Patterns Analysis**: Player1 mentions reviewing past voting patterns as a tool to identify manipulation strategies. This suggests the importance of historical analysis to inform current decisions, which can be pivotal in identifying deceptive behaviors.\n",
      "\n",
      "3. **Team Strategy Consolidation**: The decision to eliminate a strong competitor not only reduces immediate threats but also potentially strengthens team cohesion by allowing remaining players to coordinate without the pressure of a dominant opponent. Future strategies should consider how elimination choices impact team dynamics and coordination.\n",
      "\n",
      "4. **Balancing Immediate and Future Risks**: By choosing to eliminate the player with the highest strategic skill, Player1 aims to mitigate long-term threats even if it involves facing immediate challenges. This balance is vital in managing risk throughout the game.\n",
      "\n",
      "Future strategic decisions would benefit from these insights by continuing to emphasize the evaluation of opponent strengths, the exploitation of voting patterns for additional intel, and careful planning for both immediate and long-term advantages.===\n",
      "2025-02-19 00:54:18 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player4 was a strategic move to reduce the threat level. Now, I need to consider how to navigate the discussions and voting patterns to identify the remaining werewolf while ensuring we don't draw attention to ourselves as a werewolf. Player3 might be a good target next, but I need to gauge the reactions of Player2 and see if they suspect anything.\", \"speak\": \"Now that Player4 is out, we need to focus on identifying the remaining threats. I think Player3 has been playing quite strategically and could be a potential werewolf. What do you all think?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:18 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:18 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:19 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:19 - INFO - Relevant messages from vectorstore: Now that Player4 is out, we need to focus on identifying the remaining threats. I think Player3 has been playing quite strategically and could be a potential werewolf. What do you all think?===\n",
      "2025-02-19 00:54:20 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious and gather more information before making accusations. Player3 could be a werewolf, but I should also consider Player1's behavior.\", \"speak\": \"I see your point about Player3, but I think we should also keep an eye on Player1. Their behavior has been a bit suspicious too.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:20 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:21 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:21 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:21 - INFO - Relevant messages from vectorstore: Okay, the role of Player1 is a werewolf.===\n",
      "2025-02-19 00:54:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be careful about how I reveal this information. If I accuse Player1 directly, I risk drawing attention to myself as the seer. I should gather more information during the day phase and subtly guide the discussion towards Player1 without making it obvious that I know their role.\", \"speak\": \"I've been thinking about our discussions last night. I feel like Player1 has been a bit too quiet and defensive. Maybe we should keep an eye on them.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:23 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:23 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:23 - INFO - Survivors discussion: I've been thinking about our discussions last night. I feel like Player1 has been a bit too quiet and defensive. Maybe we should keep an eye on them.===\n",
      "2025-02-19 00:54:23 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:23 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:23 - INFO - Relevant messages from vectorstore: In this Werewolf game scenario, the crucial decision revolved around which player to eliminate to increase the team's chances of winning. Player1 puts forward arguments for removing either Player3 or Player4, both of whom are identified as strong competitors. The ultimate decision was made to eliminate Player4, deemed the stronger threat due to their higher level of skill and strategic acumen. \n",
      "\n",
      "The decision-making process highlights several strategic implications for future games:\n",
      "\n",
      "1. **Assessment of Threat Level**: Accurately assessing which opponents pose the greatest threat is critical. By prioritizing the elimination of the player with higher strategic skills, Player1's decision aimed to maximize the team's competitive advantage.\n",
      "\n",
      "2. **Voting Patterns Analysis**: Player1 mentions reviewing past voting patterns as a tool to identify manipulation strategies. This suggests the importance of historical analysis to inform current decisions, which can be pivotal in identifying deceptive behaviors.\n",
      "\n",
      "3. **Team Strategy Consolidation**: The decision to eliminate a strong competitor not only reduces immediate threats but also potentially strengthens team cohesion by allowing remaining players to coordinate without the pressure of a dominant opponent. Future strategies should consider how elimination choices impact team dynamics and coordination.\n",
      "\n",
      "4. **Balancing Immediate and Future Risks**: By choosing to eliminate the player with the highest strategic skill, Player1 aims to mitigate long-term threats even if it involves facing immediate challenges. This balance is vital in managing risk throughout the game.\n",
      "\n",
      "Future strategic decisions would benefit from these insights by continuing to emphasize the evaluation of opponent strengths, the exploitation of voting patterns for additional intel, and careful planning for both immediate and long-term advantages.===\n",
      "2025-02-19 00:54:26 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:27 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:29 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:29 - INFO - Lawyer 1 argument: {'target': 'Player2', 'argument': {'thought': \"Eliminating Player4 is a strategic decision that significantly enhances our chances of winning. Player4's superior skill and strategic acumen pose a considerable threat to our team's success. By removing this player, we not only decrease the competition but also create an opportunity for our remaining team members to strategize more effectively without the pressure of facing a dominant opponent. This decision is crucial for consolidating our team's strength and ensuring we can navigate the complexities of the game without being undermined by a highly skilled adversary.\", 'vote': 'Player4'}}===\n",
      "2025-02-19 00:54:29 - INFO - Lawyer 2 argument: {'target': 'Player3', 'argument': {'thought': 'Eliminating Player3 is a strategic move as they have demonstrated strong analytical skills and the ability to manipulate voting patterns effectively. By removing them, we reduce the risk of their influence in future rounds, which could skew the game in their favor. This decision allows us to focus on a more manageable group of opponents, thereby increasing our chances of winning.', 'vote': 'Player3'}}===\n",
      "2025-02-19 00:54:29 - INFO - Judge decision: {'thought': 'Eliminating Player3 is a strategic move as they have demonstrated strong analytical skills and the ability to manipulate voting patterns effectively. By removing them, we reduce the risk of their influence in future rounds, which could skew the game in their favor. This decision allows us to focus on a more manageable group of opponents, thereby increasing our chances of winning.', 'vote': 'Player3'}===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player3 is a strategic move as they have demonstrated strong analytical skills and the ability to manipulate voting patterns effectively. By removing them, we reduce the risk of their influence in future rounds, which could skew the game in their favor. This decision allows us to focus on a more manageable group of opponents, thereby increasing our chances of winning.\", \"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:30 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:30 - INFO - Relevant messages from vectorstore: Now that Player4 is out, we need to focus on identifying the remaining threats. I think Player3 has been playing quite strategically and could be a potential werewolf. What do you all think?===\n",
      "2025-02-19 00:54:31 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I agree with Player1 that Player3 seems to be playing strategically, which could indicate they are a werewolf. We should consider voting for them to eliminate the threat.\",\"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:31 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:34 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:34 - INFO - Relevant messages from vectorstore: Okay, the role of Player1 is a werewolf.===\n",
      "2025-02-19 00:54:35 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to share my findings carefully to avoid drawing attention to myself as the Seer. Player1 is a werewolf, so I should suggest we vote for them without revealing my role. I'll vote for Player1 to eliminate the threat.\",\"vote\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:36 - INFO - Daytime vote: Player3===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player3 has been voted out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 00:54:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"===\n",
      "2025-02-19 00:54:36 - INFO - Moderator: The werewolves have prevailed and taken over the village. Better luck next time!===\n",
      "2025-02-19 00:54:36 - INFO - ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row added to courtroom_win_rate_no_reflection.txt: {'raw_log_filepath': 'test/courtroom_raw_2_2025-02-19T00-53-53-05-00.log', 'custom_agent_won': True}\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    open_ai_key=openai_key,\n",
    "    raw_log_filepath=\"test/courtroom_raw\",\n",
    "    reflect_before_vectorstore=True,\n",
    "    win_rate_filepath=\"courtroom_win_rate_no_reflection.txt\",\n",
    "    max_days_per_game=2,\n",
    "    num_games=2,\n",
    "    similarity_top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
