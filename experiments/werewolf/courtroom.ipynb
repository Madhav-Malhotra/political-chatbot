{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf Simulator\n",
    "The following runs a simulation of the game Werewolf with multiple agents, it is built on top of and uses the Agentscope library, documentation can be found here https://doc.agentscope.io/, and the open source code that we extend on is here https://github.com/modelscope/agentscope/tree/main.\n",
    "### Rules of Werewolf\n",
    "- There are 4 roles, the Werewolves, Villagers, Witch, and Seer (Witch and Seer are on the Villager team)\n",
    "- Each night:\n",
    "    - The Werewolves discuss and vote on a player to eliminate\n",
    "    - The Witch is told what player the wolves voted to eliminate, and is given the choice to use their potion of healing to reserruct the eliminated player, or use their poison to eliminate another player. Note that each power can only be used once in a game\n",
    "    - The seer can pick any other player and find out what their role is (one player per night)\n",
    "- After the events of the night, all surviving players discuss amongst themselves and vote on a player to eliminate\n",
    "- Werewolves win if their numbers equal or exceed Villagers.\n",
    "- Villagers win if all Werewolves are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: agentscope==0.1.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: faiss-cpu==1.9.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: black in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: docstring-parser in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.16)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.10.6)\n",
      "Requirement already satisfied: loguru==0.6.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: inputimeout in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: Flask==3.0.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: Flask-Cors==4.0.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: Flask-SocketIO==5.3.6 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (5.3.6)\n",
      "Requirement already satisfied: flask-sqlalchemy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: flake8 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (7.1.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: openai>=1.3.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.60.2)\n",
      "Requirement already satisfied: dashscope>=1.19.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.22.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from faiss-cpu==1.9.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: python-socketio>=5.0.2 in ./venv/lib/python3.12/site-packages (from Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (5.12.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.11.11)\n",
      "Requirement already satisfied: websocket-client in ./venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (2.12.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.16 in ./venv/lib/python3.12/site-packages (from flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.37)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2024.12.14)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken->agentscope==0.1.1->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./venv/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in ./venv/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (4.11.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in ./venv/lib/python3.12/site-packages (from python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wsproto in ./venv/lib/python3.12/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional, Union, Sequence, Any, List\n",
    "from functools import partial\n",
    "import openai  \n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "from agentscope.parsers.json_object_parser import MarkdownJsonDictParser\n",
    "from agentscope.parsers import ParserBase\n",
    "from agentscope.message import Msg\n",
    "from agentscope.msghub import msghub\n",
    "from agentscope.agents import AgentBase\n",
    "from agentscope.memory.temporary_memory import TemporaryMemory\n",
    "from agentscope.pipelines.functional import sequentialpipeline\n",
    "import agentscope\n",
    "\n",
    "from utils.werewolf_utils import (\n",
    "    extract_name_and_id,\n",
    "    n2s,\n",
    "    set_parsers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementations\n",
    "Vector store classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            self.messages.append(msg.name + \": \" + msg.content)\n",
    "        \n",
    "    def summarize_cycle(self, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Combine all messages since the last summary\n",
    "        history = \"\\n\".join(self.messages)\n",
    "        \n",
    "        # Use the slow mind (gpt-4o) to summarize\n",
    "        slow_mind_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Slow mind model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"Summarize the following chat history from the last {cycle_type}:\"},\n",
    "                {\"role\": \"user\", \"content\": history}, \n",
    "            ],\n",
    "        )\n",
    "        summary = slow_mind_response.choices[0].message.content\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        # Clear messages for the next cycle (current implementation gets summary from the point after the last summary was created)\n",
    "        self.messages.clear()\n",
    "\n",
    "    def get_relevant_summaries(self, query: str, top_k: int = 1):\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "        results = [\n",
    "            self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "        ]\n",
    "        return results\n",
    "    \n",
    "    \n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        # add embedding of message to vector store\n",
    "        embeddings = [self._get_embedding(msg.name + \": \" + msg.content) for msg in message]\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.messages.extend(message)\n",
    "\n",
    "    def get_relevant_messages(self, query: str, top_k: int = 10):\n",
    "        \"\"\"Retrieve the top-k most relevant messages.\"\"\"\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "        results = [\n",
    "            self.messages[idx] for idx in indices[0] if idx < len(self.messages)\n",
    "        ]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agents (edit here)\n",
    "You can define custom Agents by inheriting from the AgentBase class like shown here.\n",
    "\n",
    "Below is a copy of the DictDialogAgent that generates responses in a dict format that is compatible with our simulator. More documentation on AgentScope agents can be found here https://doc.agentscope.io/build_tutorial/builtin_agent.html, existing agent implementations can be found here https://github.com/modelscope/agentscope/tree/main/src/agentscope/agents.\n",
    "\n",
    "For our case, modifying the agent class is, in conjunction with the parsers we pass in (more details in next cell), play a critical role in defining our agent behavior. We can edit this section by either defining a brand new agent type we want to explore, or modifying the current one. The primary (and mostly only) source of focus should be the reply the function in the agent class as that controls what an agent uses to generate a response, particular when we are working with different architectures, eg. how we use memory, single vs multi agent, we make those edits here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lawyer(AgentBase):\n",
    "    \"\"\"Agent that argues for eliminating a specific target dynamically using LLM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Lawyer agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the Lawyer agent.\n",
    "            \n",
    "            sys_prompt (`str`):\n",
    "                The system prompt that provides the agent with role instructions\n",
    "                and context for decision-making.\n",
    "            \n",
    "            model_config_name (`str`):\n",
    "                The name of the model configuration, which determines the LLM\n",
    "                used for evaluating arguments.\n",
    "            \n",
    "            parser (`ParserBase`):\n",
    "                The parser responsible for formatting the model’s output,\n",
    "                extracting structured responses, and defining valid response fields.\n",
    "            \n",
    "            context_messages (`list`):\n",
    "                A list of past messages providing context for the lawyer’s argumentation.\n",
    "            \n",
    "            model:\n",
    "                The LLM model used to generate arguments.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "\n",
    "    def argue(self, game_state: dict, available_targets: list) -> dict:\n",
    "        \"\"\"\n",
    "        Generate an argument for eliminating a player.\n",
    "\n",
    "        Args:\n",
    "            game_state (`dict`): The current state of the game.\n",
    "            available_targets (`list`): List of players who can be eliminated.\n",
    "\n",
    "        Returns:\n",
    "            `dict`: A dictionary containing:\n",
    "                - `\"target\"` (`str`): The chosen target for elimination.\n",
    "                - `\"argument\"` (`str`): The argument supporting the choice.\n",
    "        \"\"\"\n",
    "        if not available_targets:\n",
    "            return {\"target\": None, \"argument\": \"No valid targets available.\"}\n",
    "\n",
    "        # Construct prompt for argument generation\n",
    "        lawyer_prompt = f\"\"\"\n",
    "        You are a lawyer in a game of Werewolf. Argue for eliminating one of {', '.join(available_targets)}.\n",
    "        \n",
    "        Consider:\n",
    "        - The player's past behavior (e.g., suspicions, accusations, voting history).\n",
    "        - Logical reasoning that would convince the judge.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", lawyer_prompt, role=\"system\"),\n",
    "            self.context_messages,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call the LLM to generate an argument\n",
    "        response = self.model(formatted_prompt)\n",
    "\n",
    "        return {\n",
    "            \"target\": extract_name_and_id(response.text)[0],  # Extract chosen target\n",
    "            \"argument\": response.text  # Return generated argument\n",
    "        }\n",
    "    \n",
    "class Judge(AgentBase):\n",
    "    \"\"\"Agent that evaluates arguments and decides who should be eliminated in the Werewolf game.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Judge agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the Judge agent.\n",
    "            \n",
    "            sys_prompt (`str`):\n",
    "                The system prompt that provides the agent with role instructions\n",
    "                and context for decision-making.\n",
    "            \n",
    "            model_config_name (`str`):\n",
    "                The name of the model configuration, which determines the LLM\n",
    "                used for evaluating arguments.\n",
    "            \n",
    "            parser (`ParserBase`):\n",
    "                The parser responsible for formatting the model’s output,\n",
    "                extracting structured responses, and defining valid response fields.\n",
    "            \n",
    "            context_messages (`list`):\n",
    "                A list of past messages providing context for the judge’s decision-making.\n",
    "            \n",
    "            model:\n",
    "                The LLM model used to evaluate arguments and make elimination decisions.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "\n",
    "    def decide(self, game_state: dict, argument_1: dict, argument_2: dict):\n",
    "        \"\"\"\n",
    "        Evaluate the lawyers' arguments and make a decision.\n",
    "\n",
    "        Args:\n",
    "            game_state (`dict`): The current state of the game.\n",
    "            argument_1 (`dict`): The first lawyer's argument, containing:\n",
    "                - `\"target\"` (`str`): The player they want to eliminate.\n",
    "                - `\"argument\"` (`str`): The reasoning behind their choice.\n",
    "            argument_2 (`dict`): The second lawyer's argument, with the same structure.\n",
    "\n",
    "        Returns:\n",
    "            The raw response from the model without modification.\n",
    "        \"\"\"\n",
    "        argument_1_target = argument_1[\"target\"]\n",
    "        argument_1_argument = argument_1[\"argument\"]\n",
    "        \n",
    "        argument_2_target = argument_2[\"target\"]\n",
    "        argument_2_argument = argument_2[\"argument\"]\n",
    "\n",
    "        # Construct prompt for decision-making\n",
    "        judge_prompt = f\"\"\"\n",
    "        You are a judge in a game of Werewolf, responsible for deciding who should be eliminated.\n",
    "        \n",
    "        Game State:\n",
    "        - Alive players: {game_state[\"survivors\"]}\n",
    "        - Dead players: {game_state[\"dead\"]}\n",
    "        - Known werewolves: {game_state[\"werewolves\"]}\n",
    "\n",
    "        The two lawyers have made their arguments:\n",
    "        - **argument_1** wants to eliminate {argument_1_target}: \"{argument_1_argument}\"\n",
    "        - **argument_2** wants to eliminate {argument_2_target}: \"{argument_2_argument}\"\n",
    "\n",
    "        Evaluate both arguments carefully and decide who should be eliminated.\n",
    "        Your decision should consider:\n",
    "        - Which argument is more logically sound.\n",
    "        - The strategic advantage of eliminating one target over the other.\n",
    "        - Any potential risks for the werewolves.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", judge_prompt, role=\"system\"),\n",
    "            self.context_messages,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        raw_response = self.model(formatted_prompt)\n",
    "\n",
    "        return raw_response\n",
    "\n",
    "    \n",
    "class WerewolfAgent(AgentBase):\n",
    "    \"\"\"An agent that represents a \"werewolf\" in the game and generates responses in a dictionary format.\n",
    "    \n",
    "    The user can specify required fields in the response via a parser.\n",
    "\n",
    "    Includes a FAISS vectorstore for memory retrieval.\n",
    "\n",
    "    For usage examples, refer to `examples/game_werewolf`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the werewolf agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            \n",
    "            sys_prompt (`str`):\n",
    "                The system prompt that provides role instructions.\n",
    "            \n",
    "            model_config_name (`str`):\n",
    "                The name of the model configuration, which determines the LLM used.\n",
    "            \n",
    "            reflect_before_vectorstore (`bool`):\n",
    "                Whether to use reflective memory before retrieving context from FAISS.\n",
    "            \n",
    "            openai_api_key (`str`, defaults to `\"\"`):\n",
    "                The API key used for OpenAI integration.\n",
    "            \n",
    "            model:\n",
    "                The LLM model used to generate responses.\n",
    "            \n",
    "            parser (`ParserBase`):\n",
    "                The parser responsible for formatting and extracting structured responses.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.model_config_name = model_config_name\n",
    "\n",
    "        # Set OpenAI API key\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        # Initialize FAISS memory store\n",
    "\n",
    "        if reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which formats responses, parses them, and filters fields when storing messages.\"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Generate a response based on input messages.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to process.\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "        \"\"\"\n",
    "        global game_log\n",
    "\n",
    "        # Add the input message to memory\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "        query = x.content if x is not None else \"\"\n",
    "\n",
    "        # Retrieve relevant messages from memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            if len(self.memory.summaries) > 0:\n",
    "                relevant_summaries = self.memory.get_relevant_summaries(query=query, top_k=self.similarity_top_k)\n",
    "                summary_context = \"\\n\".join(relevant_summaries)\n",
    "            else:\n",
    "                summary_context = \"\"\n",
    "\n",
    "            game_log.append(f\"Summary context: {summary_context}\")\n",
    "\n",
    "            # prepare prompt with context set by retrieved summaries\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")\n",
    "                or x,  # type: ignore[arg-type]\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "        else:\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=1) or []\n",
    "\n",
    "            game_log.append(f\"Relevant messages: {\"\\n\".join([msg.content for msg in relevant_messages])}\")\n",
    "\n",
    "            # prepare prompt with retrieved messages similar to input message\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_messages\n",
    "                or x,  # type: ignore[arg-type]\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "\n",
    "        # Call LLM\n",
    "        raw_response = self.model(prompt)\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parse raw response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=msg)\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def lawyer_judge_decision(self, game_state: dict, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Decision-making function that involves lawyers and a judge to determine an elimination target.\n",
    "\n",
    "        Args:\n",
    "            game_state (`dict`): The current state of the game.\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`): Optional input messages.\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The final decision generated by the judge.\n",
    "        \"\"\"\n",
    "        global game_log\n",
    "\n",
    "        # Add input message to memory\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "        query = x.content if x is not None else \"\"\n",
    "        relevant_message = []\n",
    "\n",
    "        # Retrieve relevant messages from memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            if len(self.memory.summaries) > 0:\n",
    "                relevant_summary = self.memory.get_relevant_summaries(query=query, top_k=self.similarity_top_k)\n",
    "                relevant_summary_messag = Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")\n",
    "                summary_context = \"\\n\".join(relevant_summary)\n",
    "            else:\n",
    "                summary_context = \"\"\n",
    "            \n",
    "            relevant_messages = [Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")]\n",
    "            game_log.append(f\"Summary context: {summary_context}\")\n",
    "\n",
    "            # prepare prompt with context set by retrieved summaries\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")\n",
    "                or x,  # type: ignore[arg-type]\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "        else:\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=1)\n",
    "\n",
    "            game_log.append(f\"Relevant messages: {\"\\n\".join([msg.content for msg in relevant_messages])}\")\n",
    "\n",
    "            # prepare prompt with retrieved messages similar to input message\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_messages\n",
    "                or x,  # type: ignore[arg-type]\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "\n",
    "        regular_agent_uncertain = False\n",
    "        raw_response = None\n",
    "\n",
    "        # Uncertainty score\n",
    "        if self.reflect_before_vectorstore:\n",
    "\n",
    "            # use regular decision making and collect outputted uncertainty of decision\n",
    "            non_lawyer_judge_response = self.model(prompt)\n",
    "            data = json.loads(non_lawyer_judge_response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "            uncertainty =  float(data.get(\"uncertainty_score\", 0.0))\n",
    "\n",
    "            if uncertainty < 0.6:   \n",
    "                game_log.append(\"Regular agent is not satisfied with their certainty. Applying lawyer judge architecture...\")\n",
    "\n",
    "                raw_response = non_lawyer_judge_response\n",
    "                regular_agent_uncertain = True\n",
    "\n",
    "        if regular_agent_uncertain or not self.reflect_before_vectorstore:\n",
    "            \n",
    "            # Initialize lawyers and judge\n",
    "            lawyer_1 = Lawyer(\n",
    "                f'{self.name} (Lawyer 1)',\n",
    "                self.sys_prompt,\n",
    "                self.model_config_name,\n",
    "                self.parser,\n",
    "                relevant_messages,              \n",
    "                self.memory\n",
    "            )\n",
    "\n",
    "            lawyer_2 = Lawyer(\n",
    "                f'{self.name} (Lawyer 2)',\n",
    "                self.sys_prompt,\n",
    "                self.model_config_name,\n",
    "                self.parser,\n",
    "                relevant_messages,\n",
    "                self.memory\n",
    "            )\n",
    "\n",
    "            judge = Judge(\n",
    "                f'{self.name} (Judge)',\n",
    "                self.sys_prompt,\n",
    "                self.model_config_name,\n",
    "                self.parser,\n",
    "                relevant_messages,\n",
    "                self.memory\n",
    "            )\n",
    "        \n",
    "            available_targets = [player for player in game_state[\"survivors\"] if player not in game_state[\"werewolves\"]]\n",
    "\n",
    "            lawyer_1_argument = lawyer_1.argue(game_state, available_targets)\n",
    "            available_targets.remove(lawyer_1_argument[\"target\"])\n",
    "            lawyer_2_argument = lawyer_2.argue(game_state, available_targets)\n",
    "\n",
    "            raw_response = judge.decide(game_state, lawyer_1_argument, lawyer_2_argument)\n",
    "\n",
    "            game_log.append(f\"Lawyer 1 argument: {lawyer_1_argument}\")\n",
    "            game_log.append(f\"Lawyer 2 argument: {lawyer_2_argument}\")\n",
    "            game_log.append(f\"Judge decision: {raw_response.text}\")\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parse response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "\n",
    "        # Save decision to memory\n",
    "        self.memory.add_message(message=msg)\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"Store input messages in memory without responding.\n",
    "\n",
    "        Args:\n",
    "            x (`Union[Msg, Sequence[Msg]]`): The message(s) to be recorded in memory.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "    def summarize_cycle(self, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary at the end of a day or night cycle.\n",
    "        \n",
    "        Args:\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        self.memory.summarize_cycle(cycle_type=cycle_type)\n",
    "\n",
    "            \n",
    "            \n",
    "class NormalAgent(AgentBase):\n",
    "    \"\"\"An agent that generates response in a dict format, where user can\n",
    "    specify the required fields in the response via specifying the parser\n",
    "    \n",
    "    Includes a FAISS vectorstore for memory\n",
    "\n",
    "    About parser, please refer to our\n",
    "    [tutorial](https://modelscope.github.io/agentscope/en/tutorial/203-parser.html)\n",
    "\n",
    "    For usage example, please refer to the example of werewolf in\n",
    "    `examples/game_werewolf`\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the custom agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            sys_prompt (`Optional[str]`, defaults to `None`):\n",
    "                The system prompt of the agent, which can be passed by args\n",
    "                or hard-coded in the agent.\n",
    "            model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load model from\n",
    "                configuration.\n",
    "\n",
    "            max_retries (`Optional[int]`, defaults to `None`):\n",
    "                The maximum number of retries when failed to parse the model\n",
    "                output.\n",
    "                \n",
    "            similarity_top_k (`int`, defaults to `None`):\n",
    "                The top k most similar items in the vectorstore that will be\n",
    "                used in the context to the model\n",
    "        \"\"\"  # noqa\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        openai.api_key = openai_api_key\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which will provide 1) format instruction; 2)\n",
    "        response parsing; 3) filtering fields when returning message, storing\n",
    "        message in memory. So developers only need to change the\n",
    "        parser, and the agent will work as expected.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Reply function of the agent.\n",
    "        Processes the input data, generates a prompt using the current\n",
    "        dialogue memory and system prompt, and invokes the language\n",
    "        model to produce a response. The response is then formatted\n",
    "        and added to the dialogue memory.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        global game_log\n",
    "\n",
    "        # Add the input message to memory\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "        query = x.content if x is not None else \"\"\n",
    "\n",
    "        relevant_messages = self.memory.get_relevant_messages(query=query, top_k=1)\n",
    "\n",
    "        game_log.append(f\"Relevant messages from vectorstore: {\"\\n\".join([msg.content for msg in relevant_messages])}\")\n",
    "\n",
    "        # prepare prompt with retrieved messages similar to input message\n",
    "        prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            relevant_messages\n",
    "            or x,  # type: ignore[arg-type]\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # call llm\n",
    "        raw_response = self.model(prompt)\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parsing the raw response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "        \n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=msg)\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"Observe the input, store it in memory without response to it.\n",
    "\n",
    "        Args:\n",
    "            x (`Union[Msg, Sequence[Msg]]`):\n",
    "                The input message to be recorded in memory.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "            \n",
    "# Register your custom class\n",
    "agentscope.agents.NormalAgent = NormalAgent\n",
    "agentscope.agents.WerewolfAgent = WerewolfAgent\n",
    "agentscope.agents.Lawyer = Lawyer\n",
    "agentscope.agents.Judge = Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Parser (edit minimally)\n",
    "Below are the prompts that control and run the Werwolf game. They are implemented using the built in AgentScope MarkdownJsonDictParser which allows us to generate responses in a dictionary format that is compatible with our game. Currently structured to obtain both the private thoughts of an Agent which remain to themselves and showcase their reasoning, and the words that they speak to other agents.\n",
    "\n",
    "Since these prompts control the game logic itself, we should try to limit the prompt edits here to a minimum, but we can make edits to the parser to control the reasoning process a particular role goes through before making a decision, eg. instead of a thought field, we ask it to make ask it to provide a reason to vote for a particular agent, and a reason to not vote for a particular agent, then ask it to make a final decision based off those fields. Fields other than \"thought\" however should not be touched as they play a critical role to the control flow of the game.\n",
    "\n",
    "More details on the AgentScope parsers can be found here https://doc.agentscope.io/build_tutorial/structured_output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    \"\"\"Prompts for werewolf game\"\"\"\n",
    "\n",
    "    to_wolves = (\n",
    "        \"{}, if you are the only werewolf, eliminate a player. Otherwise, \"\n",
    "        \"discuss with your teammates and reach an agreement.\"\n",
    "    )\n",
    "\n",
    "    wolves_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"finish_discussion\": \"whether the discussion reached an agreement or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"finish_discussion\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"finish_discussion\"],\n",
    "    )\n",
    "\n",
    "    to_wolves_vote = \"Which player do you vote to kill?\"\n",
    "\n",
    "    wolves_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_wolves_res = \"The player with the most votes is {}.\"\n",
    "\n",
    "    to_witch_resurrect = (\n",
    "        \"{witch_name}, you're the witch. Tonight {dead_name} is eliminated. \"\n",
    "        \"Would you like to resurrect {dead_name}?\"\n",
    "    )\n",
    "\n",
    "    to_witch_resurrect_no = \"The witch has chosen not to resurrect the player.\"\n",
    "    to_witch_resurrect_yes = \"The witch has chosen to resurrect the player.\"\n",
    "\n",
    "    witch_resurrect_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"whether to resurrect the player and the reason\",\n",
    "            \"resurrect\": \"whether to resurrect the player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"resurrect\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"resurrect\"],\n",
    "    )\n",
    "\n",
    "    to_witch_poison = (\n",
    "        \"Would you like to eliminate one player? If yes, \"\n",
    "        \"specify the player_name.\"\n",
    "    )\n",
    "\n",
    "    witch_poison_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"eliminate\": \"whether to eliminate a player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"eliminate\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"eliminate\"],\n",
    "    )\n",
    "\n",
    "    to_seer = (\n",
    "        \"{}, you're the seer. Which player in {} would you like to check \"\n",
    "        \"tonight?\"\n",
    "    )\n",
    "\n",
    "    seer_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    to_seer_result = \"Okay, the role of {} is a {}.\"\n",
    "\n",
    "    to_all_danger = (\n",
    "        \"The day is coming, all the players open your eyes. Last night, \"\n",
    "        \"the following player(s) has been eliminated: {}.\"\n",
    "    )\n",
    "\n",
    "    to_all_peace = (\n",
    "        \"The day is coming, all the players open your eyes. Last night is \"\n",
    "        \"peaceful, no player is eliminated.\"\n",
    "    )\n",
    "\n",
    "    to_all_discuss = (\n",
    "        \"Now the alive players are {}. Given the game rules and your role, \"\n",
    "        \"based on the situation and the information you gain, to vote a \"\n",
    "        \"player eliminated among alive players and to win the game, what do \"\n",
    "        \"you want to say to others? You can decide whether to reveal your \"\n",
    "        \"role.\"\n",
    "    )\n",
    "\n",
    "    survivors_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    survivors_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_all_vote = (\n",
    "        \"Given the game rules and your role, based on the situation and the\"\n",
    "        \" information you gain, to win the game, it's time to vote one player\"\n",
    "        \" eliminated among the alive players. Which player do you vote to \"\n",
    "        \"kill?\"\n",
    "    )\n",
    "\n",
    "    to_all_res = \"{} has been voted out.\"\n",
    "\n",
    "    to_all_wolf_win = (\n",
    "        \"The werewolves have prevailed and taken over the village. Better \"\n",
    "        \"luck next time!\"\n",
    "    )\n",
    "\n",
    "    to_all_village_win = (\n",
    "        \"The game is over. The werewolves have been defeated, and the village \"\n",
    "        \"is safe once again!\"\n",
    "    )\n",
    "\n",
    "    to_all_continue = \"The game goes on.\"\n",
    "\n",
    "    \n",
    "# Moderator message function\n",
    "HostMsg = partial(Msg, name=\"Moderator\", role=\"assistant\", echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Initialization (edit here)\n",
    "To initialize the agents, you must define their parameters and settings in the config objects that are passed in for initialization. There is a model config, which defines the base foundational model being used, and an agent config, which defines each of the agents being used in the game, including which model their using, their name, and what type of Agent they are (based off the agent classes we defined earlier). \n",
    "\n",
    "Pay particular attention to the system prompt, this is what defines the rules of the game to the agent and gives them the role and what their responsibilities are, we could perhaps do some prompt engineering with that.\n",
    "\n",
    "Also we can play around with the settings of the game, eg. max rounds, how many werewolves we have, etc. Just make sure to update the roles, witch, seer objects below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds game data \n",
    "game_log = []\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (don't edit)\n",
    "Functions to check and update game state throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_winning(game_state: dict, alive_agents: list, wolf_agents: list, host: str) -> bool:\n",
    "    \"\"\"check which group wins\"\"\"\n",
    "    if len(wolf_agents) >= len(alive_agents):\n",
    "        msg = Msg(host, Prompts.to_all_wolf_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"werewolves\"\n",
    "        logger.chat(msg)\n",
    "        return True\n",
    "    if alive_agents and not wolf_agents:\n",
    "        msg = Msg(host, Prompts.to_all_village_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"villagers\"\n",
    "        logger.chat(msg)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_wolves_won(wolf_agents: list, alive_agents: list):\n",
    "    \"\"\"check if wolves win\"\"\"\n",
    "    return len(wolf_agents) * 2 >= len(alive_agents)\n",
    "\n",
    "def update_alive_players(\n",
    "    game_state: dict,\n",
    "    survivors: Sequence[AgentBase],\n",
    "    wolves: Sequence[AgentBase],\n",
    "    dead_names: Union[str, list[str]],\n",
    ") -> tuple[list, list]:\n",
    "    \"\"\"update the list of alive agents\"\"\"\n",
    "    if not isinstance(dead_names, list):\n",
    "        dead_names = [dead_names]\n",
    "    for player in dead_names:\n",
    "        for player in dead_names:\n",
    "            if player in game_state[\"survivors\"]:\n",
    "                game_state[\"survivors\"].remove(player)\n",
    "\n",
    "        if player not in game_state[\"dead\"]:\n",
    "            game_state[\"dead\"].append(player)  \n",
    "        else:\n",
    "            continue\n",
    "    return [_ for _ in survivors if _.name not in dead_names], [\n",
    "        _ for _ in wolves if _.name not in dead_names\n",
    "    ]\n",
    "\n",
    "\n",
    "def majority_vote(votes: list) -> Any:\n",
    "    \"\"\"majority_vote function\"\"\"\n",
    "    votes_valid = [item for item in votes if item != \"Abstain\"]\n",
    "    # Count the votes excluding abstentions.\n",
    "    unit, counts = np.unique(votes_valid, return_counts=True)\n",
    "    return unit[np.argmax(counts)]\n",
    "\n",
    "def save_logs_to_logfile(logs, filepath):\n",
    "    \"\"\" Saves a list of log messages to a .log file, each on a new line. \"\"\"\n",
    "    # Ensure the directory exists before writing\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "    # Write each log message to a new line in the .log file\n",
    "    with open(filepath, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        file.writelines(log + \"\\n\" for log in logs)\n",
    "\n",
    "    print(f\"Game log saved to {filepath}\")\n",
    "\n",
    "def save_results(win_rate_filepath, row):\n",
    "    \"\"\"\n",
    "    Appends a row to the CSV file containing win rates. If the file does not exist, it creates one with headers.\n",
    "    \n",
    "    Arguments:\n",
    "    - win_rate_filepath (str): The path to the CSV file.\n",
    "    - row (dict): A dictionary containing \"raw_log_filepath\" and \"custom_agent_won\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(win_rate_filepath)\n",
    "    \n",
    "    # Open file in append mode\n",
    "    with open(win_rate_filepath, mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"raw_log_filepath\", \"custom_agent_won\"])\n",
    "        \n",
    "        # If file does not exist, write the header first\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Write the row to the CSV file\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"Row added to {win_rate_filepath}: {row}\")\n",
    "\n",
    "def generate_log_filepath(game_num, basepath=\"logs/courtroom_raw\"):\n",
    "    \"\"\"Generates a .log filename in the format 'courtroom_raw_{game_num}_{iso_timestamp}.log'\"\"\"\n",
    "    iso_timestamp = datetime.utcnow().isoformat(timespec='seconds').replace(\":\", \"-\")\n",
    "    return f\"{basepath}_{game_num}_{iso_timestamp}.log\"\n",
    "\n",
    "def custom_agent_won(game_state):\n",
    "    return game_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Game (don't edit)\n",
    "Following is a function that game through the various Night and Day phases, taking different actions for each agent based on their roles. Multi agent functionality and communication is facilitated through the AgentScope Pipeline and MsgHub, more detailed documentation found here https://doc.agentscope.io/build_api/agentscope.pipelines.pipeline.html#module-agentscope.pipelines.pipeline and https://doc.agentscope.io/build_api/agentscope.msghub.html#module-agentscope.msghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(\n",
    "        game_num,\n",
    "        max_days_per_game,\n",
    "        raw_log_filepath,\n",
    "        win_rate_filepath,\n",
    "        reflect_before_vectorstore,\n",
    "        max_werewolf_discussion_round,\n",
    "        wolves,\n",
    "        seer,\n",
    "        witch,\n",
    "        roles,\n",
    "        survivors,\n",
    "        game_state\n",
    "    ):\n",
    "    global game_log\n",
    "    game_log = []  # Store data for CSV\n",
    "\n",
    "    for _ in range(1, max_days_per_game + 1):\n",
    "        # Werewolves discussion & voting\n",
    "        hint = HostMsg(content=Prompts.to_wolves.format(n2s(wolves)))\n",
    "        with msghub(wolves, announcement=hint) as hub:\n",
    "            game_log.append(f\"Moderator: {hint.content}\")\n",
    "            set_parsers(wolves, Prompts.wolves_discuss_parser)\n",
    "\n",
    "            for _ in range(max_werewolf_discussion_round):\n",
    "                x = sequentialpipeline(wolves)\n",
    "                game_log.append(f\"Werewolves Discussion: {x.content}\")\n",
    "                if x.metadata.get(\"finish_discussion\", False):\n",
    "                    break\n",
    "            # Werewolves vote\n",
    "            set_parsers(wolves, Prompts.wolves_vote_parser)\n",
    "            hint = HostMsg(content=Prompts.to_wolves_vote)\n",
    "            game_log.append(f\"Moderator: {hint.content}\")\n",
    "            votes = [extract_name_and_id(wolf.lawyer_judge_decision(game_state, hint).content)[0] for wolf in wolves]\n",
    "            dead_player = [majority_vote(votes)]\n",
    "            game_log.append(f\"Werewolves voted to eliminate: {dead_player[0]}\")\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_wolves_res.format(dead_player[0])))\n",
    "\n",
    "        # Witch decision night   \n",
    "        if witch in survivors:\n",
    "            hint = HostMsg(content=Prompts.to_witch_resurrect.format_map({\"witch_name\": witch.name, \"dead_name\": dead_player[0]}))\n",
    "            game_log.append(f\"Moderator: {hint.content}\")\n",
    "            set_parsers(witch, Prompts.witch_resurrect_parser)\n",
    "\n",
    "            response = witch(hint)\n",
    "            game_log.append(f\"Witch Decision: {response.content}\")\n",
    "\n",
    "            if response.metadata.get(\"resurrect\", False):\n",
    "                dead_player.pop()\n",
    "                game_state[\"witch_healing_used\"] = True\n",
    "                game_log.append(\"Moderator: The witch has chosen to resurrect the player.\")\n",
    "\n",
    "        # Seer decision night\n",
    "        if seer in survivors:\n",
    "            hint = HostMsg(content=Prompts.to_seer.format(seer.name, n2s(survivors)))\n",
    "            game_log.append(f\"Moderator: {hint.content}\")\n",
    "            set_parsers(seer, Prompts.seer_parser)\n",
    "\n",
    "            x = seer(hint)\n",
    "            game_log.append(f\"Seer Decision: {x.content}\")\n",
    "\n",
    "            role = \"werewolf\" if roles[extract_name_and_id(x.content)[1]] == \"werewolf\" else \"villager\"\n",
    "            game_log.append(f\"Moderator: The role of {extract_name_and_id(x.content)[0]} is {role}.\")\n",
    "\n",
    "        # Update survivors after night\n",
    "        survivors, wolves = update_alive_players(game_state, survivors, wolves, dead_player)\n",
    "\n",
    "        if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "            game_log.append(\"Moderator: The game is over.\")\n",
    "            game_state[\"endgame\"] = True\n",
    "            break\n",
    "\n",
    "        # update vectorstore summaries with summaries of the night\n",
    "        if reflect_before_vectorstore:\n",
    "            for wolf in wolves:\n",
    "                wolf.summarize_cycle(cycle_type=\"night\")\n",
    "\n",
    "        # Daytime discussion & voting\n",
    "        content = Prompts.to_all_danger.format(n2s(dead_player)) if dead_player else Prompts.to_all_peace\n",
    "        game_log.append(f\"Moderator: {content}\")\n",
    "\n",
    "        hints = [\n",
    "            HostMsg(content=content),\n",
    "            HostMsg(content=Prompts.to_all_discuss.format(n2s(survivors))),\n",
    "        ]\n",
    "\n",
    "        with msghub(survivors, announcement=hints) as hub:\n",
    "            set_parsers(survivors, Prompts.survivors_discuss_parser)\n",
    "            discussion = sequentialpipeline(survivors)\n",
    "            game_log.append(f\"Survivors Discussion: {discussion.content}\")\n",
    "\n",
    "            # Voting\n",
    "            set_parsers(survivors, Prompts.survivors_vote_parser)\n",
    "            hint = HostMsg(content=Prompts.to_all_vote.format(n2s(survivors)))\n",
    "            game_log.append(f\"Moderator: {hint.content}\")\n",
    "\n",
    "            votes = [extract_name_and_id(_.reply(hint).content)[0] for _ in survivors]\n",
    "            vote_res = majority_vote(votes)\n",
    "            game_log.append(f\"Daytime Vote Result: {vote_res}\")\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_res.format(vote_res)))\n",
    "\n",
    "            survivors, wolves = update_alive_players(game_state, survivors, wolves, vote_res)\n",
    "\n",
    "            if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "                game_log.append(\"Moderator: The game is over.\")\n",
    "                game_state[\"endgame\"] = True\n",
    "                break\n",
    "\n",
    "            # update vectorstore summaries with summaries of the day\n",
    "            if reflect_before_vectorstore:\n",
    "                for wolf in wolves:\n",
    "                    wolf.summarize_cycle(cycle_type=\"day\")\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_continue))\n",
    "\n",
    "    updated_raw_log_filepath = generate_log_filepath(game_num, raw_log_filepath)\n",
    "\n",
    "    # Record raw logs\n",
    "    save_logs_to_logfile(game_log, updated_raw_log_filepath)\n",
    "\n",
    "    row = {\n",
    "        \"raw_log_filepath\": updated_raw_log_filepath,\n",
    "        \"custom_agent_won\": check_wolves_won(survivors, wolves)\n",
    "    }\n",
    "\n",
    "    # Record results\n",
    "    save_results(win_rate_filepath, row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    open_ai_key,\n",
    "    raw_log_filepath,\n",
    "    win_rate_filepath,\n",
    "    max_days_per_game=10,\n",
    "    num_games=10,\n",
    "    reflect_before_vectorstore=True,\n",
    "    use_default_agents=True,\n",
    "    primary_model=\"gpt-4o-mini\",\n",
    "    primary_model_params={},\n",
    "    secondary_model=\"gpt-4o\",\n",
    "    secondary_model_params={},\n",
    "    max_werewolf_discussion_round=3\n",
    "):  \n",
    "    # GAME INITIALIZATION\n",
    "    \n",
    "    # define model configs and settings\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": primary_model,\n",
    "            \"model_name\": primary_model,\n",
    "            \"api_key\": open_ai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # define the config settings for each agent involved\n",
    "    agent_configs = [\n",
    "        {\n",
    "            \"class\": \"WerewolfAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player1\",\n",
    "                \"sys_prompt\": \"Act as a player in a werewolf game. You are Player1 and\\nthere are totally 4 players, named Player1, Player2, Player3, Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing werewolf in this game.\\n\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player2\",\n",
    "                \"sys_prompt\": \"Act as a player in a werewolf game. You are Player2 and\\nthere are totally 4 players, named Player1, Player2, Player3, Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing villager in this game.\\n\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player3\",\n",
    "                \"sys_prompt\": \"Act as a player in a werewolf game. You are Player3 and\\nthere are totally 4 players, named Player1, Player2, Player3, Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing seer in this game.\\n\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player4\",\n",
    "                \"sys_prompt\": \"Act as a player in a werewolf game. You are Player4 and\\nthere are totally 4 players, named Player1, Player2, Player3, Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing witch in this game.\\n\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # RUNNING GAME\n",
    "\n",
    "    # read model and agent configs, and initialize agents automatically\n",
    "    survivors = agentscope.init(\n",
    "        model_configs=model_configs,\n",
    "        agent_configs=agent_configs,\n",
    "        project=\"Werewolf\",\n",
    "    )\n",
    "\n",
    "    # update these accordingly based off agent configs\n",
    "    roles = [\"werewolf\", \"villager\", \"seer\", \"witch\"]\n",
    "    wolves, witch, seer = [survivors[0]], survivors[-1], survivors[-2]\n",
    "\n",
    "    # initialize game state\n",
    "    game_state = {\n",
    "        \"werewolves\": [player.name for player in wolves],\n",
    "        \"villagers\": [player.name for player in survivors if player not in wolves],\n",
    "        \"seer\": [seer.name],\n",
    "        \"witch\": [witch.name],\n",
    "        \"survivors\": [player.name for player in survivors],\n",
    "        \"dead\": [],\n",
    "        \"witch_healing_used\": False,\n",
    "        \"witch_poison_used\": False,\n",
    "        \"endgame\": False,\n",
    "        \"winner\": None\n",
    "        \n",
    "    }\n",
    "\n",
    "    for game_num in range(1, num_games + 1):\n",
    "        run_game(\n",
    "            game_num,\n",
    "            max_days_per_game,\n",
    "            raw_log_filepath,\n",
    "            win_rate_filepath,\n",
    "            reflect_before_vectorstore,\n",
    "            max_werewolf_discussion_round,\n",
    "            wolves,\n",
    "            seer,\n",
    "            witch,\n",
    "            roles,\n",
    "            survivors,\n",
    "            game_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to convince the others to trust me and not suspect I'm a werewolf. I should suggest targeting Player3 as they seem a bit too quiet.\", \"speak\": \"I think we should consider eliminating Player3. They've been pretty quiet and haven't contributed much to the discussion. What do you all think?\", \"finish_discussion\": \"false\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"After evaluating both arguments, I find that the case against Player3 is more compelling. Player3 has been accused of shifting blame and aligning with the majority, which raises concerns about their true intentions. Eliminating Player3 could help the village identify any further werewolf activity, especially since Player1 is already known to be a werewolf. On the other hand, while Player4 has also shown suspicious behavior, their accusations may be more of a strategy to divert attention from themselves. Therefore, it is strategically advantageous to eliminate Player3 to reduce the risk of deception within the group.\", \"vote\": \"Player3\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player3.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 could have been a valuable asset if they were the seer or had useful information, but I need to consider the overall strategy. If Player3 was a villager, bringing them back might help us deduce who the werewolves are. However, if they were a werewolf, it could jeopardize our chances.\",\"speak\": \"I will not resurrect Player3 because we need to focus on identifying the werewolves, and bringing back a player could complicate our strategy.\",\"resurrect\": false}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to help the villagers identify the werewolves. I should check one of the players who seem suspicious or are leading the discussions.\",\"speak\": \"Player1\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night, the following player(s) has been eliminated: Player3.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2 and Player4. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be careful not to reveal my identity as a werewolf. I should try to blend in and point suspicion towards the other players without drawing attention to myself.\", \"speak\": \"I think we should discuss who we suspect might be the werewolves. Player2 has been pretty quiet, which makes me wonder if they might be hiding something.\"}\n",
      "```\n",
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"Player1 is trying to shift suspicion onto me. I need to defend myself and keep the focus on identifying the werewolves.\", \"speak\": \"I understand your concern, Player1, but I believe we should look at everyone’s behavior. It’s important to consider how each player has acted so far. Let's not jump to conclusions.\"}\n",
      "```\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be careful with my words and actions. As the witch, I have powerful abilities that could turn the tide of the game, but revealing too much could make me a target. I should subtly guide the discussion while keeping my identity hidden.\", \"speak\": \"I think it would be wise to consider everyone's actions last night. We should discuss who seemed suspicious and why. It's crucial we don't make hasty decisions.\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to create some confusion and deflect suspicion away from myself. Player2 has been very quiet and not contributing much to the discussion, making them a good target to eliminate without raising too many questions. Plus, if they are a villager, it will help reduce the number of votes against me later.\", \"vote\": \"Player2\"}\n",
      "```\n",
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I think Player3 has been acting a bit suspiciously. They haven't contributed much to the discussion and seem to be avoiding direct accusations. It could be a tactic to blend in as a villager. We should consider eliminating them to see if we can flush out the werewolves.\", \"vote\": \"Player3\"}\n",
      "```\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious and strategic in my voting. Since I am the witch, I have the ability to save someone if needed. However, I also need to eliminate a potential werewolf. Based on the discussions, Player1 seems suspicious and has been pointing fingers at others without much evidence. I will vote for Player1 to see if they are indeed a werewolf.\", \"vote\": \"Player1\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Player1 has been voted out.\n",
      "\u001b[93mModerator\u001b[0m: The game is over. The werewolves have been defeated, and the village is safe once again!\n",
      "Game log saved to logs/courtroom_raw_1_2025-02-05T06-35-02.log\n",
      "Row added to test: {'raw_log_filepath': 'logs/courtroom_raw_1_2025-02-05T06-35-02.log', 'custom_agent_won': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/dps6jwgd4px6n3_9j_rfrhvw0000gn/T/ipykernel_1814/2651660822.py:89: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  iso_timestamp = datetime.utcnow().isoformat(timespec='seconds').replace(\":\", \"-\")\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    open_ai_key=openai_key,\n",
    "    raw_log_filepath=\"logs/courtroom_raw\",\n",
    "    reflect_before_vectorstore=False,\n",
    "    win_rate_filepath=\"test\",\n",
    "    max_days_per_game=10,\n",
    "    num_games=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
