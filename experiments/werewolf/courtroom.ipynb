{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf Simulator\n",
    "The following runs a simulation of the game Werewolf with multiple agents, it is built on top of and uses the Agentscope library, documentation can be found here https://doc.agentscope.io/, and the open source code that we extend on is here https://github.com/modelscope/agentscope/tree/main.\n",
    "### Rules of Werewolf\n",
    "- There are 4 roles, the Werewolves, Villagers, Witch, and Seer (Witch and Seer are on the Villager team)\n",
    "- Each night:\n",
    "    - The Werewolves discuss and vote on a player to eliminate\n",
    "    - The Witch is told what player the wolves voted to eliminate, and is given the choice to use their potion of healing to reserruct the eliminated player, or use their poison to eliminate another player. Note that each power can only be used once in a game\n",
    "    - The seer can pick any other player and find out what their role is (one player per night)\n",
    "- After the events of the night, all surviving players discuss amongst themselves and vote on a player to eliminate\n",
    "- Werewolves win if their numbers equal or exceed Villagers.\n",
    "- Villagers win if all Werewolves are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting agentscope==0.1.1 (from -r requirements.txt (line 1))\n",
      "  Downloading agentscope-0.1.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting faiss-cpu==1.9.0 (from -r requirements.txt (line 2))\n",
      "  Downloading faiss_cpu-1.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Collecting networkx (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting black (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docstring-parser (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pydantic in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.10.6)\n",
      "Collecting loguru==0.6.0 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tiktoken in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.9.0)\n",
      "Collecting Pillow (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: requests in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting inputimeout (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading inputimeout-1.0.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.26.4)\n",
      "Collecting Flask==3.0.0 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting Flask-Cors==4.0.0 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting Flask-SocketIO==5.3.6 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading Flask_SocketIO-5.3.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting flask-sqlalchemy (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading flask_sqlalchemy-3.1.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting flake8 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading flake8-7.1.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (7.0.0)\n",
      "Collecting scipy (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai>=1.3.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.63.0)\n",
      "Collecting dashscope>=1.19.0 (from agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading dashscope-1.22.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from faiss-cpu==1.9.0->-r requirements.txt (line 2)) (24.2)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Jinja2>=3.1.2 (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (8.1.8)\n",
      "Collecting blinker>=1.6.2 (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting python-socketio>=5.0.2 (from Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading python_socketio-5.12.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: aiohttp in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.11.12)\n",
      "Requirement already satisfied: websocket-client in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.0)\n",
      "Collecting pathspec>=0.9.0 (from black->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (4.3.6)\n",
      "Collecting mccabe<0.8.0,>=0.7.0 (from flake8->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pycodestyle<2.13.0,>=2.12.0 (from flake8->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting pyflakes<3.3.0,>=3.2.0 (from flake8->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.16 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.38)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from tiktoken->agentscope==0.1.1->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.14.0)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting bidict>=0.21.0 (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.11.0 (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading python_engineio-4.11.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from sqlalchemy>=2.0.16->flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/mdvmlhtr/political-chatbot/.venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.18.3)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1))\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading agentscope-0.1.1-py3-none-any.whl (943 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.2/943.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
      "Downloading Flask_SocketIO-5.3.6-py3-none-any.whl (18 kB)\n",
      "Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dashscope-1.22.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading black-25.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading flake8-7.1.2-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask_sqlalchemy-3.1.1-py3-none-any.whl (25 kB)\n",
      "Downloading inputimeout-1.0.4-py3-none-any.whl (4.6 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
      "Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_socketio-5.12.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading python_engineio-4.11.2-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, scipy, pyflakes, pycodestyle, Pillow, pathspec, networkx, mccabe, MarkupSafe, loguru, itsdangerous, inputimeout, faiss-cpu, docstring-parser, blinker, bidict, Werkzeug, simple-websocket, Jinja2, flake8, black, python-engineio, Flask, dashscope, python-socketio, flask-sqlalchemy, Flask-Cors, Flask-SocketIO, agentscope\n",
      "  Attempting uninstall: faiss-cpu\n",
      "    Found existing installation: faiss-cpu 1.10.0\n",
      "    Uninstalling faiss-cpu-1.10.0:\n",
      "      Successfully uninstalled faiss-cpu-1.10.0\n",
      "Successfully installed Flask-3.0.0 Flask-Cors-4.0.0 Flask-SocketIO-5.3.6 Jinja2-3.1.5 MarkupSafe-3.0.2 Pillow-11.1.0 Werkzeug-3.1.3 agentscope-0.1.1 bidict-0.23.1 black-25.1.0 blinker-1.9.0 dashscope-1.22.1 docstring-parser-0.16 faiss-cpu-1.9.0 flake8-7.1.2 flask-sqlalchemy-3.1.1 inputimeout-1.0.4 itsdangerous-2.2.0 loguru-0.6.0 mccabe-0.7.0 networkx-3.4.2 pathspec-0.12.1 pycodestyle-2.12.1 pyflakes-3.2.0 python-engineio-4.11.2 python-socketio-5.12.1 scipy-1.15.2 simple-websocket-1.1.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional, Union, Sequence, Any, List\n",
    "from functools import partial\n",
    "import openai  \n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import sys\n",
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from agentscope.parsers.json_object_parser import MarkdownJsonDictParser\n",
    "from agentscope.parsers import ParserBase\n",
    "from agentscope.message import Msg\n",
    "from agentscope.msghub import msghub\n",
    "from agentscope.agents import AgentBase\n",
    "from agentscope.memory.temporary_memory import TemporaryMemory\n",
    "from agentscope.pipelines.functional import sequentialpipeline\n",
    "import agentscope\n",
    "\n",
    "from utils.werewolf_utils import (\n",
    "    extract_name_and_id,\n",
    "    n2s,\n",
    "    set_parsers,\n",
    "    generate_model_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging utilities\n",
    "class StructuredLogger:\n",
    "    \"\"\"\n",
    "    A logger that outputs to TSV files for structured logging of LLM interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define all possible message types for documentation\n",
    "    MESSAGE_TYPES = [\n",
    "        \"vectorstore_add\",           # Adding a case to vectorstore\n",
    "        \"vectorstore_retrieve\",      # Retrieving similar cases\n",
    "        \"vectorstore_summarize\",     # Summarizing cached decisions\n",
    "        \"pro_argument_prompt\",       # Input prompt for pro arguments\n",
    "        \"pro_argument_response\",     # Response from pro argument LLM\n",
    "        \"con_argument_prompt\",       # Input prompt for con arguments\n",
    "        \"con_argument_response\",     # Response from con argument LLM\n",
    "        \"judge_prompt\",              # Input prompt for judge\n",
    "        \"judge_response\",            # Response from judge LLM\n",
    "        \"fast_prompt\",               # Input prompt for fast LLM\n",
    "        \"fast_response\",             # Fast LLM respnose\n",
    "        \"slow_prompt\",               # Input prompt for slow LLM\n",
    "        \"slow_response\",             # Slow LLM response\n",
    "\n",
    "        # Werewolf-specific messages\n",
    "        \"step_num\",                  # Step number in game\n",
    "        \"game_init\",                 # Game initialization config\n",
    "    ]\n",
    "\n",
    "    def __init__(self, tsv_filepath: str):\n",
    "        \"\"\"\n",
    "        Initialize the logger with a base filepath.\n",
    "\n",
    "        Args:\n",
    "            tsv_filepath: Filepath for TSV log files (includes timestamp)\n",
    "        \"\"\"\n",
    "        self.tsv_filepath = tsv_filepath\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.tsv_filepath), exist_ok=True)\n",
    "\n",
    "        # Initialize TSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(self.tsv_filepath):\n",
    "            with open(self.tsv_filepath, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "                writer = csv.writer(tsvfile, delimiter=\"\\t\")\n",
    "                writer.writerow([\"timestamp\", \"message_type\", \"message\"])\n",
    "\n",
    "    def log(self, message_type: str, message_data: Union[str, Dict[str, Any], list[Any], BaseModel]) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to the TSV file.\n",
    "\n",
    "        Args:\n",
    "            message_type: Type of message (one of MESSAGE_TYPES)\n",
    "            message_data: Data to be logged (will be converted to JSON if not already a string)\n",
    "        \"\"\"\n",
    "        if message_type not in self.MESSAGE_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"Invalid message_type: {message_type}. Must be one of {self.MESSAGE_TYPES}\"\n",
    "            )\n",
    "\n",
    "        # Convert message_data to string if it's not already\n",
    "        if isinstance(message_data, BaseModel):\n",
    "            message = json.dumps(message_data.model_dump(), ensure_ascii=False)\n",
    "        elif isinstance(message_data, (dict, list)):\n",
    "            message = json.dumps(message_data, ensure_ascii=False)\n",
    "        else:\n",
    "            message = str(message_data)\n",
    "\n",
    "        timestamp = datetime.now().isoformat()\n",
    "\n",
    "        # Append to TSV file\n",
    "        with open(self.tsv_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter=\"\\t\", quotechar='`', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([timestamp, message_type, message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log configuration\n",
    "LOG_LEVEL = logging.INFO\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Initial logging setup\n",
    "logging.basicConfig(\n",
    "    filename=None,\n",
    "    level=LOG_LEVEL,\n",
    "    format=LOG_FORMAT,\n",
    "    datefmt=DATE_FORMAT\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def change_log_file(new_filename: str):\n",
    "    \"\"\"\n",
    "    Updates the logger handler to point to a different log file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    log_dir = os.path.dirname(new_filename)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)  # Create directories if they don't exist\n",
    "\n",
    "    # Remove all old handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    new_handler = logging.FileHandler(new_filename)\n",
    "    new_handler.setLevel(LOG_LEVEL)  # Reset level\n",
    "    new_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))  # Reset formatter\n",
    "\n",
    "    # Add new handler to logger\n",
    "    logger.addHandler(new_handler)\n",
    "\n",
    "def log_params(log_filepath: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Writes all parameters passed to the function into a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        log_filepath (str): Path where the JSON file will be written.\n",
    "        **kwargs: Any number of keyword arguments representing parameter names and values.\n",
    "    \"\"\"\n",
    "    with open(log_filepath, \"w\") as file:\n",
    "        json.dump(kwargs, file, separators=(\",\", \":\"))  # Compact JSON\n",
    "        file.write(\"\\n\")  # Ensures a newline at the end\n",
    "\n",
    "    print(f\"Parameters written to {log_filepath}\")\n",
    "\n",
    "def log_vectorstore_retrieval(query: str, similar_documents: str):\n",
    "    \"\"\"Logs the most similar documents from vectorstore\"\"\"\n",
    "\n",
    "    logger.info(\"===\")\n",
    "    logger.info(f\"Query: {query}\")\n",
    "    logger.info(f\"Retrieved similar documents: {similar_documents}\")\n",
    "    logger.info(\"===\")\n",
    "\n",
    "def log_message(msg: Msg):\n",
    "    \"\"\"Logs a message object\"\"\" \n",
    "\n",
    "    logger.info(msg.name + \": \" + msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementations\n",
    "Vector store classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        \n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            self.messages.append(msg.name + \": \" + msg.content)\n",
    "        \n",
    "    def summarize_cycle(self, survivors: list, secondary_model: str, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            survivors (list): List of current alive players.\n",
    "            secondary_model (str): Name of secondary model.\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Combine all messages since the last summary\n",
    "        history = \"\\n\".join(self.messages)\n",
    "\n",
    "        # Use retry_with_fallback with a lambda function\n",
    "        secondary_model_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(\n",
    "                model=secondary_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"\\n\".join([ \n",
    "                        f\"You are a strategic decision-maker reviewing past decisions in Werewolf.\",\n",
    "                        f\"Survivors: {', '.join([survivor.name for survivor in survivors])}\",\n",
    "                        f\"Summarize the decisions and reflect on the most important implications for future strategic decisions.\"\n",
    "                    ])},\n",
    "                    {\"role\": \"user\", \"content\": history},\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summary = secondary_model_response.text\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        # Clear messages for the next cycle\n",
    "        self.messages.clear()\n",
    "\n",
    "    def get_relevant_summaries_context(self, query: str, top_k: int = 1) -> str:\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        if len(self.summaries) > 0:\n",
    "            query_embedding = self._get_embedding(query)\n",
    "            distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "            results = [\n",
    "                self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "            ]\n",
    "            return \"\\n\".join(list(set(results)))\n",
    "        return \"\"\n",
    "    \n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join(\"vectorstore\", \"courtroom\", timestamp, str(game_num))\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(dir_path, f\"{player_role}.log\")\n",
    "\n",
    "        # Save messages to the log file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(\"\\n\".join(self.summaries))\n",
    "\n",
    "        print(f\"Messages saved to {file_path}\")\n",
    "\n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        # add embedding of message to vector store\n",
    "        embeddings = [self._get_embedding(msg.name + \": \" + msg.content) for msg in message]\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.messages.extend(message)\n",
    "\n",
    "    def get_relevant_messages(self, query: str, top_k: int = 10) -> list:\n",
    "        \"\"\"Retrieve the top-k most relevant messages.\"\"\"\n",
    "        if not self.messages:\n",
    "            return []\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "\n",
    "        results = [\n",
    "            self.messages[idx] for idx in indices[0] if idx < len(self.messages)\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join(\"vectorstore\", \"courtroom\", timestamp, str(game_num))\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(dir_path, f\"{player_role}.log\")\n",
    "\n",
    "        # Save messages to the log file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(convert_messages_to_string(self.messages))\n",
    "\n",
    "        print(f\"Messages saved to {file_path}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agents (edit here)\n",
    "You can define custom Agents by inheriting from the AgentBase class like shown here.\n",
    "\n",
    "Below is a copy of the DictDialogAgent that generates responses in a dict format that is compatible with our simulator. More documentation on AgentScope agents can be found here https://doc.agentscope.io/build_tutorial/builtin_agent.html, existing agent implementations can be found here https://github.com/modelscope/agentscope/tree/main/src/agentscope/agents.\n",
    "\n",
    "For our case, modifying the agent class is, in conjunction with the parsers we pass in (more details in next cell), play a critical role in defining our agent behavior. We can edit this section by either defining a brand new agent type we want to explore, or modifying the current one. The primary (and mostly only) source of focus should be the reply the function in the agent class as that controls what an agent uses to generate a response, particular when we are working with different architectures, eg. how we use memory, single vs multi agent, we make those edits here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lawyer(AgentBase):\n",
    "    \"\"\"An agent specializing in constructing LLM-based arguments to eliminate a specific target in the Werewolf game.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Lawyer agent.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the Lawyer agent.\n",
    "            sys_prompt (str): System prompt containing role instructions and context.\n",
    "            model_config_name (str): Identifier for the model configuration that determines the LLM to be used.\n",
    "            parser (ParserBase): Object responsible for formatting LLM outputs and extracting structured responses.\n",
    "            context_messages (list): A list of prior messages providing context for the lawyer’s argumentation.\n",
    "            memory: Memory management component used to store conversation history or additional data.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "        self.model_config_name = model_config_name\n",
    "\n",
    "    def argue(self, available_targets: list) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a rationale for eliminating a particular player in the Werewolf game.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict): The current state of the game (e.g., who is alive, who is dead).\n",
    "            available_targets (list): List of potential targets that can be selected for elimination.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                A dictionary containing:\n",
    "                  - \"target\" (str): The chosen player to eliminate.\n",
    "                  - \"argument\" (str): The Lawyer’s supporting reasoning.\n",
    "        \"\"\"\n",
    "        if not available_targets:\n",
    "            return {\"target\": None, \"argument\": \"No valid targets available.\"}\n",
    "        \n",
    "        target = random.choice(available_targets) # Select random a target to argue for\n",
    "\n",
    "        # Construct prompt for argument generation\n",
    "        lawyer_prompt_with_target = Prompts.lawyer_prompt.format(target)\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            Msg(\"system\", lawyer_prompt_with_target, role=\"system\"),\n",
    "            self.context_messages,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call the LLM to generate an argument\n",
    "        raw_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(self.model_config_name, formatted_prompt)\n",
    "        )\n",
    "\n",
    "        parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"argument\": parsed_response\n",
    "        }\n",
    "\n",
    "class Judge(AgentBase):\n",
    "    \"\"\"An agent that evaluates competing arguments and selects who should be eliminated in the Werewolf game.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        parser,\n",
    "        context_messages: list,\n",
    "        memory\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Judge agent.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the Judge agent.\n",
    "            sys_prompt (str): System prompt containing role instructions and context.\n",
    "            model_config_name (str): Identifier for the model configuration that determines the LLM to be used.\n",
    "            parser (ParserBase): Object responsible for formatting LLM outputs and extracting structured responses.\n",
    "            context_messages (list): List of prior messages providing context for the Judge’s decision-making.\n",
    "            memory: Memory component used to store and retrieve conversation or historical data.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "        self.parser = parser\n",
    "        self.context_messages = context_messages\n",
    "        self.memory = memory\n",
    "        self.model_config_name = model_config_name\n",
    "\n",
    "    def decide(self, game_state: dict, argument_1: dict, argument_2: dict):\n",
    "        \"\"\"\n",
    "        Evaluate two lawyers' arguments and choose a target for elimination.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict): The current state of the Werewolf game (e.g., which players are alive or dead).\n",
    "            argument_1 (dict):\n",
    "                First lawyer’s argument dictionary with:\n",
    "                  - \"target\" (str): Proposed player to eliminate.\n",
    "                  - \"argument\" (str): Reasoning for that choice.\n",
    "            argument_2 (dict): Second lawyer’s argument dictionary with the same keys as argument_1.\n",
    "\n",
    "        Returns:\n",
    "            The raw response from the model, containing the Judge’s final decision.\n",
    "        \"\"\"\n",
    "        target_1 = argument_1[\"target\"]\n",
    "        argument_1 = argument_1[\"argument\"]\n",
    "        \n",
    "        target_2 = argument_2[\"target\"]\n",
    "        argument_2 = argument_2[\"argument\"]\n",
    "\n",
    "        # Construct formatted judge prompt\n",
    "        formatted_judge_prompt = Prompts.judge_prompt.format(game_state[\"survivors\"], game_state[\"dead\"], game_state[\"werewolves\"], target_1, argument_1, target_2, argument_2)\n",
    "\n",
    "        # Prepare the prompt for the model\n",
    "        formatted_prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            Msg(\"system\", formatted_judge_prompt, role=\"system\"),\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        raw_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(self.model_config_name, formatted_prompt)\n",
    "        )\n",
    "\n",
    "        return raw_response\n",
    "\n",
    "class WerewolfAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    Represents a \"werewolf\" in the game, producing responses in dict form.\n",
    "\n",
    "    Optionally uses reflective memory before retrieving context from a FAISS vectorstore.\n",
    "    Contains parsing capabilities to ensure structured outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the WerewolfAgent.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the agent.\n",
    "            sys_prompt (str): System prompt providing role context and instructions.\n",
    "            model_config_name (str): Name of the model configuration, indicating which LLM will be used.\n",
    "            reflect_before_vectorstore (bool): Whether to summarize or reflect on memory before retrieving vectorstore data.\n",
    "            similarity_top_k (int, optional): The number of most similar messages/summaries to retrieve from the vectorstore.\n",
    "            openai_api_key (str, optional): API key for OpenAI integration.\n",
    "            model: The underlying LLM for generating responses.\n",
    "            parser (ParserBase): Parser responsible for structuring, filtering, and formatting model outputs.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.model_config_name = model_config_name\n",
    "\n",
    "        # Set OpenAI API key\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        # Initialize FAISS-based memory store\n",
    "        if reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"\n",
    "        Set the parser for handling model outputs.\n",
    "\n",
    "        This parser governs how responses are formatted, parsed,\n",
    "        and stored (including any filtering of fields).\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Generate a response based on input messages, optionally retrieving context from memory.\n",
    "\n",
    "        Args:\n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional):\n",
    "                The input message(s) to process.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                The assistant's structured output, including any metadata defined by the parser.\n",
    "        \"\"\" \n",
    "        query = Queries.werewolf_discussion_query\n",
    "\n",
    "        # Retrieve relevant messages from memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            summary_context = self.memory.get_relevant_summaries_context(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "            log_vectorstore_retrieval(query, summary_context)\n",
    "\n",
    "            # Prepare prompt with context from retrieved summaries\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\"),\n",
    "                x,\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "        else:\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "            log_vectorstore_retrieval(query, convert_messages_to_string(relevant_messages))\n",
    "\n",
    "            # Prepare prompt with retrieved messages similar to input message\n",
    "            prompt = self.model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_messages and Msg(\"system\", f\"Relevant messages from memory: {relevant_messages}\", \"system\"),\n",
    "                x,\n",
    "                Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "            )\n",
    "\n",
    "        # Call the LLM\n",
    "        raw_response = retry_with_fallback(lambda: generate_model_response(self.model_config_name, prompt))\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        # Parse the raw response\n",
    "        parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=json.dumps(parsed_response),\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=vectorstore_entry)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed_response),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed_response),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def lawyer_judge_decision(self, game_state: dict, available_targets: list, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Make an elimination decision by invoking two Lawyers and one Judge.\n",
    "\n",
    "        Args:\n",
    "            game_state (dict): The current state of the Werewolf game (e.g., active players, werewolves).\n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional): Optionally, messages to be recorded in memory before decision-making.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                The final decision from the Judge as a structured message.\n",
    "        \"\"\"\n",
    "        query = Queries.werewolf_discussion_query \n",
    "\n",
    "        # Retrieve relevant messages from memory (summaries or direct messages)\n",
    "        if self.reflect_before_vectorstore:\n",
    "            summary_context = self.memory.get_relevant_summaries_context(query=query, top_k=self.similarity_top_k)\n",
    "            \n",
    "            relevant_messages = [Msg(name=\"system\", role=\"system\", content=f\"Summary of relevant past conversations: {summary_context}\")]\n",
    "\n",
    "            log_vectorstore_retrieval(query, summary_context)\n",
    "        else:\n",
    "            assert isinstance(query, str), f\"Expected a string, but got {type(query)}\"\n",
    "            relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "            log_vectorstore_retrieval(query, convert_messages_to_string(relevant_messages))\n",
    "            \n",
    "        # Initialize the lawyers and judge\n",
    "        lawyer_1 = Lawyer(\n",
    "            f'{self.name} (Lawyer 1)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,              \n",
    "            self.memory\n",
    "        )\n",
    "\n",
    "        lawyer_2 = Lawyer(\n",
    "            f'{self.name} (Lawyer 2)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,\n",
    "            self.memory\n",
    "        )\n",
    "\n",
    "        judge = Judge(\n",
    "            f'{self.name} (Judge)',\n",
    "            self.sys_prompt,\n",
    "            self.model_config_name,\n",
    "            self.parser,\n",
    "            relevant_messages,\n",
    "            self.memory\n",
    "        )\n",
    "    \n",
    "        lawyer_1_argument = lawyer_1.argue(available_targets)\n",
    "        available_targets.remove(lawyer_1_argument[\"target\"])\n",
    "        lawyer_2_argument = lawyer_2.argue(available_targets)\n",
    "\n",
    "        raw_response = judge.decide(game_state, lawyer_1_argument, lawyer_2_argument)\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        # Parse response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        logger.info(\"===\")\n",
    "        logger.info(f\"Lawyer 1 argument: {lawyer_1_argument}\")\n",
    "        logger.info(f\"Lawyer 2 argument: {lawyer_2_argument}\")\n",
    "        logger.info(f\"Judge decision: {res.parsed}\")\n",
    "        logger.info(\"===\")\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=\"\\n\".join([\n",
    "                f\"Argument 1: {lawyer_1_argument}\",\n",
    "                f\"Argument 2: {lawyer_2_argument}\",\n",
    "                f\"Decision: {res.parsed}\"\n",
    "            ]),\n",
    "            role=\"assistant\"\n",
    "        )\n",
    "\n",
    "        # Store final decision\n",
    "        self.memory.add_message(message=vectorstore_entry)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "\n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]], role: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Record incoming messages in memory without generating a reply.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]): The message(s) to be stored in memory for future context.\n",
    "            role (`str`, optional): Role of the observer\n",
    "        \"\"\"\n",
    "        if role == \"seer\" and x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "    def summarize_cycle(self, survivors: list, secondary_model: str, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Produce a summary of key events at the end of a day or night cycle.\n",
    "\n",
    "        Args:\n",
    "            survivors: (list): List of current survivors.\n",
    "            secondary_model (str): Name of secondary model.\n",
    "            cycle_type (str, optional): The type of cycle being summarized, either \"day\" or \"night\".\n",
    "        \"\"\"\n",
    "        self.memory.summarize_cycle(survivors, secondary_model, cycle_type=cycle_type)\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str=\"werewolf\"):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.save_context(timestamp, game_num, player_role)\n",
    "\n",
    "class NormalAgent(AgentBase):\n",
    "    \"\"\"\n",
    "    A general-purpose agent that returns structured responses in JSON/dict format.\n",
    "\n",
    "    Integrates with a FAISS vectorstore for context retrieval.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the NormalAgent.\n",
    "\n",
    "        Args:\n",
    "            name (str): Agent name.\n",
    "            sys_prompt (str): The system prompt containing the agent’s role instructions.\n",
    "            model_config_name (str): Indicates which model configuration the agent should use.\n",
    "            reflect_before_vectorstore (bool): Flag to determine if reflective memory is used before vectorstore retrieval.\n",
    "            similarity_top_k (int, optional): The number of top similar items from the vectorstore to include as context.\n",
    "            openai_api_key (str, optional): API key for integrating with OpenAI services.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        openai.api_key = openai_api_key\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.model_config_name = model_config_name\n",
    "        self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"\n",
    "        Configure the parser that dictates how outputs are formatted, parsed, and stored.\n",
    "\n",
    "        By adjusting the parser, you can control the entire pipeline of formatting and extraction\n",
    "        for the model's responses.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"\n",
    "        Generate a response based on provided input messages, using stored context as needed.\n",
    "\n",
    "        Args: \n",
    "            x (Optional[Union[Msg, Sequence[Msg]]], optional): The incoming user or system messages.\n",
    "\n",
    "        Returns:\n",
    "            Msg:\n",
    "                A structured message including the assistant's content and any parsed metadata.\n",
    "        \"\"\"\n",
    "        query = Queries.non_werewolf_discussion_query\n",
    "\n",
    "        # Retrieve similar messages from memory\n",
    "        relevant_messages = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "        log_vectorstore_retrieval(query, convert_messages_to_string(relevant_messages))\n",
    "\n",
    "        # Prepare the prompt with relevant context\n",
    "        prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            relevant_messages and Msg(\"system\", f\"Relevant messages from memory: {relevant_messages}\", \"system\"),\n",
    "            x,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call the LLM\n",
    "        raw_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(self.model_config_name, prompt)\n",
    "        )\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        # Parse the model's output\n",
    "        parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=json.dumps(parsed_response),\n",
    "            role=\"assistant\"\n",
    "        )\n",
    "\n",
    "        # Save the model's response to memory\n",
    "        self.memory.add_message(message=vectorstore_entry)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed_response),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed_response),\n",
    "        )\n",
    "        \n",
    "        return msg\n",
    "\n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]], role: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Record incoming messages in memory without generating a reply.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]): The message(s) to be stored in memory for future context.\n",
    "            role (`str`, optional): Role of the observer\n",
    "        \"\"\"\n",
    "        if role == \"seer\" and x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.save_context(timestamp, game_num, player_role)\n",
    "\n",
    "\n",
    "# Register your custom classes\n",
    "agentscope.agents.NormalAgent = NormalAgent\n",
    "agentscope.agents.WerewolfAgent = WerewolfAgent\n",
    "agentscope.agents.Lawyer = Lawyer\n",
    "agentscope.agents.Judge = Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Parser (edit minimally)\n",
    "Below are the prompts that control and run the Werwolf game. They are implemented using the built in AgentScope MarkdownJsonDictParser which allows us to generate responses in a dictionary format that is compatible with our game. Currently structured to obtain both the private thoughts of an Agent which remain to themselves and showcase their reasoning, and the words that they speak to other agents.\n",
    "\n",
    "Since these prompts control the game logic itself, we should try to limit the prompt edits here to a minimum, but we can make edits to the parser to control the reasoning process a particular role goes through before making a decision, eg. instead of a thought field, we ask it to make ask it to provide a reason to vote for a particular agent, and a reason to not vote for a particular agent, then ask it to make a final decision based off those fields. Fields other than \"thought\" however should not be touched as they play a critical role to the control flow of the game.\n",
    "\n",
    "More details on the AgentScope parsers can be found here https://doc.agentscope.io/build_tutorial/structured_output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    \"\"\"Prompts for werewolf game\"\"\"\n",
    "\n",
    "    # Lawyer & Judge Architecture\n",
    "\n",
    "    lawyer_prompt = \"\"\"\n",
    "        You are arguing FOR the decision to eliminate {}.\n",
    "        Argue FOR eliminating the player in bullet-point format. Use at most 3 bullets.\n",
    "    \"\"\"\n",
    "    judge_prompt = \"\"\"\n",
    "        You are a judge deciding which player should be eliminated in a game of werewolf. Your goal is to maximize the player's chances of winning the game with forward-thinking reasoning.\n",
    "        \n",
    "        Game State:\n",
    "        - Alive players: {}\n",
    "        - Dead players: {}\n",
    "        - Werewolves: {}\n",
    "\n",
    "        The two lawyers have made their arguments:\n",
    "        - **Lawyer 1** wants to eliminate {}: \"{}\"\n",
    "        - **Lawyer 2** wants to eliminate {}: \"{}\"\n",
    "\n",
    "        Make your final decision: Which player should be eliminated between the 2? Justify your reasoning briefly.\n",
    "    \"\"\"\n",
    "\n",
    "    rules = \"\"\"\n",
    "        Player roles: In werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\n",
    "        - Werewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\n",
    "        - Villagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\n",
    "        - Seer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\n",
    "        - Witch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\n",
    "\n",
    "        Game Rule: The game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\n",
    "        1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\n",
    "        2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\n",
    "\n",
    "        Victory Condition: For werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers. For villagers, they win if they identify and eliminate all of the werewolves in the group.\n",
    "\n",
    "        Constraints:\n",
    "        1. Your response should be in the first person.\n",
    "        2. This is a conversational game. You should response only based on the conversation history and your strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    to_wolves = (\n",
    "        \"{}, if you are the only werewolf, eliminate a player. Otherwise, \"\n",
    "        \"discuss with your teammates and reach an agreement.\"\n",
    "    )\n",
    "\n",
    "    wolves_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"finish_discussion\": \"whether the discussion reached an agreement or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"finish_discussion\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"finish_discussion\"],\n",
    "    )\n",
    "\n",
    "    to_wolves_vote = \"Which player do you vote to kill?\"\n",
    "\n",
    "    wolves_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_wolves_res = \"The player with the most votes is {}.\"\n",
    "\n",
    "    to_witch_resurrect = (\n",
    "        \"{witch_name}, you're the witch. Tonight {dead_name} is eliminated. \"\n",
    "        \"Would you like to resurrect {dead_name}?\"\n",
    "    )\n",
    "\n",
    "    to_witch_resurrect_no = \"The witch has chosen not to resurrect the player.\"\n",
    "    to_witch_resurrect_yes = \"The witch has chosen to resurrect the player.\"\n",
    "\n",
    "    witch_resurrect_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"whether to resurrect the player and the reason\",\n",
    "            \"resurrect\": \"whether to resurrect the player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"resurrect\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"resurrect\"],\n",
    "    )\n",
    "\n",
    "    to_witch_poison = (\n",
    "        \"Would you like to eliminate one player? If yes, \"\n",
    "        \"specify the player_name.\"\n",
    "    )\n",
    "\n",
    "    witch_poison_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"eliminate\": \"whether to eliminate a player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"eliminate\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"eliminate\"],\n",
    "    )\n",
    "\n",
    "    to_seer = (\n",
    "        \"{}, you're the seer. Which player in {} would you like to check \"\n",
    "        \"tonight?\"\n",
    "    )\n",
    "\n",
    "    seer_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    to_seer_result = \"Okay, the role of {} is a {}.\"\n",
    "\n",
    "    to_all_danger = (\n",
    "        \"The day is coming, all the players open your eyes. Last night, \"\n",
    "        \"the following player(s) has been eliminated: {}.\"\n",
    "    )\n",
    "\n",
    "    to_all_peace = (\n",
    "        \"The day is coming, all the players open your eyes. Last night is \"\n",
    "        \"peaceful, no player is eliminated.\"\n",
    "    )\n",
    "\n",
    "    to_all_discuss = (\n",
    "        \"Now the alive players are {}. Given the game rules and your role, \"\n",
    "        \"based on the situation and the information you gain, to vote a \"\n",
    "        \"player eliminated among alive players and to win the game, what do \"\n",
    "        \"you want to say to others? You can decide whether to reveal your \"\n",
    "        \"role.\"\n",
    "    )\n",
    "\n",
    "    survivors_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    survivors_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_all_vote = (\n",
    "        \"Given the game rules and your role, based on the situation and the\"\n",
    "        \" information you gain, to win the game, it's time to vote one player\"\n",
    "        \" eliminated among the alive players. Which player do you vote to \"\n",
    "        \"kill?\"\n",
    "    )\n",
    "\n",
    "    to_all_res = \"{} has been voted out.\"\n",
    "\n",
    "    to_all_wolf_win = (\n",
    "        \"The werewolves have prevailed and taken over the village. Better \"\n",
    "        \"luck next time!\"\n",
    "    )\n",
    "\n",
    "    to_all_village_win = (\n",
    "        \"The game is over. The werewolves have been defeated, and the village \"\n",
    "        \"is safe once again!\"\n",
    "    )\n",
    "\n",
    "    to_all_continue = \"The game goes on.\"\n",
    "\n",
    "    \n",
    "# Moderator message function\n",
    "HostMsg = partial(Msg, name=\"Moderator\", role=\"assistant\", echo=True)\n",
    "QueryMsg = partial(Msg, name=\"Query\", role=\"user\", echo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries:\n",
    "    \"\"\"Queries for the vectorstore based on certain points in game\"\"\"\n",
    "\n",
    "    # Retrieves discussions about suspicions of non-werewolves to help identify potential enemies.\n",
    "    werewolf_discussion_query = \"Discussions related to villagers, seer, witch.\"\n",
    "\n",
    "    # Retrieves discussions focused on identifying werewolves.\n",
    "    non_werewolf_discussion_query = \"Discussions related to werewolves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Initialization (edit here)\n",
    "To initialize the agents, you must define their parameters and settings in the config objects that are passed in for initialization. There is a model config, which defines the base foundational model being used, and an agent config, which defines each of the agents being used in the game, including which model their using, their name, and what type of Agent they are (based off the agent classes we defined earlier). \n",
    "\n",
    "Pay particular attention to the system prompt, this is what defines the rules of the game to the agent and gives them the role and what their responsibilities are, we could perhaps do some prompt engineering with that.\n",
    "\n",
    "Also we can play around with the settings of the game, eg. max rounds, how many werewolves we have, etc. Just make sure to update the roles, witch, seer objects below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (don't edit)\n",
    "Functions to check and update game state throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_fallback(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Tries to execute the given function. If it fails, waits 5 seconds and retries once.\n",
    "    If it fails again, exits the script.\n",
    "\n",
    "    Args:\n",
    "        func (Callable[..., Any]): The function to execute.\n",
    "        *args (Any): Positional arguments to pass to the function.\n",
    "        **kwargs (Any): Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        Any: The result of the function if successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)  # Try executing the function\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)  # Wait 5 seconds before retrying\n",
    "\n",
    "        try:\n",
    "            return func(*args, **kwargs)  # Try again\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Failed again. Stopping execution.\")\n",
    "            sys.exit(1)  # Exit the script if it fails twice\n",
    "\n",
    "def load_json_response(response: str) -> dict:\n",
    "    \"\"\"Converts a json response to a python dictionary\"\"\"\n",
    "    return json.loads(response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "\n",
    "def convert_messages_to_string(relevant_messages: list) -> str:\n",
    "    \"\"\"Takes a list of messages, and converts them to a string separated by new line\"\"\"\n",
    "    return '\\n'.join([f\"{msg.name}: {msg.content}\" for msg in relevant_messages])\n",
    "\n",
    "def majority_vote(votes: list) -> Any:\n",
    "    \"\"\"Given a list of votes, return the name with the highest frequency.\"\"\"\n",
    "    votes_valid = [v for v in votes if v != \"Abstain\"]\n",
    "    if not votes_valid:\n",
    "        return \"No Votes\"\n",
    "    unique_vals, counts = np.unique(votes_valid, return_counts=True)\n",
    "    return unique_vals[np.argmax(counts)]\n",
    "\n",
    "def update_alive_players(game_state: dict, survivors: list, wolves: list, dead_names):\n",
    "    \"\"\"\n",
    "    Removes 'dead_names' from 'game_state[\"survivors\"]', updates game_state[\"dead\"],\n",
    "    and returns updated survivors and wolves lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(dead_names, list):\n",
    "        dead_names = [dead_names]\n",
    "    for d in dead_names:\n",
    "        if d in game_state[\"survivors\"]:\n",
    "            game_state[\"survivors\"].remove(d)\n",
    "        if d not in game_state[\"dead\"]:\n",
    "            game_state[\"dead\"].append(d)\n",
    "\n",
    "    # Rebuild survivors / wolves lists\n",
    "    new_survivors = [s for s in survivors if s.name in game_state[\"survivors\"]]\n",
    "    new_wolves = [w for w in wolves if w.name in game_state[\"survivors\"]]\n",
    "    return new_survivors, new_wolves\n",
    "\n",
    "def check_winning(game_state: dict, survivors: list, wolves: list, host: str) -> bool:\n",
    "    \"\"\"\n",
    "    If #werewolves * 2 >= #survivors => werewolves instantly win.\n",
    "    If all werewolves are dead => villagers instantly win.\n",
    "    Otherwise => game continues.\n",
    "    \"\"\"\n",
    "    if len(wolves) * 2 >= len(survivors):\n",
    "        msg = Msg(host, Prompts.to_all_wolf_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"werewolves\"\n",
    "        log_message(msg)\n",
    "        return True\n",
    "    if survivors and not wolves:\n",
    "        msg = Msg(host, Prompts.to_all_village_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"villagers\"\n",
    "        log_message(msg)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate_log_filepath(basepath: str, timestamp: str, game_num: int) -> str:\n",
    "    \"\"\"Generate log file path with EST timestamp.\"\"\"\n",
    "    return f\"{basepath}/{timestamp}/courtroom_raw_game{game_num}.log\"\n",
    "\n",
    "def save_results(win_rate_filepath: str, row: dict):\n",
    "    \"\"\"\n",
    "    Append a row to a CSV file. If doesn't exist, create with headers.\n",
    "\n",
    "    row = {\n",
    "      \"raw_log_filepath\": <str>,\n",
    "      \"custom_agent_won\": <bool>,\n",
    "    }\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(win_rate_filepath)\n",
    "    with open(win_rate_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"raw_log_filepath\", \"custom_agent_won\"])\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "    print(f\"Row added to {win_rate_filepath}: {row}\")\n",
    "\n",
    "def custom_agent_won(game_state: dict, alive_agents: list, wolf_agents: list):\n",
    "    \"\"\"Records whether custom agent won (even if game didn't complete)\"\"\"\n",
    "\n",
    "    # If game is completed, check if werewolves have won\n",
    "    if game_state[\"endgame\"]:\n",
    "        if game_state[\"winner\"] == \"werewolves\":\n",
    "            return True \n",
    "        return False\n",
    "    \n",
    "    # If game not completed, then custom agent wins because it is still alive and eliminated another player\n",
    "    return True\n",
    "\n",
    "def get_available_targets(survivors: list, wolves: list):\n",
    "    \"\"\"Returns list of currently alive non-werewolves\"\"\"\n",
    "    return [survivor.name for survivor in survivors if survivor not in wolves]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Game (don't edit)\n",
    "Following is a function that game through the various Night and Day phases, taking different actions for each agent based on their roles. Multi agent functionality and communication is facilitated through the AgentScope Pipeline and MsgHub, more detailed documentation found here https://doc.agentscope.io/build_api/agentscope.pipelines.pipeline.html#module-agentscope.pipelines.pipeline and https://doc.agentscope.io/build_api/agentscope.msghub.html#module-agentscope.msghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(\n",
    "        secondary_model,\n",
    "        max_days_per_game,\n",
    "        reflect_before_vectorstore,\n",
    "        max_werewolf_discussion_round,\n",
    "        wolves,\n",
    "        seer,\n",
    "        witch,\n",
    "        roles,\n",
    "        survivors,\n",
    "        game_state\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Runs a single game, storing logs in game_log. This example merges both\n",
    "    day/night flow, werewolf talk, witch usage, seer usage, etc. Then\n",
    "    saves logs and results.\n",
    "    \"\"\"\n",
    "\n",
    "    for day_i in range(1, max_days_per_game + 1):\n",
    "\n",
    "        # 1) Night Phase: Werewolves discussion\n",
    "        hint = HostMsg(content=Prompts.to_wolves.format(n2s(wolves)))\n",
    "        log_message(hint)\n",
    "        with msghub(wolves, announcement=hint) as hub:\n",
    "            set_parsers(wolves, Prompts.wolves_discuss_parser)\n",
    "            for r in range(max_werewolf_discussion_round):\n",
    "                x = retry_with_fallback(lambda: sequentialpipeline(wolves))\n",
    "                logger.info(f\"Werewolves discussion: {x.content}\")\n",
    "                if x.metadata.get(\"finish_discussion\", False):\n",
    "                    break\n",
    "            # Then vote\n",
    "            set_parsers(wolves, Prompts.wolves_vote_parser)\n",
    "            hint_vote = HostMsg(content=Prompts.to_wolves_vote)\n",
    "            log_message(hint_vote)\n",
    "            votes = [extract_name_and_id(wolf.lawyer_judge_decision(game_state, get_available_targets(survivors, wolves), hint_vote).content)[0] for wolf in wolves]\n",
    "\n",
    "            voted_out = majority_vote(votes)\n",
    "            dead_player = [voted_out]\n",
    "\n",
    "            voted_out_hint = HostMsg(content=Prompts.to_wolves_res.format(voted_out))\n",
    "            hub.broadcast(voted_out_hint)\n",
    "            log_message(voted_out_hint)\n",
    "\n",
    "        # 2) Witch Decision Night   \n",
    "        healing_used_tonight = False\n",
    "        if witch in survivors:\n",
    "            if not game_state[\"witch_healing_used\"]:\n",
    "                hint = HostMsg(\n",
    "                    content=Prompts.to_witch_resurrect.format_map(\n",
    "                        {\n",
    "                            \"witch_name\": witch.name,\n",
    "                            \"dead_name\": dead_player[0],\n",
    "                        },\n",
    "                    ),\n",
    "                )\n",
    "                log_message(hint)\n",
    "                set_parsers(witch, Prompts.witch_resurrect_parser)\n",
    "                \n",
    "                # Capture the witch's resurrection response and log it\n",
    "                resurrection_response = retry_with_fallback(lambda: witch(hint))\n",
    "\n",
    "                if resurrection_response.metadata.get(\"resurrect\", False):\n",
    "                    healing_used_tonight = True\n",
    "                    dead_player.pop()\n",
    "                    game_state[\"witch_healing_used\"] = True\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_yes)\n",
    "                    log_message(HostMsg(content=Prompts.to_witch_resurrect_yes))\n",
    "                else:\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_no)\n",
    "                    log_message(HostMsg(content=Prompts.to_witch_resurrect_no))\n",
    "\n",
    "            if not game_state[\"witch_poison_used\"] and not healing_used_tonight:\n",
    "                set_parsers(witch, Prompts.witch_poison_parser)\n",
    "                \n",
    "                # Capture the witch's poison response and log it\n",
    "                poison_response = retry_with_fallback(lambda: witch(HostMsg(content=Prompts.to_witch_poison)))\n",
    "                logger.info(f\"Witch Poison Response: {poison_response.content}\")\n",
    "\n",
    "                if poison_response.metadata.get(\"eliminate\", False):\n",
    "                    target_player = extract_name_and_id(poison_response.content)[0]\n",
    "                    dead_player.append(target_player)\n",
    "                    game_state[\"witch_poison_used\"] = True\n",
    "                    logger.info(f\"The witch has chosen to poison {target_player}.\")\n",
    "                else:\n",
    "                    logger.info(\"The witch has chosen not to poison a player.\")\n",
    "\n",
    "        # 3) Seer checks a role\n",
    "        if seer in survivors:\n",
    "            seer_hint = HostMsg(content=Prompts.to_seer.format(seer.name, n2s(survivors)))\n",
    "            log_message(seer_hint)\n",
    "            set_parsers(seer, Prompts.seer_parser)\n",
    "\n",
    "            x = seer.reply(seer_hint) # Use seer hint as query\n",
    "            logger.info(f\"Seer response: {x.content}\")\n",
    "            pl, idx = extract_name_and_id(x.content)\n",
    "            role_name = roles[idx] if idx < len(roles) else \"villager\"\n",
    "            # Let seer quietly observe the result\n",
    "            role_msg = HostMsg(content=Prompts.to_seer_result.format(pl, role_name))\n",
    "            logger.info(f\"Moderator: {role_msg.content}\")\n",
    "            seer.observe(role_msg, \"seer\")\n",
    "\n",
    "        # 4) Update survivors after night\n",
    "        survivors, wolves = update_alive_players(game_state, survivors, wolves, dead_player)\n",
    "        if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "            break\n",
    "\n",
    "        # If reflecting, do a night reflection\n",
    "        if reflect_before_vectorstore:\n",
    "            for w in wolves:\n",
    "                w.summarize_cycle(survivors, secondary_model)\n",
    "\n",
    "        # 5) Daytime discussion\n",
    "        content = (\n",
    "            Prompts.to_all_danger.format(n2s(dead_player))\n",
    "            if dead_player\n",
    "            else Prompts.to_all_peace\n",
    "        )\n",
    "        discuss_hints = [\n",
    "            HostMsg(content=content),\n",
    "            HostMsg(content=Prompts.to_all_discuss.format(n2s(survivors))),\n",
    "        ]\n",
    "        with msghub(survivors, announcement=discuss_hints) as hub:\n",
    "            set_parsers(survivors, Prompts.survivors_discuss_parser)\n",
    "            discussion_out = retry_with_fallback(lambda: sequentialpipeline(survivors)) \n",
    "            logger.info(f\"Survivors Discussion: {discussion_out.content}\")\n",
    "\n",
    "            # Daytime vote\n",
    "            set_parsers(survivors, Prompts.survivors_vote_parser)\n",
    "            day_vote_hint = HostMsg(content=Prompts.to_all_vote)\n",
    "            log_message(day_vote_hint)\n",
    "            \n",
    "            votes_day = [\n",
    "                extract_name_and_id(wolf.lawyer_judge_decision(game_state, get_available_targets(survivors, wolves), day_vote_hint).content)[0] for wolf in wolves\n",
    "            ]\n",
    "\n",
    "            votes_day.extend([\n",
    "                extract_name_and_id(_.reply(day_vote_hint).content)[0] for _ in survivors if _ not in wolves\n",
    "            ])\n",
    "            day_result = majority_vote(votes_day)\n",
    "            day_result_out = HostMsg(content=Prompts.to_all_res.format(day_result))\n",
    "            hub.broadcast(day_result_out)\n",
    "            log_message(day_result_out)\n",
    "\n",
    "            survivors, wolves = update_alive_players(game_state, survivors, wolves, day_result)\n",
    "            if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "                break\n",
    "\n",
    "            # If reflecting, do a day reflection\n",
    "            if reflect_before_vectorstore:\n",
    "                for w in wolves:\n",
    "                    w.summarize_cycle(survivors, secondary_model)\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_continue))\n",
    "    \n",
    "    return custom_agent_won(\n",
    "            game_state,\n",
    "            survivors,\n",
    "            wolves\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    timestamp: str,\n",
    "    open_ai_key: str,\n",
    "    raw_log_filepath: str,\n",
    "    win_rate_filepath: str,\n",
    "    max_days_per_game: int = 10,\n",
    "    num_games: int = 10,\n",
    "    reflect_before_vectorstore: bool = True,\n",
    "    primary_model: str = \"gpt-4o-mini\",\n",
    "    secondary_model: str=\"gpt-4o\",\n",
    "    max_werewolf_discussion_round: int = 3,\n",
    "    similarity_top_k: int = 1\n",
    ") -> None:  \n",
    "    \"\"\"\n",
    "    Initialize and run multiple games of the Werewolf simulation.\n",
    "\n",
    "    Arguments:\n",
    "        open_ai_key (`str`):\n",
    "            The OpenAI API key used for model calls.\n",
    "        raw_log_filepath (`str`):\n",
    "            Base path for logs, e.g., \"logs/game_raw\".\n",
    "        win_rate_filepath (`str`):\n",
    "            CSV file to store results, e.g., \"results.csv\".\n",
    "        max_days_per_game (`int`, defaults to `3`):\n",
    "            Maximum number of day/night cycles per game.\n",
    "        num_games (`int`, defaults to `2`):\n",
    "            Number of games to simulate.\n",
    "        reflect_before_vectorstore (`bool`, defaults to `False`):\n",
    "            Whether to use reflective memory before vectorstore retrieval.\n",
    "        primary_model (`str`, defaults to `\"gpt-4o-mini\"`):\n",
    "            Name of the model used for fast processing.\n",
    "        max_werewolf_discussion_round (`int`, defaults to `3`):\n",
    "            Maximum number of discussion rounds for werewolves.\n",
    "        similarity_top_k (`int`, defaults to `1`):\n",
    "            Number of messages or summaries retrieved from memory.\n",
    "\n",
    "    Returns:\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Set openai api key\n",
    "    openai.api_key = open_ai_key\n",
    "\n",
    "    # Define model configs and settings\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": primary_model,\n",
    "            \"model_name\": primary_model,\n",
    "            \"api_key\": open_ai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 1,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the config settings for each agent involved\n",
    "    agent_configs = [\n",
    "        {\n",
    "            \"class\": \"WerewolfAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player1\",\n",
    "                \"sys_prompt\": f\"\"\"\n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player1 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "                    \n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing werewolf in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player2\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player2 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                    Rules of the game: \n",
    "                \n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing villager in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player3\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                    You are Player3 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing seer in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": similarity_top_k,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"class\": \"NormalAgent\",\n",
    "            \"args\": {\n",
    "                \"name\": \"Player4\",\n",
    "                \"sys_prompt\": f\"\"\" \n",
    "                    You are a strategic decision-maker playing a game of werewolf.\n",
    "                    \n",
    "                    You are Player4 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "                    \n",
    "                    Rules of the game:\n",
    "\n",
    "                    {Prompts.rules}\n",
    "                    \n",
    "                    You are playing witch in this game.\n",
    "                \"\"\",\n",
    "                \"model_config_name\": primary_model,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"similarity_top_k\": 1,\n",
    "                \"openai_api_key\": open_ai_key\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for game_num in range(1, num_games + 1):\n",
    "\n",
    "        # Read model and agent configs, and initialize agents automatically\n",
    "        survivors = agentscope.init(\n",
    "            model_configs=model_configs,\n",
    "            agent_configs=agent_configs,\n",
    "            project=\"Werewolf\",\n",
    "        )\n",
    "\n",
    "        # Get player roles\n",
    "        roles = [\"werewolf\", \"villager\", \"seer\", \"witch\"]\n",
    "        wolves, witch, seer, villagers = [survivors[0]], survivors[-1], survivors[-2], [survivors[-3]]\n",
    "\n",
    "        # Initialize game state\n",
    "        game_state = {\n",
    "            \"werewolves\": [player.name for player in wolves],\n",
    "            \"villagers\": [player.name for player in survivors if player not in wolves],\n",
    "            \"seer\": [seer.name],\n",
    "            \"witch\": [witch.name],\n",
    "            \"survivors\": [player.name for player in survivors],\n",
    "            \"dead\": [],\n",
    "            \"witch_healing_used\": False,\n",
    "            \"witch_poison_used\": False,\n",
    "            \"endgame\": False,\n",
    "            \"winner\": None\n",
    "        }\n",
    "\n",
    "        current_log_path = generate_log_filepath(raw_log_filepath, timestamp, game_num)\n",
    "        change_log_file(current_log_path)\n",
    "\n",
    "        # Log parameters\n",
    "        log_params(\n",
    "            current_log_path, \n",
    "            primary_model=primary_model, \n",
    "            secondary_model=secondary_model,\n",
    "            raw_log_filepath=raw_log_filepath, \n",
    "            win_rate_filepath=win_rate_filepath, \n",
    "            max_days_per_game=max_days_per_game, \n",
    "            num_games=num_games,\n",
    "            reflect_before_vectorstore=reflect_before_vectorstore, \n",
    "            max_werewolf_discussion_round=max_werewolf_discussion_round, \n",
    "            similarity_top_k=similarity_top_k\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting game {game_num}\")\n",
    "\n",
    "        custom_agent_won = run_game(\n",
    "            secondary_model,\n",
    "            max_days_per_game,\n",
    "            reflect_before_vectorstore,\n",
    "            max_werewolf_discussion_round,\n",
    "            wolves,\n",
    "            seer,\n",
    "            witch,\n",
    "            roles,\n",
    "            survivors,\n",
    "            game_state\n",
    "        )\n",
    "\n",
    "        wolves[0].save_context(timestamp, game_num)\n",
    "        witch.save_context(timestamp, game_num, \"witch\")\n",
    "        seer.save_context(timestamp, game_num, \"seer\")\n",
    "        villagers[0].save_context(timestamp, game_num, \"villager\")\n",
    "\n",
    "\n",
    "        row = {\n",
    "            \"raw_log_filepath\": current_log_path,\n",
    "            \"custom_agent_won\": custom_agent_won\n",
    "        }\n",
    "        save_results(f\"{win_rate_filepath}/{timestamp}/courtroom_win_rate.csv\", row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:54 | INFO     | agentscope.manager._model:load_model_configs:115 - Load configs for model wrapper: gpt-4o-mini\n",
      "2025-02-26 17:41:54 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:54 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:41:54 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:55 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:41:55 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:55 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:41:55 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:55 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "Parameters written to logs/courtroom/20250226_174154/courtroom_raw_game1.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:55 - INFO - Starting game 1\n",
      "2025-02-26 17:41:55 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-26 17:41:55 - INFO - ===\n",
      "2025-02-26 17:41:55 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:41:55 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:41:55 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:56 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to maintain my cover and sow some doubt among the villagers while protecting my fellow werewolf. I should subtly point out suspicious behavior without revealing my own identity.\", \"speak\": \"I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\", \"finish_discussion\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:57 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:41:57 - INFO - Werewolves discussion: I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\n",
      "2025-02-26 17:41:57 - INFO - Moderator: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:57 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:41:57 - INFO - ===\n",
      "2025-02-26 17:41:57 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:41:57 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to maintain my cover and sow some doubt among the villagers while protecting my fellow werewolf. I should subtly point out suspicious behavior without revealing my own identity.\", \"speak\": \"I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I need to maintain my cover and sow some doubt among the villagers while protecting my fellow werewolf. I should subtly point out suspicious behavior without revealing my own identity.\", \"speak\": \"I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\", \"finish_discussion\": \"false\"}\n",
      "2025-02-26 17:41:57 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:41:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:41:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:41:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:41:59 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:00 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:01 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:01 - INFO - ===\n",
      "2025-02-26 17:42:01 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': 'I believe Player3 has been acting suspiciously and their behavior could indicate they are hiding something. Eliminating them might help us narrow down our suspects.', 'vote': 'Player3'}}\n",
      "2025-02-26 17:42:01 - INFO - Lawyer 2 argument: {'target': 'Player4', 'argument': {'thought': \"Player4 has been very quiet and hasn't contributed much to the discussion. This raises suspicion about their role. Also, they seem to be avoiding drawing attention to themselves, which is a typical tactic of a werewolf. Finally, their actions have not aligned with our efforts to root out the werewolves, indicating possible complicity.\", 'vote': 'Player4'}}\n",
      "2025-02-26 17:42:01 - INFO - Judge decision: {'thought': \"Both Player3 and Player4 have presented reasons for suspicion, but Player4's quietness and lack of contribution stand out as red flags. A player remaining passive often raises more suspicion, as it's a common tactic for werewolves to avoid drawing attention. Eliminating Player4 could help us identify potential werewolves more effectively.\", 'vote': 'Player4'}\n",
      "2025-02-26 17:42:01 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Both Player3 and Player4 have presented reasons for suspicion, but Player4's quietness and lack of contribution stand out as red flags. A player remaining passive often raises more suspicion, as it's a common tactic for werewolves to avoid drawing attention. Eliminating Player4 could help us identify potential werewolves more effectively.\", \"vote\": \"Player4\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:01 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:01 - INFO - Moderator: The player with the most votes is Player4.\n",
      "2025-02-26 17:42:01 - INFO - Moderator: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?\n",
      "2025-02-26 17:42:01 - INFO - ===\n",
      "2025-02-26 17:42:01 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:01 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:01 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player4.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player4 is eliminated. Would you like to resurrect Player4?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:03 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"As the witch, my role is crucial, and I can strategically use my potion of life. However, I need to consider if Player4 holds significant information or if it's wiser to keep them out.\", \"speak\": \"I will not resurrect Player4. While I understand the importance of keeping players in the game, I believe it is strategically better to save my potion for a more crucial moment when another villager might face elimination.\", \"resurrect\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:03 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:03 - INFO - Moderator: The witch has chosen not to resurrect the player.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen not to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: The witch has chosen not to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Would you like to eliminate one player? If yes, specify the player_name.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:03 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:03 - INFO - ===\n",
      "2025-02-26 17:42:03 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:03 - INFO - Retrieved similar documents: Player4: {\"thought\": \"As the witch, my role is crucial, and I can strategically use my potion of life. However, I need to consider if Player4 holds significant information or if it's wiser to keep them out.\", \"speak\": \"I will not resurrect Player4. While I understand the importance of keeping players in the game, I believe it is strategically better to save my potion for a more crucial moment when another villager might face elimination.\", \"resurrect\": false}\n",
      "2025-02-26 17:42:03 - INFO - ===\n",
      "2025-02-26 17:42:05 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully consider who to eliminate tonight. It's crucial to eliminate a player who might be a werewolf, but I should also keep in mind that I have the power to save someone. I must remain stealthy and gather more information.\", \"speak\": \"I think we should keep discussing and gathering insights about each other's suspicions. We need to identify the werewolves carefully.\", \"eliminate\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:05 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:05 - INFO - Witch Poison Response: I think we should keep discussing and gathering insights about each other's suspicions. We need to identify the werewolves carefully.\n",
      "2025-02-26 17:42:05 - INFO - The witch has chosen not to poison a player.\n",
      "2025-02-26 17:42:05 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "2025-02-26 17:42:05 - INFO - ===\n",
      "2025-02-26 17:42:05 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:05 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:05 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:06 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I want to gather information tonight to help the villagers. I suspect Player1 might be a werewolf based on their previous behavior. Checking them will give me a clearer idea.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:07 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:07 - INFO - Seer response: Player1\n",
      "2025-02-26 17:42:07 - INFO - Moderator: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:07 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night, the following player(s) has been eliminated: Player4.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2 and Player3. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:07 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:07 - INFO - ===\n",
      "2025-02-26 17:42:07 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:07 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to maintain my cover and sow some doubt among the villagers while protecting my fellow werewolf. I should subtly point out suspicious behavior without revealing my own identity.\", \"speak\": \"I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\", \"finish_discussion\": \"false\"}\n",
      "Player1: Argument 1: {'target': 'Player3', 'argument': {'thought': 'I believe Player3 has been acting suspiciously and their behavior could indicate they are hiding something. Eliminating them might help us narrow down our suspects.', 'vote': 'Player3'}}\n",
      "Argument 2: {'target': 'Player4', 'argument': {'thought': \"Player4 has been very quiet and hasn't contributed much to the discussion. This raises suspicion about their role. Also, they seem to be avoiding drawing attention to themselves, which is a typical tactic of a werewolf. Finally, their actions have not aligned with our efforts to root out the werewolves, indicating possible complicity.\", 'vote': 'Player4'}}\n",
      "Decision: {'thought': \"Both Player3 and Player4 have presented reasons for suspicion, but Player4's quietness and lack of contribution stand out as red flags. A player remaining passive often raises more suspicion, as it's a common tactic for werewolves to avoid drawing attention. Eliminating Player4 could help us identify potential werewolves more effectively.\", 'vote': 'Player4'}\n",
      "2025-02-26 17:42:07 - INFO - ===\n",
      "2025-02-26 17:42:08 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information about the other players and figure out who the werewolves are. I'll try to steer the conversation without revealing my role, while observing the reactions of the others. I should also be careful not to draw suspicion towards myself, as I need to blend in and assess the clues.\",\"speak\": \"I think we should discuss our suspicions openly. Anyone have any thoughts on who might be acting strangely or seems to be withholding information?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:09 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:09 - INFO - ===\n",
      "2025-02-26 17:42:09 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:09 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:09 - INFO - ===\n",
      "2025-02-26 17:42:10 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather more information and see if anyone is showing suspicious behavior. I don't want to jump to conclusions too early.\", \"speak\": \"I agree, Player1. It's important to share our thoughts. I’ve been keeping an eye on Player3. They seemed a bit quiet last night.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:10 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:10 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:10 - INFO - ===\n",
      "2025-02-26 17:42:10 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:10 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I want to gather information tonight to help the villagers. I suspect Player1 might be a werewolf based on their previous behavior. Checking them will give me a clearer idea.\", \"speak\": \"Player1\"}\n",
      "2025-02-26 17:42:10 - INFO - ===\n",
      "2025-02-26 17:42:15 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"Player2 is investigating me, which could mean they're suspicious. I need to defend myself and lead the suspicion elsewhere while maintaining my cover as the seer.\", \"speak\": \"I can assure you, Player2, that I am just trying to gather information. Maybe we should focus on who has been quiet during the discussions. Player4 hasn't said much either. What do you think?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:15 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:15 - INFO - Survivors Discussion: I can assure you, Player2, that I am just trying to gather information. Maybe we should focus on who has been quiet during the discussions. Player4 hasn't said much either. What do you think?\n",
      "2025-02-26 17:42:15 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:15 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:15 - INFO - ===\n",
      "2025-02-26 17:42:15 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:15 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to maintain my cover and sow some doubt among the villagers while protecting my fellow werewolf. I should subtly point out suspicious behavior without revealing my own identity.\", \"speak\": \"I don't know, but I've been watching how Player2 is behaving. They seem a bit too eager to push for a vote on Player3, almost like they're trying to divert attention away from themselves.\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I need to gather information about the other players and figure out who the werewolves are. I'll try to steer the conversation without revealing my role, while observing the reactions of the others. I should also be careful not to draw suspicion towards myself, as I need to blend in and assess the clues.\", \"speak\": \"I think we should discuss our suspicions openly. Anyone have any thoughts on who might be acting strangely or seems to be withholding information?\"}\n",
      "2025-02-26 17:42:15 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:15 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:15 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:15 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:15 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:15 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:15 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:17 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:18 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:19 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:19 - INFO - ===\n",
      "2025-02-26 17:42:19 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': 'Player3 has been acting suspiciously, not contributing much to the discussion and avoiding direct accusations. Eliminating them could help us get closer to identifying the werewolves.', 'vote': 'Player3'}}\n",
      "2025-02-26 17:42:19 - INFO - Lawyer 2 argument: {'target': 'Player2', 'argument': {'thought': 'I believe eliminating Player2 is a strategic move to shake things up and potentially expose the real werewolves hiding among us. Their behavior has been questionable, and this could be a way to test the waters.', 'vote': 'Player2'}}\n",
      "2025-02-26 17:42:19 - INFO - Judge decision: {'thought': 'I believe that eliminating Player3 is the better option since their lack of contribution and avoidance of accusations makes them a potential werewolf candidate. Removing them could streamline the discussion and allow us to focus on Player2, whose behavior is less suspicious in my view as they are engaging in the conversation.', 'vote': 'Player3'}\n",
      "2025-02-26 17:42:19 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I believe that eliminating Player3 is the better option since their lack of contribution and avoidance of accusations makes them a potential werewolf candidate. Removing them could streamline the discussion and allow us to focus on Player2, whose behavior is less suspicious in my view as they are engaging in the conversation.\", \"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:19 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:19 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:19 - INFO - ===\n",
      "2025-02-26 17:42:19 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:19 - INFO - Retrieved similar documents: Player2: {\"thought\": \"I need to gather more information and see if anyone is showing suspicious behavior. I don't want to jump to conclusions too early.\", \"speak\": \"I agree, Player1. It's important to share our thoughts. I\\u2019ve been keeping an eye on Player3. They seemed a bit quiet last night.\"}\n",
      "Player2: {\"thought\": \"I need to gather more information and see if anyone is showing suspicious behavior. I don't want to jump to conclusions too early.\", \"speak\": \"I agree, Player1. It's important to share our thoughts. I\\u2019ve been keeping an eye on Player3. They seemed a bit quiet last night.\"}\n",
      "2025-02-26 17:42:19 - INFO - ===\n",
      "2025-02-26 17:42:21 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I'm suspicious of Player1's behavior during our discussions, as they have been somewhat evasive and haven't contributed much to reasoning against the potential werewolves. It's important to reduce the number of players, so I vote to eliminate Player1 for now.\", \"vote\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:22 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:22 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:22 - INFO - ===\n",
      "2025-02-26 17:42:22 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:22 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I want to gather information tonight to help the villagers. I suspect Player1 might be a werewolf based on their previous behavior. Checking them will give me a clearer idea.\", \"speak\": \"Player1\"}\n",
      "2025-02-26 17:42:22 - INFO - ===\n",
      "2025-02-26 17:42:23 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be careful and strategic about my vote. Based on my knowledge as the seer, I've identified that Player2 is actually a werewolf. It's crucial to eliminate them to help the villagers. Therefore, I will vote for Player2.\", \"vote\": \"Player2\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:23 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:23 - INFO - Moderator: Player1 has been voted out.\n",
      "2025-02-26 17:42:23 - INFO - Moderator: The game is over. The werewolves have been defeated, and the village is safe once again!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1 has been voted out.\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\1\\werewolf.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\1\\witch.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\1\\seer.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\1\\villager.log\n",
      "Row added to logs/courtroom/20250226_174154/courtroom_win_rate.csv: {'raw_log_filepath': 'logs/courtroom/20250226_174154/courtroom_raw_game1.log', 'custom_agent_won': False}\n",
      "2025-02-26 17:42:23 | WARNING  | agentscope.manager._model:load_model_configs:108 - config_name [gpt-4o-mini] already exists.\n",
      "2025-02-26 17:42:23 | INFO     | agentscope.manager._model:load_model_configs:115 - Load configs for model wrapper: gpt-4o-mini\n",
      "2025-02-26 17:42:23 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:23 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:23 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:23 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:23 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:23 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:23 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:24 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:24 - INFO - Starting game 2\n",
      "2025-02-26 17:42:24 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-26 17:42:24 - INFO - ===\n",
      "2025-02-26 17:42:24 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:24 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:24 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters written to logs/courtroom/20250226_174154/courtroom_raw_game2.log\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:25 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to blend in and avoid suspicion while subtly guiding the conversation towards eliminating a potential villager. We should focus on who seems most uncertain about their role to direct attention away from us werewolves.\",\"speak\": \"I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\",\"finish_discussion\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:25 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:25 - INFO - Werewolves discussion: I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\n",
      "2025-02-26 17:42:25 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:25 - INFO - ===\n",
      "2025-02-26 17:42:25 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:25 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to blend in and avoid suspicion while subtly guiding the conversation towards eliminating a potential villager. We should focus on who seems most uncertain about their role to direct attention away from us werewolves.\", \"speak\": \"I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\", \"finish_discussion\": false}\n",
      "Player1: {\"thought\": \"I need to blend in and avoid suspicion while subtly guiding the conversation towards eliminating a potential villager. We should focus on who seems most uncertain about their role to direct attention away from us werewolves.\", \"speak\": \"I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\", \"finish_discussion\": false}\n",
      "2025-02-26 17:42:25 - INFO - ===\n",
      "2025-02-26 17:42:27 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gauge the suspicions of the other players and see if they are turning against each other. It's important for me to blend in and not reveal my identity as a werewolf. I want to subtly steer the conversation away from myself and the other werewolf, while possibly aiming to eliminate one of the villagers or the seer if they seem suspicious. The discussion seems to be getting heated, which is good for us, as it can create distrust among the villagers.\",\"speak\": \"I think we should focus on who seems to be acting differently or more suspiciously. Maybe we can point out what certain players have been saying or doing that might indicate if they're werewolves or just trying to cover their tracks. What does everyone else think?\",\"finish_discussion\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:28 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:28 - INFO - Werewolves discussion: I think we should focus on who seems to be acting differently or more suspiciously. Maybe we can point out what certain players have been saying or doing that might indicate if they're werewolves or just trying to cover their tracks. What does everyone else think?\n",
      "2025-02-26 17:42:28 - INFO - Moderator: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:28 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:28 - INFO - ===\n",
      "2025-02-26 17:42:28 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:28 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to gauge the suspicions of the other players and see if they are turning against each other. It's important for me to blend in and not reveal my identity as a werewolf. I want to subtly steer the conversation away from myself and the other werewolf, while possibly aiming to eliminate one of the villagers or the seer if they seem suspicious. The discussion seems to be getting heated, which is good for us, as it can create distrust among the villagers.\", \"speak\": \"I think we should focus on who seems to be acting differently or more suspiciously. Maybe we can point out what certain players have been saying or doing that might indicate if they're werewolves or just trying to cover their tracks. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I need to blend in and avoid suspicion while subtly guiding the conversation towards eliminating a potential villager. We should focus on who seems most uncertain about their role to direct attention away from us werewolves.\", \"speak\": \"I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\", \"finish_discussion\": false}\n",
      "2025-02-26 17:42:28 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:28 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:28 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:28 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:28 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:28 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:28 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:30 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:31 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:32 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:32 - INFO - ===\n",
      "2025-02-26 17:42:32 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': 'Player3 has been acting suspiciously and their defense seems weak. We should leverage the doubts within the group to push for their elimination. This could help us eliminate a potential threat to the werewolves while keeping our identities hidden.', 'vote': 'Player3'}}\n",
      "2025-02-26 17:42:32 - INFO - Lawyer 2 argument: {'target': 'Player4', 'argument': {'thought': \"I believe eliminating Player4 is strategic since they have been quiet and not contributing much to the discussions, which might indicate they are trying to avoid suspicion. It's also a risk to let them stay since they could be a villager who may uncover our identities.\", 'vote': 'Player4'}}\n",
      "2025-02-26 17:42:32 - INFO - Judge decision: {'thought': 'Eliminating Player3 is the better choice because their behavior has raised suspicion among the group, making them a target for discussion and potential lynching. Keeping Player4 around, who is quieter, could be beneficial since they may not pose an immediate threat and might help divert attention away from me. Reducing the number of potential threats by eliminating Player3 helps to manage the dynamics of the vote in favor of the werewolves.', 'vote': 'Player3'}\n",
      "2025-02-26 17:42:32 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player3 is the better choice because their behavior has raised suspicion among the group, making them a target for discussion and potential lynching. Keeping Player4 around, who is quieter, could be beneficial since they may not pose an immediate threat and might help divert attention away from me. Reducing the number of potential threats by eliminating Player3 helps to manage the dynamics of the vote in favor of the werewolves.\",\"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:32 - INFO - Moderator: The player with the most votes is Player3.\n",
      "2025-02-26 17:42:32 - INFO - Moderator: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n",
      "2025-02-26 17:42:32 - INFO - ===\n",
      "2025-02-26 17:42:32 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:32 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:32 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player3.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:34 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully consider whether resurrecting Player3 is beneficial. If Player3 is a villager, resurrecting them could help us in the next day phase. But if they are a werewolf, that would be counterproductive. However, without any indication of their role as of now, I should err on the side of supporting the villagers.\",\"speak\": \"I choose to resurrect Player3 because I believe they can help us identify the werewolves and contribute to the discussion in the next day phase.\",\"resurrect\": true}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:34 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:34 - INFO - Moderator: The witch has chosen to resurrect the player.\n",
      "2025-02-26 17:42:34 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "2025-02-26 17:42:34 - INFO - ===\n",
      "2025-02-26 17:42:34 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:34 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:34 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:35 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to help the villagers identify the werewolves. I will check Player1 tonight to see if they are one of them.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:35 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:35 - INFO - Seer response: Player1\n",
      "2025-02-26 17:42:35 - INFO - Moderator: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:35 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night is peaceful, no player is eliminated.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2, Player3 and Player4. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:36 - INFO - ===\n",
      "2025-02-26 17:42:36 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:36 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to gauge the suspicions of the other players and see if they are turning against each other. It's important for me to blend in and not reveal my identity as a werewolf. I want to subtly steer the conversation away from myself and the other werewolf, while possibly aiming to eliminate one of the villagers or the seer if they seem suspicious. The discussion seems to be getting heated, which is good for us, as it can create distrust among the villagers.\", \"speak\": \"I think we should focus on who seems to be acting differently or more suspiciously. Maybe we can point out what certain players have been saying or doing that might indicate if they're werewolves or just trying to cover their tracks. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I need to blend in and avoid suspicion while subtly guiding the conversation towards eliminating a potential villager. We should focus on who seems most uncertain about their role to direct attention away from us werewolves.\", \"speak\": \"I think we should consider Player3. They've been acting a bit oddly and seem unsure about their accusations. Maybe we should keep a close eye on them today.\", \"finish_discussion\": false}\n",
      "2025-02-26 17:42:36 - INFO - ===\n",
      "2025-02-26 17:42:37 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious and blend in while figuring out who the other villager roles are. Player2 and Player3 seem to be discussing strategic moves, while Player4 is quieter. I should keep an eye on their interactions and try not to reveal too much about my identity as a werewolf.\",\"speak\": \"I think we should all share our suspicions. It's important to discuss who we've been watching closely. I have a feeling Player4 might have more information than they're letting on.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:38 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:38 - INFO - ===\n",
      "2025-02-26 17:42:38 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:38 - INFO - Retrieved similar documents: \n",
      "2025-02-26 17:42:38 - INFO - ===\n",
      "2025-02-26 17:42:40 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"Player1 is trying to steer the conversation toward Player4, which could be a tactic to divert attention. I need to stay focused and observe Player3's reactions as well.\", \"speak\": \"I agree that discussing our suspicions is important. Player3, do you have any thoughts about Player4 or anyone else?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:40 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:40 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:40 - INFO - ===\n",
      "2025-02-26 17:42:40 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:40 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to gather information to help the villagers identify the werewolves. I will check Player1 tonight to see if they are one of them.\", \"speak\": \"Player1\"}\n",
      "2025-02-26 17:42:40 - INFO - ===\n",
      "2025-02-26 17:42:42 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I should carefully gauge the reactions of other players, especially since I have insight as the seer. I need to hint at my role without revealing too much, while still helping the villagers.\",\"speak\": \"I've been observing Player4's behavior, and I think they might be acting a bit suspiciously. We should definitely keep an eye on them.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:42 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:42 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:42 - INFO - ===\n",
      "2025-02-26 17:42:42 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:42 - INFO - Retrieved similar documents: Player4: {\"thought\": \"I need to carefully consider whether resurrecting Player3 is beneficial. If Player3 is a villager, resurrecting them could help us in the next day phase. But if they are a werewolf, that would be counterproductive. However, without any indication of their role as of now, I should err on the side of supporting the villagers.\", \"speak\": \"I choose to resurrect Player3 because I believe they can help us identify the werewolves and contribute to the discussion in the next day phase.\", \"resurrect\": true}\n",
      "2025-02-26 17:42:42 - INFO - ===\n",
      "2025-02-26 17:42:44 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 is assuming suspicion on me, which could either be a strategy to divert attention or genuine suspicion. I need to defend myself without revealing my identity as the witch too soon.\", \"speak\": \"I understand your concerns, Player3, but I think we have to look at the bigger picture. I haven't seen anything that makes me suspicious of myself. Let's not jump to conclusions.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:44 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:44 - INFO - Survivors Discussion: I understand your concerns, Player3, but I think we have to look at the bigger picture. I haven't seen anything that makes me suspicious of myself. Let's not jump to conclusions.\n",
      "2025-02-26 17:42:44 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:44 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:44 - INFO - ===\n",
      "2025-02-26 17:42:44 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-26 17:42:44 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to be cautious and blend in while figuring out who the other villager roles are. Player2 and Player3 seem to be discussing strategic moves, while Player4 is quieter. I should keep an eye on their interactions and try not to reveal too much about my identity as a werewolf.\", \"speak\": \"I think we should all share our suspicions. It's important to discuss who we've been watching closely. I have a feeling Player4 might have more information than they're letting on.\"}\n",
      "Player1: {\"thought\": \"I need to gauge the suspicions of the other players and see if they are turning against each other. It's important for me to blend in and not reveal my identity as a werewolf. I want to subtly steer the conversation away from myself and the other werewolf, while possibly aiming to eliminate one of the villagers or the seer if they seem suspicious. The discussion seems to be getting heated, which is good for us, as it can create distrust among the villagers.\", \"speak\": \"I think we should focus on who seems to be acting differently or more suspiciously. Maybe we can point out what certain players have been saying or doing that might indicate if they're werewolves or just trying to cover their tracks. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-26 17:42:44 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:44 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:44 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:44 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:44 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-26 17:42:44 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-26 17:42:44 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:45 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:46 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:48 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:48 - INFO - ===\n",
      "2025-02-26 17:42:48 - INFO - Lawyer 1 argument: {'target': 'Player3', 'argument': {'thought': 'Player3 has been acting suspiciously, trying to manipulate our discussions without providing solid reasoning for their accusations. Eliminating Player3 could help us narrow down the werewolf suspects.', 'vote': 'Player3'}}\n",
      "2025-02-26 17:42:48 - INFO - Lawyer 2 argument: {'target': 'Player2', 'argument': {'thought': \"Player2 has been acting a bit too confidently and making assumptions about others without providing strong reasoning. It's suspicious and could indicate they are trying to divert attention from themselves.\", 'vote': 'Player2'}}\n",
      "2025-02-26 17:42:48 - INFO - Judge decision: {'thought': \"While both Player2 and Player3 exhibit suspicious behavior, Player3's actions seem to lack solid reasoning and could indicate manipulation. Eliminating Player3 may help us gain clearer insights and narrow down our suspect list. It's better to address the player exhibiting the least genuine participation.\", 'vote': 'Player3'}\n",
      "2025-02-26 17:42:48 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"While both Player2 and Player3 exhibit suspicious behavior, Player3's actions seem to lack solid reasoning and could indicate manipulation. Eliminating Player3 may help us gain clearer insights and narrow down our suspect list. It's better to address the player exhibiting the least genuine participation.\", \"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:48 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:48 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:48 - INFO - ===\n",
      "2025-02-26 17:42:48 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:48 - INFO - Retrieved similar documents: Player2: {\"thought\": \"Player1 is trying to steer the conversation toward Player4, which could be a tactic to divert attention. I need to stay focused and observe Player3's reactions as well.\", \"speak\": \"I agree that discussing our suspicions is important. Player3, do you have any thoughts about Player4 or anyone else?\"}\n",
      "Player2: {\"thought\": \"Player1 is trying to steer the conversation toward Player4, which could be a tactic to divert attention. I need to stay focused and observe Player3's reactions as well.\", \"speak\": \"I agree that discussing our suspicions is important. Player3, do you have any thoughts about Player4 or anyone else?\"}\n",
      "2025-02-26 17:42:48 - INFO - ===\n",
      "2025-02-26 17:42:49 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully consider who I suspect might be a werewolf based on the previous discussions. Player3 seemed a bit defensive when we were discussing the potential werewolves, so I am leaning towards them.\", \"vote\": \"Player3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:50 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:50 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:50 - INFO - ===\n",
      "2025-02-26 17:42:50 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:50 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to gather information to help the villagers identify the werewolves. I will check Player1 tonight to see if they are one of them.\", \"speak\": \"Player1\"}\n",
      "2025-02-26 17:42:50 - INFO - ===\n",
      "2025-02-26 17:42:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious with my vote. Since I'm the seer, I should use my knowledge wisely. I have not yet revealed my findings, but I need to choose a player who seems suspicious based on the discussions. It's essential to eliminate a potential werewolf without drawing too much attention to myself.\", \"vote\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:51 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:51 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:51 - INFO - ===\n",
      "2025-02-26 17:42:51 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-26 17:42:51 - INFO - Retrieved similar documents: Player4: {\"thought\": \"I need to carefully consider whether resurrecting Player3 is beneficial. If Player3 is a villager, resurrecting them could help us in the next day phase. But if they are a werewolf, that would be counterproductive. However, without any indication of their role as of now, I should err on the side of supporting the villagers.\", \"speak\": \"I choose to resurrect Player3 because I believe they can help us identify the werewolves and contribute to the discussion in the next day phase.\", \"resurrect\": true}\n",
      "2025-02-26 17:42:51 - INFO - ===\n",
      "2025-02-26 17:42:52 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to identify and eliminate a potential werewolf to protect the villagers. Based on the conversation, Player1 seems suspicious with their vague explanations and attempts to deflect attention. It's a strategic choice to eliminate them first.\", \"vote\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 17:42:53 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-26 17:42:53 - INFO - Moderator: Player1 has been voted out.\n",
      "2025-02-26 17:42:53 - INFO - Moderator: The game is over. The werewolves have been defeated, and the village is safe once again!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1 has been voted out.\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\2\\werewolf.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\2\\witch.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\2\\seer.log\n",
      "Messages saved to vectorstore\\courtroom\\20250226_174154\\2\\villager.log\n",
      "Row added to logs/courtroom/20250226_174154/courtroom_win_rate.csv: {'raw_log_filepath': 'logs/courtroom/20250226_174154/courtroom_raw_game2.log', 'custom_agent_won': False}\n"
     ]
    }
   ],
   "source": [
    "# Get current timestamp\n",
    "timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_experiment(\n",
    "    timestamp=timestamp_str,\n",
    "    open_ai_key=openai_key,\n",
    "    primary_model = \"gpt-4o-mini\",\n",
    "    secondary_model =\"gpt-4o\",\n",
    "    raw_log_filepath=\"logs/courtroom\",\n",
    "    reflect_before_vectorstore=False,\n",
    "    win_rate_filepath=\"logs/courtroom\",\n",
    "    max_days_per_game=2,\n",
    "    num_games=2,\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
