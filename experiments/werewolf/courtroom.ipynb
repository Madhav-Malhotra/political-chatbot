{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    open_ai_key,\n",
    "    raw_log_filepath,\n",
    "    win_rate_filepath,\n",
    "    max_steps_per_game=10,\n",
    "    num_games=10,\n",
    "    reflect_before_vectorstore=True,\n",
    "    use_default_agents=True,\n",
    "    primary_model=\"gpt-4o-mini\",\n",
    "    primary_model_params={},\n",
    "    secondary_model=\"gpt-4o\",\n",
    "    secondary_model_params={},\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run an experiment with the given parameters, ensuring ALL data are saved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    open_ai_key : str\n",
    "        The OpenAI API key.\n",
    "    raw_log_basepath : str\n",
    "        The base filepath to save outputs from ALL LLM completions and vectorstore \n",
    "        search results. \n",
    "        Example: \"logs/courtroom_raw\" as a basepath will store files as \n",
    "        \"logs/courtroom_raw_{game_num}_{iso_timestamp}.log\". \n",
    "    win_rate_basepath : str\n",
    "        The base filepath to save CSV win rate data. There are two CSV columns: \n",
    "        raw_log_filepath (str filepath for that game's log) and \n",
    "        custom_agent_won (bool). \n",
    "        Example: \"logs/courtroom_win_rate\" as a basepath will store files as \n",
    "        \"logs/courtroom_win_rate_{iso_timestamp}.csv\".\n",
    "    max_steps_per_game : int, optional\n",
    "        The maximum number of steps per game.\n",
    "    num_games : int, optional\n",
    "        The number of games to play to get the average win rate.\n",
    "    reflect_before_vectorstore : bool, optional\n",
    "        Whether to reflect and summarise data before adding past context to \n",
    "        vectorstore.\n",
    "    use_default_agents : bool, optional\n",
    "        If true, create one custom agent and leave the rest as default.\n",
    "        If false, create all custom agents working against each other. We don't\n",
    "        need to support this yet.\n",
    "    primary_model : str, optional\n",
    "        The fast mind model / lawyer model. \n",
    "    primary_model_params : dict, optional\n",
    "        For now, we'll only have one key: \"temperature\".\n",
    "    secondary_model : str, optional\n",
    "        The slow mind model / judge model. Note: in the courtroom architecture,\n",
    "        the judge model is the same as the lawyer model. In the slow-mind-fast-mind\n",
    "        architecture, the slow mind is more expensive than the fast mind.\n",
    "    secondary_model_params : dict, optional\n",
    "        For now, we'll only have one key: \"temperature\".\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Add your own code above and call whichever functions you need to\n",
    "    # support the functionality above: \n",
    "    # 1. Setting up the right models, with the API key and parameters passed.\n",
    "    # 2. Setting up logging\n",
    "    # 3. Ensuring a winner can always be computed, even if a full game isn't run\n",
    "    #    (ideas: winner if most money at given step in Monopoly, winner if alive \n",
    "    #     in given step in Werewolf). Might want to think more about Werewolf.\n",
    "    # 4. Configuring the vectorstore step to reflect or not reflect before adding\n",
    "    #    past context. If you're confused about what reflection means, check out\n",
    "    #    https://docs.google.com/document/d/1noyndjsrsLqEgX0mwNuWQuma8bk-hX-RT6XVLW6QE5Q/edit?tab=t.0#heading=h.ocoz0vsmhwo\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
