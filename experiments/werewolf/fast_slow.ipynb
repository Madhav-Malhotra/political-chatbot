{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf Simulator\n",
    "The following runs a simulation of the game Werewolf with multiple agents, it is built on top of and uses the Agentscope library, documentation can be found here https://doc.agentscope.io/, and the open source code that we extend on is here https://github.com/modelscope/agentscope/tree/main.\n",
    "### Rules of Werewolf\n",
    "- There are 4 roles, the Werewolves, Villagers, Witch, and Seer (Witch and Seer are on the Villager team)\n",
    "- Each night:\n",
    "    - The Werewolves discuss and vote on a player to eliminate\n",
    "    - The Witch is told what player the wolves voted to eliminate, and is given the choice to use their potion of healing to reserruct the eliminated player, or use their poison to eliminate another player. Note that each power can only be used once in a game\n",
    "    - The seer can pick any other player and find out what their role is (one player per night)\n",
    "- After the events of the night, all surviving players discuss amongst themselves and vote on a player to eliminate\n",
    "- Werewolves win if their numbers equal or exceed Villagers.\n",
    "- Villagers win if all Werewolves are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: agentscope==0.1.1 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: faiss-cpu==1.9.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: black in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.0)\n",
      "Requirement already satisfied: docstring-parser in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.16)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: loguru==0.6.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (9.4.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: inputimeout in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: Flask==3.0.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: Flask-Cors==4.0.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: Flask-SocketIO==5.3.6 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (5.3.6)\n",
      "Requirement already satisfied: flask-sqlalchemy in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: flake8 in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: openai>=1.3.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.54.4)\n",
      "Requirement already satisfied: dashscope>=1.19.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.20.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from faiss-cpu==1.9.0->-r requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: python-socketio>=5.0.2 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (5.11.4)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from loguru==0.6.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from loguru==0.6.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.8.3)\n",
      "Requirement already satisfied: websocket-client in c:\\programdata\\anaconda3\\lib\\site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.58.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.16 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.36)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken->agentscope==0.1.1->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: bidict>=0.21.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (4.10.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=2.0.16->flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from websocket-client->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from python-engineio>=4.8.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wsproto in c:\\users\\mehar\\appdata\\roaming\\python\\python311\\site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional, Union, Sequence, Any, List, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from functools import partial\n",
    "import openai  \n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import random \n",
    "import logging\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import sys\n",
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "from agentscope.parsers.json_object_parser import MarkdownJsonDictParser\n",
    "from agentscope.parsers import ParserBase\n",
    "from agentscope.message import Msg\n",
    "from agentscope.msghub import msghub\n",
    "from agentscope.agents import AgentBase\n",
    "from agentscope.memory.temporary_memory import TemporaryMemory\n",
    "from agentscope.pipelines.functional import sequentialpipeline\n",
    "from agentscope.manager import ModelManager\n",
    "import agentscope\n",
    "\n",
    "from utils.werewolf_utils import (\n",
    "    extract_name_and_id,\n",
    "    n2s,\n",
    "    set_parsers,\n",
    "    generate_model_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default log configuration\n",
    "LOG_LEVEL = logging.INFO\n",
    "LOG_FORMAT = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Initial logging setup\n",
    "logging.basicConfig(\n",
    "    filename=None,\n",
    "    level=LOG_LEVEL,\n",
    "    format=LOG_FORMAT,\n",
    "    datefmt=DATE_FORMAT\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def change_log_file(new_filename: str):\n",
    "    \"\"\"\n",
    "    Updates the logger handler to point to a different log file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    log_dir = os.path.dirname(new_filename)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)  # Create directories if they don't exist\n",
    "\n",
    "    # Remove all old handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "        handler.close()\n",
    "\n",
    "    # Create a new file handler\n",
    "    new_handler = logging.FileHandler(new_filename)\n",
    "    new_handler.setLevel(LOG_LEVEL)  # Reset level\n",
    "    new_handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))  # Reset formatter\n",
    "\n",
    "    # Add new handler to logger\n",
    "    logger.addHandler(new_handler)\n",
    "\n",
    "def log_params(log_filepath: str, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Writes all parameters passed to the function into a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        log_filepath (str): Path where the JSON file will be written.\n",
    "        **kwargs: Any number of keyword arguments representing parameter names and values.\n",
    "    \"\"\"\n",
    "    with open(log_filepath, \"w\") as file:\n",
    "        json.dump(kwargs, file, separators=(\",\", \":\"))  # Compact JSON\n",
    "        file.write(\"\\n\")  # Ensures a newline at the end\n",
    "\n",
    "    print(f\"Parameters written to {log_filepath}\")\n",
    "\n",
    "def log_vectorstore_retrieval(query: str, similar_documents: str):\n",
    "    \"\"\"Logs the most similar documents from vectorstore\"\"\"\n",
    "\n",
    "    logger.info(\"===\")\n",
    "    logger.info(f\"Query: {query}\")\n",
    "    logger.info(f\"Retrieved similar documents: {similar_documents}\")\n",
    "    logger.info(\"===\")\n",
    "\n",
    "def log_message(msg: Msg):\n",
    "    \"\"\"Logs a message object\"\"\" \n",
    "\n",
    "    logger.info(msg.name + \": \" + msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSV Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging utilities\n",
    "class StructuredLogger:\n",
    "    \"\"\"\n",
    "    A logger that outputs to TSV files for structured logging of LLM interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define all possible message types for documentation\n",
    "    MESSAGE_TYPES = [\n",
    "        \"vectorstore_add\",           # Adding a case to vectorstore\n",
    "        \"vectorstore_retrieve\",      # Retrieving similar cases\n",
    "        \"vectorstore_summarize\",     # Summarizing cached decisions\n",
    "        \"pro_argument_prompt\",       # Input prompt for pro arguments\n",
    "        \"pro_argument_response\",     # Response from pro argument LLM\n",
    "        \"con_argument_prompt\",       # Input prompt for con arguments\n",
    "        \"con_argument_response\",     # Response from con argument LLM\n",
    "        \"judge_prompt\",              # Input prompt for judge\n",
    "        \"judge_response\",            # Response from judge LLM\n",
    "        \"fast_prompt\",               # Input prompt for fast LLM\n",
    "        \"fast_response\",             # Fast LLM respnose\n",
    "        \"slow_prompt\",               # Input prompt for slow LLM\n",
    "        \"slow_response\",             # Slow LLM response\n",
    "        \n",
    "        # Werewolf-specific messages\n",
    "        \"phase\",\n",
    "\n",
    "        # Monopoly-specific messages\n",
    "        \"property_details\",          # Property information being considered\n",
    "        \"player_stats\",              # Player statistics at decision time\n",
    "        \"default_decision\",          # Default player's decision\n",
    "        \"step_num\",                  # Step number in game\n",
    "        \"game_init\",                 # Game initialization config\n",
    "        \"player_summary\"             # Players' state at decision time\n",
    "    ]\n",
    "\n",
    "    def __init__(self, tsv_filepath: str):\n",
    "        \"\"\"\n",
    "        Initialize the logger with a base filepath.\n",
    "\n",
    "        Args:\n",
    "            tsv_filepath: Filepath for TSV log files (includes timestamp)\n",
    "        \"\"\"\n",
    "        self.tsv_filepath = tsv_filepath\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.tsv_filepath), exist_ok=True)\n",
    "\n",
    "        # Initialize TSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(self.tsv_filepath):\n",
    "            with open(self.tsv_filepath, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "                writer = csv.writer(tsvfile, delimiter=\"\\t\")\n",
    "                writer.writerow([\"timestamp\", \"message_type\", \"message\"])\n",
    "\n",
    "    def log(self, message_type: str, message_data: Union[str, Dict[str, Any], list[Any], BaseModel]) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to the TSV file.\n",
    "\n",
    "        Args:\n",
    "            message_type: Type of message (one of MESSAGE_TYPES)\n",
    "            message_data: Data to be logged (will be converted to JSON if not already a string)\n",
    "        \"\"\"\n",
    "        if message_type not in self.MESSAGE_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"Invalid message_type: {message_type}. Must be one of {self.MESSAGE_TYPES}\"\n",
    "            )\n",
    "\n",
    "        # Convert message_data to string if it's not already\n",
    "        if isinstance(message_data, BaseModel):\n",
    "            message = json.dumps(message_data.model_dump(), ensure_ascii=False)\n",
    "        elif isinstance(message_data, (dict, list)):\n",
    "            message = json.dumps(message_data, ensure_ascii=False)\n",
    "        else:\n",
    "            message = str(message_data)\n",
    "\n",
    "        timestamp = datetime.now().isoformat()\n",
    "\n",
    "        # Append to TSV file\n",
    "        with open(self.tsv_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter=\"\\t\", quotechar='`', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([timestamp, message_type, message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementations\n",
    "Vector store classes with both uncompressed, and reflective functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        \n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            self.messages.append(msg.name + \": \" + msg.content)\n",
    "        \n",
    "    def summarize_cycle(self, survivors: list, secondary_model: str, cycle_type: str = \"day\", tsv_logger: Optional[StructuredLogger] = None):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            survivors (list): List of current alive players.\n",
    "            secondary_model (str): Name of secondary model.\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Combine all messages since the last summary\n",
    "        history = \"\\n\".join(self.messages)\n",
    "\n",
    "        # Use retry_with_fallback with a lambda function\n",
    "        secondary_model_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(\n",
    "                model=secondary_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"\\n\".join([ \n",
    "                        f\"You are a strategic decision-maker reviewing past decisions in Werewolf.\",\n",
    "                        f\"Survivors: {', '.join([survivor.name for survivor in survivors])}\",\n",
    "                        f\"Summarize the decisions and reflect on the most important implications for future strategic decisions.\"\n",
    "                    ])},\n",
    "                    {\"role\": \"user\", \"content\": history},\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summary = secondary_model_response.text\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "        \n",
    "        # add summarized vectorstore entry to tsv logger\n",
    "        tsv_logger.log(\"vectorstore_summarize\", {\n",
    "            \"messages\": convert_messages_to_string(self.messages), \n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "        # Clear messages for the next cycle\n",
    "        self.messages.clear()\n",
    "\n",
    "    def get_relevant_summaries_context(self, query: str, top_k: int = 1) -> str:\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        if len(self.summaries) > 0:\n",
    "            query_embedding = self._get_embedding(query)\n",
    "            distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "            results = [\n",
    "                self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "            ]\n",
    "            return \"\\n\".join(list(set(results)))\n",
    "        return \"\"\n",
    "    \n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join(\"vectorstore\", \"fastslow\", timestamp, str(game_num))\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(dir_path, f\"{player_role}.log\")\n",
    "\n",
    "        # Save messages to the log file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(\"\\n\".join(self.summaries))\n",
    "\n",
    "        print(f\"Messages saved to {file_path}\")\n",
    "\n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = retry_with_fallback(lambda: client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        ))\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]], tsv_logger: Optional[StructuredLogger] = None):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        # add embedding of message to vector store and add log entry\n",
    "        embeddings = []\n",
    "        for msg in message:\n",
    "            embeddings.append(self._get_embedding(msg.name + \": \" + msg.content))\n",
    "            tsv_logger.log(\"vectorstore_add\", {\n",
    "                \"player\": msg.name, \n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.messages.extend(message)\n",
    "\n",
    "    def get_relevant_messages(self, query: str, top_k: int = 10) -> list:\n",
    "        \"\"\"Retrieve the top-k most relevant messages.\"\"\"\n",
    "        if not self.messages:\n",
    "            return []\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "\n",
    "        results = [\n",
    "            self.messages[idx] for idx in indices[0] if idx < len(self.messages)\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "        # Construct the directory path\n",
    "        dir_path = os.path.join(\"vectorstore\", \"fastslow\", timestamp, str(game_num))\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(dir_path, f\"{player_role}.log\")\n",
    "\n",
    "        # Save messages to the log file\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(convert_messages_to_string(self.messages))\n",
    "\n",
    "        print(f\"Messages saved to {file_path}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agents (edit here)\n",
    "Below are implementations of a custom Agent utilitizing the Slow Mind Fast Mind Architecture with a reflective vector store, and an agent that generates responses based off a normal vector store search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlowFastMindAgent(AgentBase):\n",
    "    \"\"\"An agent that generates response using the Fast Mind Slow Mind Architecture.\n",
    "    \n",
    "    Includes a FAISS vectorstore for memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        fast_mind_model_config_name: str,\n",
    "        slow_mind_model_config_name: str,\n",
    "        reflect_before_vectorstore: bool = False,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "        tsv_logger: Optional[StructuredLogger] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the custom agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            sys_prompt (`Optional[str]`, defaults to `None`):\n",
    "                The system prompt of the agent, which can be passed by args\n",
    "                or hard-coded in the agent.\n",
    "            fast_mind_model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load fast mind \n",
    "                model from configuration.\n",
    "            slow_mind_model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load slow mind \n",
    "                model from configuration.\n",
    "            max_retries (`Optional[int]`, defaults to `None`):\n",
    "                The maximum number of retries when failed to parse the model\n",
    "                output.      \n",
    "            similarity_top_k (`int`, defaults to `None`):\n",
    "                The top k most similar items in the vectorstore that will be\n",
    "                used in the context to the model\n",
    "            openai_api_key (`str`)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "        )\n",
    "        self.parser = None\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.fast_mind_model_config_name = fast_mind_model_config_name \n",
    "        self.slow_mind_model_config_name = slow_mind_model_config_name\n",
    "        self.tsv_logger=tsv_logger\n",
    "\n",
    "        openai.api_key = openai_api_key\n",
    "\n",
    "        # Load the two models from Agentscope's ModelManager\n",
    "        model_manager = ModelManager.get_instance()\n",
    "        self.fast_mind_model = model_manager.get_model_by_config_name(fast_mind_model_config_name)\n",
    "        self.slow_mind_model = model_manager.get_model_by_config_name(slow_mind_model_config_name)\n",
    "\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which will provide 1) format instruction; 2)\n",
    "        response parsing; 3) filtering fields when returning message, storing\n",
    "        message in memory. So developers only need to change the\n",
    "        parser, and the agent will work as expected.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def _build_prompt(self, x: Optional[Msg]):\n",
    "        \"\"\"Helper to build a prompt for either fast or slow mind.\"\"\"\n",
    "        \n",
    "        query = Queries.werewolf_discussion_query\n",
    "        if self.reflect_before_vectorstore:\n",
    "            summary_context = self.memory.get_relevant_summaries_context(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "            # Log file entry\n",
    "            log_vectorstore_retrieval(query, summary_context)\n",
    "            \n",
    "            # Log vector store retrieval results to structured TSV\n",
    "            if self.tsv_logger:\n",
    "                serializable_results = [\n",
    "                    {\"content\": doc, \"query\": query}\n",
    "                    for doc in summary_context\n",
    "                ]\n",
    "                self.tsv_logger.log(\"vectorstore_retrieve\", serializable_results)\n",
    "\n",
    "            prompt = self.fast_mind_model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(\"system\", f\"Summary of relevant past conversations: {summary_context}\", role=\"system\"),\n",
    "                Msg(\"system\", self.parser.format_instruction, role=\"system\"),\n",
    "            )\n",
    "        else:\n",
    "            relevant_msgs = self.memory.get_relevant_messages(query, self.similarity_top_k)\n",
    "            prompt = self.fast_mind_model.format(\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_msgs and Msg(\"system\", f\"Relevant messages from memory: {relevant_msgs}\", \"system\"),\n",
    "                x,\n",
    "                Msg(\"system\", self.parser.format_instruction, role=\"system\"),\n",
    "            )\n",
    "            log_vectorstore_retrieval(query, convert_messages_to_string(relevant_msgs))\n",
    "            \n",
    "            # Log vector store retrieval results to structured TSV\n",
    "            if self.tsv_logger:\n",
    "                serializable_results = [\n",
    "                    {\"content\": doc.name + \": \" + doc.content, \"query\": query}\n",
    "                    for doc in relevant_msgs\n",
    "                ]\n",
    "                self.tsv_logger.log(\"vectorstore_retrieve\", serializable_results)\n",
    "        return prompt\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Reply function of the agent.\n",
    "        Processes the input data, generates a prompt using the current\n",
    "        dialogue memory and system prompt, and invokes the language\n",
    "        model to produce a response. The response is then formatted\n",
    "        and added to the dialogue memory.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        model_prompt = self._build_prompt(x)\n",
    "\n",
    "        raw_response= retry_with_fallback(\n",
    "            lambda: generate_model_response(self.fast_mind_model_config_name, model_prompt)\n",
    "        )\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        # Parse the raw response\n",
    "        parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=json.dumps(parsed_response),\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory.add_message(message=vectorstore_entry)\n",
    "        else:\n",
    "            self.memory.add_message(message=vectorstore_entry, tsv_logger=self.tsv_logger)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed_response),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed_response),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def fast_slow_mind(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Fast Mind Slow Mind decision making function of the agent.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        model_prompt = self._build_prompt(x)\n",
    "        \n",
    "        # tsv log fast prompt\n",
    "        self.tsv_logger.log(\"fast_prompt\", model_prompt[0][\"content\"])\n",
    "        \n",
    "        fast_resp = retry_with_fallback(\n",
    "            lambda: generate_model_response(self.fast_mind_model_config_name, model_prompt)\n",
    "        )\n",
    "\n",
    "        # Parse the raw response\n",
    "        parsed_response = self.parser.parse(fast_resp).parsed\n",
    "\n",
    "        # Obtain 'uncertainty_score'\n",
    "        try:\n",
    "            data = json.loads(fast_resp.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "            uncertainty = float(data.get(\"uncertainty_score\", 0.0))\n",
    "            thought = data.get(\"thought\")\n",
    "            vote = data.get(\"vote\")\n",
    "        except:\n",
    "            uncertainty = 1.0\n",
    "            thought = \"\"\n",
    "            vote = \"\"\n",
    "        \n",
    "        logger.info(\"===\")\n",
    "        logger.info(f\"Fast Response: {json.dumps(parsed_response)}\")\n",
    "        logger.info(f\"Fast Uncertainty: {uncertainty}\")\n",
    "        logger.info(\"===\")\n",
    "        \n",
    "        # tsv log fast response\n",
    "        self.tsv_logger.log(\"fast_response\", {\n",
    "            \"thought\": thought,\n",
    "            \"vote\": vote,\n",
    "            \"uncertainty\": uncertainty\n",
    "        })\n",
    "\n",
    "        if uncertainty >= 0.6:\n",
    "            # tsv log slow prompt\n",
    "            self.tsv_logger.log(\"slow_prompt\", model_prompt[0][\"content\"])\n",
    "            raw_response = retry_with_fallback(\n",
    "                lambda: generate_model_response(self.slow_mind_model_config_name, model_prompt)\n",
    "            )\n",
    "\n",
    "            parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "            logger.info(\"===\")\n",
    "            logger.info(f\"Slow Response: {json.dumps(parsed_response)}\")\n",
    "            logger.info(\"===\")\n",
    "            \n",
    "            # tsv log slow response\n",
    "            self.tsv_logger.log(\"slow_response\", parsed_response)\n",
    "        else:\n",
    "            raw_response = fast_resp\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=json.dumps(parsed_response),\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory.add_message(message=vectorstore_entry)\n",
    "        else:\n",
    "            self.memory.add_message(message=vectorstore_entry, tsv_logger=self.tsv_logger)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed_response),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed_response),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]], role: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Record incoming messages in memory without generating a reply.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]): The message(s) to be stored in memory for future context.\n",
    "            role (`str`, optional): Role of the observer\n",
    "        \"\"\"\n",
    "        if role == \"seer\" and x is not None:\n",
    "            if self.reflect_before_vectorstore:\n",
    "                self.memory.add_message(message=vectorstore_entry)\n",
    "            else:\n",
    "                self.memory.add_message(message=vectorstore_entry, tsv_logger=self.tsv_logger)\n",
    "            \n",
    "    def summarize_cycle(self, survivors: list, slow_mind_model: str, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary at the end of a day or night cycle.\n",
    "        \n",
    "        Args:\n",
    "            survivors: (list): List of current survivors.\n",
    "            slow_mind_model (str): Name of the slow mind model.\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory.summarize_cycle(survivors, slow_mind_model, cycle_type=cycle_type, tsv_logger=self.tsv_logger)\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str=\"werewolf\"):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.save_context(timestamp, game_num, player_role)\n",
    "        \n",
    "class NormalAgent(AgentBase):\n",
    "    \"\"\"Agent with normal uncompressed VectorStore,\n",
    "    no other underlying decision making architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        max_retries: Optional[int] = 3,\n",
    "        similarity_top_k: int = None,\n",
    "        openai_api_key: str = \"\",\n",
    "        tsv_logger: Optional[StructuredLogger] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the dict dialog agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            sys_prompt (`Optional[str]`, defaults to `None`):\n",
    "                The system prompt of the agent, which can be passed by args\n",
    "                or hard-coded in the agent.\n",
    "            model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load model from\n",
    "                configuration.\n",
    "            max_retries (`Optional[int]`, defaults to `None`):\n",
    "                The maximum number of retries when failed to parse the model\n",
    "                output.\n",
    "        \"\"\"  # noqa\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        self.max_retries = max_retries\n",
    "        openai.api_key = openai_api_key  # Set OpenAI API key\n",
    "        self.memory = VectorstoreMemory()  # Use FAISS vector store for memory\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "        self.model_config_name = model_config_name\n",
    "        self.tsv_logger=tsv_logger\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which will provide 1) format instruction; 2)\n",
    "        response parsing; 3) filtering fields when returning message, storing\n",
    "        message in memory. So developers only need to change the\n",
    "        parser, and the agent will work as expected.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Reply function of the agent.\n",
    "        Processes the input data, generates a prompt using the current\n",
    "        dialogue memory and system prompt, and invokes the language\n",
    "        model to produce a response. The response is then formatted\n",
    "        and added to the dialogue memory.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        query = Queries.non_werewolf_discussion_query\n",
    "\n",
    "        relevant_msgs = self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "\n",
    "        log_vectorstore_retrieval(query, convert_messages_to_string(relevant_msgs))\n",
    "        \n",
    "        # Log vector store retrieval results to structured TSV\n",
    "        if self.tsv_logger:\n",
    "            serializable_results = [\n",
    "                {\"content\": doc.name + \": \" + doc.content, \"query\": query}\n",
    "                for doc in relevant_msgs\n",
    "            ]\n",
    "            self.tsv_logger.log(\"vectorstore_retrieve\", serializable_results)\n",
    "\n",
    "        # prepare prompt with retrieved messages similar to input message\n",
    "        prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            relevant_msgs and Msg(\"system\", f\"Relevant messages from memory: {relevant_msgs}\", \"system\"),\n",
    "            x,\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # Call llm\n",
    "        raw_response = retry_with_fallback(\n",
    "            lambda: generate_model_response(self.model_config_name, prompt)\n",
    "        )\n",
    "\n",
    "        self.speak(raw_response.text)\n",
    "\n",
    "        # Parsing the raw response\n",
    "        parsed_response = self.parser.parse(raw_response).parsed\n",
    "\n",
    "        vectorstore_entry = Msg(\n",
    "            self.name,\n",
    "            content=json.dumps(parsed_response),\n",
    "            role=\"assistant\",\n",
    "        )\n",
    "\n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=vectorstore_entry, tsv_logger=self.tsv_logger)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed_response),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed_response),\n",
    "        )\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]], role: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Record incoming messages in memory without generating a reply.\n",
    "\n",
    "        Args:\n",
    "            x (Union[Msg, Sequence[Msg]]): The message(s) to be stored in memory for future context.\n",
    "            role (`str`, optional): Role of the observer\n",
    "        \"\"\"\n",
    "        if role == \"seer\" and x is not None:\n",
    "            self.memory.add_message(message=x, tsv_logger=self.tsv_logger)\n",
    "\n",
    "    def save_context(self, timestamp: str, game_num: int, player_role: str=\"werewolf\"):\n",
    "        \"\"\"\n",
    "        Save self.messages to a log file in the format:\n",
    "        vectorstore/[timestamp]/[game_num]/[player_role].log\n",
    "\n",
    "        Args:\n",
    "            game_num (int): The game index.\n",
    "            player_role (str): The role of the player.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.save_context(timestamp, game_num, player_role)\n",
    "            \n",
    "            \n",
    "# Register custom classes\n",
    "agentscope.agents.SlowFastMindAgent = SlowFastMindAgent\n",
    "agentscope.agents.NormalAgent = NormalAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Parser (edit minimally)\n",
    "Below are the prompts that control and run the Werwolf game. They are implemented using the built in AgentScope MarkdownJsonDictParser which allows us to generate responses in a dictionary format that is compatible with our game. Currently structured to obtain both the private thoughts of an Agent which remain to themselves and showcase their reasoning, and the words that they speak to other agents.\n",
    "\n",
    "Since these prompts control the game logic itself, we should try to limit the prompt edits here to a minimum, but we can make edits to the parser to control the reasoning process a particular role goes through before making a decision, eg. instead of a thought field, we ask it to make ask it to provide a reason to vote for a particular agent, and a reason to not vote for a particular agent, then ask it to make a final decision based off those fields. Fields other than \"thought\" however should not be touched as they play a critical role to the control flow of the game.\n",
    "\n",
    "More details on the AgentScope parsers can be found here https://doc.agentscope.io/build_tutorial/structured_output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    \"\"\"Prompts for werewolf game\"\"\"\n",
    "\n",
    "    rules = \"\"\"\n",
    "        Player roles: In werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\n",
    "        - Werewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\n",
    "        - Villagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\n",
    "        - Seer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\n",
    "        - Witch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\n",
    "\n",
    "        Game Rule: The game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\n",
    "        1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\n",
    "        2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\n",
    "\n",
    "        Victory Condition: For werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers. For villagers, they win if they identify and eliminate all of the werewolves in the group.\n",
    "\n",
    "        Constraints:\n",
    "        1. Your response should be in the first person.\n",
    "        2. This is a conversational game. You should response only based on the conversation history and your strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    two_phases = \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    to_wolves = (\n",
    "        \"{}, if you are the only werewolf, eliminate a player. Otherwise, \"\n",
    "        \"discuss with your teammates and reach an agreement.\"\n",
    "    )\n",
    "\n",
    "    wolves_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"finish_discussion\": \"whether the discussion reached an agreement or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"finish_discussion\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"finish_discussion\"],\n",
    "    )\n",
    "\n",
    "    to_wolves_vote = \"Which player do you vote to kill?\"\n",
    "\n",
    "    # Adding uncertainty score for Fast Mind Slow Mind Architecture\n",
    "    wolves_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "            \"uncertainty_score\": \"score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\", \"uncertainty_score\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_wolves_res = \"The player with the most votes is {}.\"\n",
    "\n",
    "    to_witch_resurrect = (\n",
    "        \"{witch_name}, you're the witch. Tonight {dead_name} is eliminated. \"\n",
    "        \"Would you like to resurrect {dead_name}?\"\n",
    "    )\n",
    "\n",
    "    to_witch_resurrect_no = \"The witch has chosen not to resurrect the player.\"\n",
    "    to_witch_resurrect_yes = \"The witch has chosen to resurrect the player.\"\n",
    "\n",
    "    witch_resurrect_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"whether to resurrect the player and the reason\",\n",
    "            \"resurrect\": \"whether to resurrect the player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"resurrect\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"resurrect\"],\n",
    "    )\n",
    "\n",
    "    to_witch_poison = (\n",
    "        \"Would you like to eliminate one player? If yes, \"\n",
    "        \"specify the player_name.\"\n",
    "    )\n",
    "\n",
    "    witch_poison_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"eliminate\": \"whether to eliminate a player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"eliminate\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"eliminate\"],\n",
    "    )\n",
    "\n",
    "    to_seer = (\n",
    "        \"{}, you're the seer. Which player in {} would you like to check \"\n",
    "        \"tonight?\"\n",
    "    )\n",
    "\n",
    "    seer_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    to_seer_result = \"Okay, the role of {} is a {}.\"\n",
    "\n",
    "    to_all_danger = (\n",
    "        \"The day is coming, all the players open your eyes. Last night, \"\n",
    "        \"the following player(s) has been eliminated: {}.\"\n",
    "    )\n",
    "\n",
    "    to_all_peace = (\n",
    "        \"The day is coming, all the players open your eyes. Last night is \"\n",
    "        \"peaceful, no player is eliminated.\"\n",
    "    )\n",
    "\n",
    "    to_all_discuss = (\n",
    "        \"Now the alive players are {}. Given the game rules and your role, \"\n",
    "        \"based on the situation and the information you gain, to vote a \"\n",
    "        \"player eliminated among alive players and to win the game, what do \"\n",
    "        \"you want to say to others? You can decide whether to reveal your \"\n",
    "        \"role.\"\n",
    "    )\n",
    "\n",
    "    survivors_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    # Adding uncertainty score for Fast Mind Slow Mind Architecture\n",
    "    survivors_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "            \"uncertainty_score\": \"score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain\",\n",
    "\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\", \"uncertainty_score\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_all_vote = (\n",
    "        \"Given the game rules and your role, based on the situation and the\"\n",
    "        \" information you gain, to win the game, it's time to vote one player\"\n",
    "        \" eliminated among the alive players. Which player do you vote to \"\n",
    "        \"kill?\"\n",
    "    )\n",
    "\n",
    "    to_all_res = \"{} has been voted out.\"\n",
    "\n",
    "    to_all_wolf_win = (\n",
    "        \"The werewolves have prevailed and taken over the village. Better \"\n",
    "        \"luck next time!\"\n",
    "    )\n",
    "\n",
    "    to_all_village_win = (\n",
    "        \"The game is over. The werewolves have been defeated, and the village \"\n",
    "        \"is safe once again!\"\n",
    "    )\n",
    "\n",
    "    to_all_continue = \"The game goes on.\"\n",
    "\n",
    "    \n",
    "# Moderator message function\n",
    "HostMsg = partial(Msg, name=\"Moderator\", role=\"assistant\", echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorstore Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries:\n",
    "    \"\"\"Queries for the vectorstore based on certain points in game\"\"\"\n",
    "\n",
    "    # Retrieves discussions about suspicions of non-werewolves to help identify potential enemies.\n",
    "    werewolf_discussion_query = \"Discussions related to villagers, seer, witch.\"\n",
    "\n",
    "    # Retrieves discussions focused on identifying werewolves.\n",
    "    non_werewolf_discussion_query = \"Discussions related to werewolves.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Initialization (edit here)\n",
    "To initialize the agents, you must define their parameters and settings in the config objects that are passed in for initialization. There is a model config, which defines the base foundational model being used, and an agent config, which defines each of the agents being used in the game, including which model their using, their name, and what type of Agent they are (based off the agent classes we defined earlier). \n",
    "\n",
    "Pay particular attention to the system prompt, this is what defines the rules of the game to the agent and gives them the role and what their responsibilities are, we could perhaps do some prompt engineering with that.\n",
    "\n",
    "Also we can play around with the settings of the game, eg. max rounds, how many werewolves we have, etc. Just make sure to update the roles, witch, seer objects below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (don't edit)\n",
    "Functions to check and update game state throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_with_fallback(func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Tries to execute the given function. If it fails, waits 5 seconds and retries once.\n",
    "    If it fails again, exits the script.\n",
    "\n",
    "    Args:\n",
    "        func (Callable[..., Any]): The function to execute.\n",
    "        *args (Any): Positional arguments to pass to the function.\n",
    "        **kwargs (Any): Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        Any: The result of the function if successful.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)  # Try executing the function\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}. Retrying in 5 seconds...\")\n",
    "        time.sleep(5)  # Wait 5 seconds before retrying\n",
    "\n",
    "        try:\n",
    "            return func(*args, **kwargs)  # Try again\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}. Failed again. Stopping execution.\")\n",
    "            sys.exit(1)  # Exit the script if it fails twice\n",
    "\n",
    "def load_json_response(response: str) -> dict:\n",
    "    \"\"\"Converts a json response to a python dictionary\"\"\"\n",
    "    return json.loads(response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "\n",
    "def convert_messages_to_string(relevant_messages: list) -> str:\n",
    "    \"\"\"Takes a list of messages, and converts them to a string separated by new line\"\"\"\n",
    "    return '\\n'.join([f\"{msg.name}: {msg.content}\" for msg in relevant_messages])\n",
    "\n",
    "def majority_vote(votes: list) -> Any:\n",
    "    \"\"\"Given a list of votes, return the name with the highest frequency.\"\"\"\n",
    "    votes_valid = [v for v in votes if v != \"Abstain\"]\n",
    "    if not votes_valid:\n",
    "        return \"No Votes\"\n",
    "    unique_vals, counts = np.unique(votes_valid, return_counts=True)\n",
    "    return unique_vals[np.argmax(counts)]\n",
    "\n",
    "def update_alive_players(game_state: dict, survivors: list, wolves: list, dead_names):\n",
    "    \"\"\"\n",
    "    Removes 'dead_names' from 'game_state[\"survivors\"]', updates game_state[\"dead\"],\n",
    "    and returns updated survivors and wolves lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(dead_names, list):\n",
    "        dead_names = [dead_names]\n",
    "    for d in dead_names:\n",
    "        if d in game_state[\"survivors\"]:\n",
    "            game_state[\"survivors\"].remove(d)\n",
    "        if d not in game_state[\"dead\"]:\n",
    "            game_state[\"dead\"].append(d)\n",
    "\n",
    "    # Rebuild survivors / wolves lists\n",
    "    new_survivors = [s for s in survivors if s.name in game_state[\"survivors\"]]\n",
    "    new_wolves = [w for w in wolves if w.name in game_state[\"survivors\"]]\n",
    "    return new_survivors, new_wolves\n",
    "\n",
    "def check_winning(game_state: dict, survivors: list, wolves: list, host: str) -> bool:\n",
    "    \"\"\"\n",
    "    If #werewolves * 2 >= #survivors => werewolves instantly win.\n",
    "    If all werewolves are dead => villagers instantly win.\n",
    "    Otherwise => game continues.\n",
    "    \"\"\"\n",
    "    if len(wolves) * 2 >= len(survivors):\n",
    "        msg = Msg(host, Prompts.to_all_wolf_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"werewolves\"\n",
    "        log_message(msg)\n",
    "        return True\n",
    "    if survivors and not wolves:\n",
    "        msg = Msg(host, Prompts.to_all_village_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"villagers\"\n",
    "        logger.info(f\"Moderator: {Prompts.to_all_village_win}\")\n",
    "        log_message(msg)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate_log_filepath(basepath: str, \n",
    "                          reflect_before_vectorstore: bool, \n",
    "                          random_seed: int, \n",
    "                          timestamp: str, \n",
    "                          game_num: int) -> str:\n",
    "    \"\"\"Generate log file path with EST timestamp.\"\"\"\n",
    "    return f\"{basepath}/{'reflect' if reflect_before_vectorstore else 'noreflect'}/{random_seed}/{timestamp}/fastslow_raw_game{game_num}.log\"\n",
    "\n",
    "def save_results(win_rate_filepath: str, row: dict):\n",
    "    \"\"\"\n",
    "    Append a row to a CSV file. If doesn't exist, create with headers.\n",
    "\n",
    "    row = {\n",
    "      \"raw_log_filepath\": <str>,\n",
    "      \"custom_agent_won\": <bool>,\n",
    "    }\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(win_rate_filepath)\n",
    "    with open(win_rate_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"raw_log_filepath\", \"custom_agent_won\"])\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "    print(f\"Row added to {win_rate_filepath}: {row}\")\n",
    "\n",
    "def custom_agent_won(game_state: dict, alive_agents: list, wolf_agents: list):\n",
    "    \"\"\"Records whether custom agent won (even if game didn't complete)\"\"\"\n",
    "\n",
    "    # If game is completed, check if werewolves have won\n",
    "    if game_state[\"endgame\"]:\n",
    "        if game_state[\"winner\"] == \"werewolves\":\n",
    "            return True \n",
    "        return False\n",
    "    \n",
    "    # If game not completed, then custom agent wins because it is still alive and eliminated another player\n",
    "    return True\n",
    "\n",
    "def get_available_targets(survivors: list, wolves: list):\n",
    "    \"\"\"Returns list of currently alive non-werewolves\"\"\"\n",
    "    return [survivor.name for survivor in survivors if survivor not in wolves]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Game \n",
    "Following is a function that game through the various Night and Day phases, taking different actions for each agent based on their roles. Multi agent functionality and communication is facilitated through the AgentScope Pipeline and MsgHub, more detailed documentation found here https://doc.agentscope.io/build_api/agentscope.pipelines.pipeline.html#module-agentscope.pipelines.pipeline and https://doc.agentscope.io/build_api/agentscope.msghub.html#module-agentscope.msghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(\n",
    "    slow_mind_model: str,\n",
    "    max_days_per_game: int,\n",
    "    reflect_before_vectorstore: bool,\n",
    "    max_werewolf_discussion_round: int,\n",
    "    wolves: list,\n",
    "    seer,\n",
    "    witch,\n",
    "    roles,\n",
    "    survivors,\n",
    "    game_state,\n",
    "    tsv_logger: Optional[StructuredLogger] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a single game, storing logs in game_log. This example merges both\n",
    "    day/night flow, werewolf talk, witch usage, seer usage, etc. Then\n",
    "    saves logs and results.\n",
    "    \"\"\"\n",
    "    for day_i in range(1, max_days_per_game + 1):\n",
    "        \n",
    "        tsv_logger.log(\"phase\", \"Night Time\")\n",
    "\n",
    "        # 1) Night Phase: Werewolves discussion\n",
    "        hint = HostMsg(content=Prompts.to_wolves.format(n2s(wolves)))\n",
    "        log_message(hint)\n",
    "        logger.info(f\"Moderator: {hint.content}\")\n",
    "        with msghub(wolves, announcement=hint) as hub:\n",
    "            set_parsers(wolves, Prompts.wolves_discuss_parser)\n",
    "            for r in range(max_werewolf_discussion_round):\n",
    "                x = retry_with_fallback(lambda: sequentialpipeline(wolves))\n",
    "                logger.info(f\"Werewolves discussion: {x.content}\")\n",
    "                if x.metadata.get(\"finish_discussion\", False):\n",
    "                    break\n",
    "            # Then vote\n",
    "            set_parsers(wolves, Prompts.wolves_vote_parser)\n",
    "            hint_vote = HostMsg(content=Prompts.to_wolves_vote)\n",
    "            log_message(hint_vote)\n",
    "            logger.info(f\"Moderator: {hint_vote.content}\")\n",
    "            votes = [extract_name_and_id(wolf.fast_slow_mind(hint_vote).content)[0] for wolf in wolves]\n",
    "            \n",
    "            voted_out = majority_vote(votes)\n",
    "            dead_player = [voted_out]\n",
    "\n",
    "            voted_out_hint = HostMsg(content=Prompts.to_wolves_res.format(voted_out))\n",
    "            log_message(voted_out_hint)\n",
    "            hub.broadcast(voted_out_hint)\n",
    "\n",
    "        # 2) Witch Decision Night   \n",
    "        healing_used_tonight = False\n",
    "        if witch in survivors:\n",
    "            if not game_state[\"witch_healing_used\"]:\n",
    "                hint = HostMsg(\n",
    "                    content=Prompts.to_witch_resurrect.format_map(\n",
    "                        {\n",
    "                            \"witch_name\": witch.name,\n",
    "                            \"dead_name\": dead_player[0],\n",
    "                        },\n",
    "                    ),\n",
    "                )\n",
    "                log_message(hint)\n",
    "                set_parsers(witch, Prompts.witch_resurrect_parser)\n",
    "                \n",
    "                # Capture the witch's resurrection response and log it\n",
    "                resurrection_response = retry_with_fallback(lambda: witch(hint))\n",
    "                logger.info(f\"Witch resurrection response: {resurrection_response.content}\")\n",
    "\n",
    "                if resurrection_response.metadata.get(\"resurrect\", False):\n",
    "                    healing_used_tonight = True\n",
    "                    dead_player.pop()\n",
    "                    game_state[\"witch_healing_used\"] = True\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_yes)\n",
    "                    log_message(HostMsg(content=Prompts.to_witch_resurrect_yes))\n",
    "                else:\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_no)\n",
    "                    log_message(HostMsg(content=Prompts.to_witch_resurrect_no))\n",
    "\n",
    "            if not game_state[\"witch_poison_used\"] and not healing_used_tonight:\n",
    "                set_parsers(witch, Prompts.witch_poison_parser)\n",
    "                \n",
    "                # Capture the witch's poison response and log it\n",
    "                poison_response = retry_with_fallback(lambda: witch(HostMsg(content=Prompts.to_witch_poison)))\n",
    "                logger.info(f\"Witch Poison Response: {poison_response.content}\")\n",
    "\n",
    "                if poison_response.metadata.get(\"eliminate\", False):\n",
    "                    target_player = extract_name_and_id(poison_response.content)[0]\n",
    "                    dead_player.append(target_player)\n",
    "                    game_state[\"witch_poison_used\"] = True\n",
    "                    logger.info(f\"The witch has chosen to poison {target_player}.\")\n",
    "                else:\n",
    "                    logger.info(\"The witch has chosen not to poison a player.\")\n",
    "\n",
    "        # 3) Seer checks a role\n",
    "        if seer in survivors:\n",
    "            seer_hint = HostMsg(content=Prompts.to_seer.format(seer.name, n2s(survivors)))\n",
    "            log_message(seer_hint)\n",
    "            set_parsers(seer, Prompts.seer_parser)\n",
    "            x = seer.reply(seer_hint)\n",
    "            logger.info(f\"Seer response: {x.content}\")\n",
    "            pl, idx = extract_name_and_id(x.content)\n",
    "            role_name = roles[idx] if idx < len(roles) else \"villager\"\n",
    "            # Let seer quietly observe the result\n",
    "            role_msg = HostMsg(content=Prompts.to_seer_result.format(pl, role_name))\n",
    "            logger.info(f\"Moderator: {role_msg.content}\")\n",
    "            seer.observe(role_msg, \"seer\")\n",
    "\n",
    "        # 4) Update survivors after night\n",
    "        survivors, wolves = update_alive_players(game_state, survivors, wolves, dead_player)\n",
    "        if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "            break\n",
    "\n",
    "        # If reflecting, do a night reflection\n",
    "        if reflect_before_vectorstore:\n",
    "            for w in wolves:\n",
    "                w.summarize_cycle(survivors, slow_mind_model, \"night\", tsv_logger)\n",
    "\n",
    "        tsv_logger.log(\"phase\", \"Day Time\")\n",
    "                \n",
    "        # 5) Daytime discussion\n",
    "        content = (\n",
    "            Prompts.to_all_danger.format(n2s(dead_player))\n",
    "            if dead_player\n",
    "            else Prompts.to_all_peace\n",
    "        )\n",
    "        discuss_hints = [\n",
    "            HostMsg(content=content),\n",
    "            HostMsg(content=Prompts.to_all_discuss.format(n2s(survivors))),\n",
    "        ]\n",
    "        with msghub(survivors, announcement=discuss_hints) as hub:\n",
    "            set_parsers(survivors, Prompts.survivors_discuss_parser)\n",
    "            discussion_out = retry_with_fallback(lambda: sequentialpipeline(survivors))\n",
    "            logger.info(f\"Survivors Discussion: {discussion_out.content}\")\n",
    "\n",
    "            # Daytime vote\n",
    "            set_parsers(survivors, Prompts.survivors_vote_parser)\n",
    "            day_vote_hint = HostMsg(content=Prompts.to_all_vote)\n",
    "            log_message(day_vote_hint)\n",
    "            # use fast mind slow mind for werewolf vote\n",
    "            votes_day = [\n",
    "                extract_name_and_id(wolf.fast_slow_mind(day_vote_hint).content)[0] for wolf in wolves\n",
    "            ]\n",
    "\n",
    "            votes_day.extend([\n",
    "                extract_name_and_id(_.reply(day_vote_hint).content)[0] for _ in survivors if _ not in wolves\n",
    "            ])\n",
    "            day_result = majority_vote(votes_day)\n",
    "            day_result_out = HostMsg(content=Prompts.to_all_res.format(day_result))\n",
    "            hub.broadcast(day_result_out)\n",
    "            log_message(day_result_out)\n",
    "\n",
    "            survivors, wolves = update_alive_players(game_state, survivors, wolves, day_result)\n",
    "            if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "                break\n",
    "\n",
    "            # If reflecting, do a day reflection\n",
    "            if reflect_before_vectorstore:\n",
    "                for w in wolves:\n",
    "                    w.summarize_cycle(survivors, slow_mind_model, \"day\", tsv_logger)\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_continue))\n",
    "\n",
    "    return custom_agent_won(\n",
    "        game_state,\n",
    "        survivors,\n",
    "        wolves\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Entry: run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    open_ai_key: str,\n",
    "    basefilepath: str,\n",
    "    max_days_per_game: int = 3,\n",
    "    num_games: int = 2,\n",
    "    reflect_before_vectorstore: bool = False,\n",
    "    fast_mind_model: str = \"gpt-4o-mini\",\n",
    "    slow_mind_model: str = \"gpt-4o\",\n",
    "    max_werewolf_discussion_round: int = 3,\n",
    "    similarity_top_k: int = 1,\n",
    "    random_seed: int = 42\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Initialize and run multiple games of the Werewolf simulation.\n",
    "\n",
    "    Arguments:\n",
    "        open_ai_key (`str`):\n",
    "            The OpenAI API key used for model calls.\n",
    "        basefilepath (`str`):\n",
    "            Base path for logs, e.g., \"logs/game_raw\".\n",
    "        max_days_per_game (`int`, defaults to `3`):\n",
    "            Maximum number of day/night cycles per game.\n",
    "        num_games (`int`, defaults to `2`):\n",
    "            Number of games to simulate.\n",
    "        reflect_before_vectorstore (`bool`, defaults to `False`):\n",
    "            Whether to use reflective memory before vectorstore retrieval.\n",
    "        fast_mind_model (`str`, defaults to `\"gpt-4o-mini\"`):\n",
    "            Name of the model used for fast processing.\n",
    "        slow_mind_model (`str`, defaults to `\"gpt-4o\"`):\n",
    "            Name of the model used for complex reasoning.\n",
    "        max_werewolf_discussion_round (`int`, defaults to `3`):\n",
    "            Maximum number of discussion rounds for werewolves.\n",
    "        similarity_top_k (`int`, defaults to `1`):\n",
    "            Number of messages or summaries retrieved from memory.\n",
    "\n",
    "    Returns:\n",
    "        `None`\n",
    "    \"\"\"\n",
    "    # Get current timestamp\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Set openai api key\n",
    "    openai.api_key = open_ai_key\n",
    "\n",
    "    # Define model configs and settings\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": fast_mind_model,\n",
    "            \"model_name\": fast_mind_model,\n",
    "            \"api_key\": openai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 1,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": slow_mind_model,\n",
    "            \"model_name\": slow_mind_model,\n",
    "            \"api_key\": openai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 1,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "    for game_num in range(0, num_games):\n",
    "        # TSV log file params\n",
    "        game_tsv_file = f\"{basefilepath}/{'reflect' if reflect_before_vectorstore else 'noreflect'}/{random_seed}/{timestamp_str}/fastslow_raw_game{game_num}.tsv\"\n",
    "        os.makedirs(os.path.dirname(game_tsv_file), exist_ok=True)\n",
    "        tsv_logger = StructuredLogger(game_tsv_file)\n",
    "        \n",
    "        config = {\n",
    "                \"max_days_per_game\": max_days_per_game,\n",
    "                \"num_games\": num_games,\n",
    "                \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                \"fast_mind_model\": fast_mind_model,\n",
    "                \"slow_mind_model\": slow_mind_model,\n",
    "                \"max_werewolf_discussion_round\": max_werewolf_discussion_round, \n",
    "                \"similarity_top_k\": similarity_top_k\n",
    "        }\n",
    "        tsv_logger.log(\"game_init\", config)\n",
    "        \n",
    "        # Define the config settings for each agent involved\n",
    "        agent_configs = [\n",
    "            {\n",
    "                \"class\": \"SlowFastMindAgent\",\n",
    "                \"args\": {\n",
    "                    \"name\": \"Player1\",\n",
    "                    \"sys_prompt\": f\"\"\"\n",
    "                        You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                        You are Player1 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                        Rules of the game:\n",
    "\n",
    "                        {Prompts.rules}\n",
    "\n",
    "                        You are playing werewolf in this game.\n",
    "                    \"\"\",\n",
    "                    \"fast_mind_model_config_name\": fast_mind_model,\n",
    "                    \"slow_mind_model_config_name\": slow_mind_model,\n",
    "                    \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "                    \"similarity_top_k\": similarity_top_k,\n",
    "                    \"openai_api_key\": open_ai_key,\n",
    "                    \"tsv_logger\": tsv_logger\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class\": \"NormalAgent\",\n",
    "                \"args\": {\n",
    "                    \"name\": \"Player2\",\n",
    "                    \"sys_prompt\": f\"\"\" \n",
    "                        You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                        You are Player2 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                        Rules of the game: \n",
    "\n",
    "                        {Prompts.rules}\n",
    "\n",
    "                        You are playing villager in this game.\n",
    "                    \"\"\",\n",
    "                    \"model_config_name\": fast_mind_model,\n",
    "                    \"similarity_top_k\": similarity_top_k,\n",
    "                    \"openai_api_key\": open_ai_key,\n",
    "                    \"tsv_logger\": tsv_logger\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class\": \"NormalAgent\",\n",
    "                \"args\": {\n",
    "                    \"name\": \"Player3\",\n",
    "                    \"sys_prompt\": f\"\"\" \n",
    "                        You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                        You are Player3 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                        Rules of the game:\n",
    "\n",
    "                        {Prompts.rules}\n",
    "\n",
    "                        You are playing seer in this game.\n",
    "                    \"\"\",\n",
    "                    \"model_config_name\": fast_mind_model,\n",
    "                    \"similarity_top_k\": similarity_top_k,\n",
    "                    \"openai_api_key\": open_ai_key,\n",
    "                    \"tsv_logger\": tsv_logger\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"class\": \"NormalAgent\",\n",
    "                \"args\": {\n",
    "                    \"name\": \"Player4\",\n",
    "                    \"sys_prompt\": f\"\"\" \n",
    "                        You are a strategic decision-maker playing a game of werewolf.\n",
    "\n",
    "                        You are Player4 and there are totally 4 players, named Player1, Player2, Player3, Player4.\n",
    "\n",
    "                        Rules of the game:\n",
    "\n",
    "                        {Prompts.rules}\n",
    "\n",
    "                        You are playing witch in this game.\n",
    "                    \"\"\",\n",
    "                    \"model_config_name\": fast_mind_model,\n",
    "                    \"similarity_top_k\": 1,\n",
    "                    \"openai_api_key\": open_ai_key,\n",
    "                    \"tsv_logger\": tsv_logger\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Read model and agent configs, and initialize agents automatically\n",
    "        survivors = agentscope.init(\n",
    "            model_configs=model_configs,\n",
    "            agent_configs=agent_configs,\n",
    "            project=\"Werewolf\"\n",
    "        )\n",
    "\n",
    "        # Get player roles\n",
    "        roles = [\"werewolf\", \"villager\", \"seer\", \"witch\"]\n",
    "        wolves, witch, seer, villagers = [survivors[0]], survivors[-1], survivors[-2], [survivors[-3]]\n",
    "\n",
    "        # Initialize game state\n",
    "        game_state = {\n",
    "            \"werewolves\": [player.name for player in wolves],\n",
    "            \"villagers\": [player.name for player in survivors if player not in wolves],\n",
    "            \"seer\": [seer.name],\n",
    "            \"witch\": [witch.name],\n",
    "            \"survivors\": [player.name for player in survivors],\n",
    "            \"dead\": [],\n",
    "            \"witch_healing_used\": False,\n",
    "            \"witch_poison_used\": False,\n",
    "            \"endgame\": False,\n",
    "            \"winner\": None\n",
    "        }\n",
    "\n",
    "        current_log_path = generate_log_filepath(basefilepath, reflect_before_vectorstore, random_seed, timestamp_str, game_num)\n",
    "        change_log_file(current_log_path)\n",
    "\n",
    "        # Log parameters\n",
    "        log_params(\n",
    "            current_log_path, \n",
    "            basefilepath=basefilepath, \n",
    "            max_days_per_game=max_days_per_game, \n",
    "            num_games=num_games,\n",
    "            reflect_before_vectorstore=reflect_before_vectorstore, \n",
    "            max_werewolf_discussion_round=max_werewolf_discussion_round, \n",
    "            similarity_top_k=similarity_top_k,\n",
    "            random_seed=random_seed\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting game {game_num}\")\n",
    "\n",
    "        custom_agent_won = run_game(\n",
    "            slow_mind_model,\n",
    "            max_days_per_game,\n",
    "            reflect_before_vectorstore,\n",
    "            max_werewolf_discussion_round,\n",
    "            wolves,\n",
    "            seer,\n",
    "            witch,\n",
    "            roles,\n",
    "            survivors,\n",
    "            game_state,\n",
    "            tsv_logger\n",
    "        )\n",
    "\n",
    "        wolves[0].save_context(timestamp_str, game_num)\n",
    "        witch.save_context(timestamp_str, game_num, \"witch\")\n",
    "        seer.save_context(timestamp_str, game_num, \"seer\")\n",
    "        villagers[0].save_context(timestamp_str, game_num, \"villager\")\n",
    "\n",
    "        row = {\n",
    "            \"raw_log_filepath\": current_log_path,\n",
    "            \"custom_agent_won\": custom_agent_won\n",
    "        }\n",
    "        save_results(f\"{basefilepath}/{'reflect' if reflect_before_vectorstore else 'noreflect'}/{random_seed}/{timestamp_str}/fastslow_win_rate.csv\", row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:51 | INFO     | agentscope.manager._model:load_model_configs:115 - Load configs for model wrapper: gpt-4o-mini, gpt-4o\n",
      "2025-02-27 14:17:51 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:17:51 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:17:51 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o]\n",
      "2025-02-27 14:17:51 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o: 'Model [gpt-4o] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:17:51 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:17:51 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:17:51 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:17:51 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:17:51 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:17:51 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:51 - INFO - Starting game 0\n",
      "2025-02-27 14:17:51 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-27 14:17:51 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-27 14:17:51 - INFO - ===\n",
      "2025-02-27 14:17:51 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:17:51 - INFO - Retrieved similar documents: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters written to logs/fastslow/noreflect/43/20250227_141751/fastslow_raw_game0.log\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:51 - INFO - ===\n",
      "2025-02-27 14:17:53 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night’s events. It’s important we discuss this carefully since we’re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:54 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:54 - INFO - Werewolves discussion: I've been noticing some inconsistencies in what Player2 mentioned regarding last night’s events. It’s important we discuss this carefully since we’re down to just four players. What does everyone else think?\n",
      "2025-02-27 14:17:54 - INFO - Moderator: Which player do you vote to kill?\n",
      "2025-02-27 14:17:54 - INFO - Moderator: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:54 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:54 - INFO - ===\n",
      "2025-02-27 14:17:54 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:17:54 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-27 14:17:54 - INFO - ===\n",
      "2025-02-27 14:17:55 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:55 - INFO - ===\n",
      "2025-02-27 14:17:55 - INFO - Fast Response: {\"thought\": \"I suspect Player3 because of their suspicious behavior during the discussion. They seem to deflect accusations and haven't contributed much to the conversation.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.3\"}\n",
      "2025-02-27 14:17:55 - INFO - Fast Uncertainty: 0.3\n",
      "2025-02-27 14:17:55 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player3 because of their suspicious behavior during the discussion. They seem to deflect accusations and haven't contributed much to the conversation.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:56 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:56 - INFO - Moderator: The player with the most votes is Player3.\n",
      "2025-02-27 14:17:56 - INFO - Moderator: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n",
      "2025-02-27 14:17:56 - INFO - ===\n",
      "2025-02-27 14:17:56 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:17:56 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:17:56 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player3.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:57 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 could potentially have valuable information as the seer, and bringing them back might help us identify the werewolves more effectively. However, I need to consider the risks of resurrecting them in case they are not on our side.\",\"speak\": \"I will resurrect Player3 because I believe they could provide crucial insights into who the werewolves are.\",\"resurrect\": true}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:57 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:57 - INFO - Witch resurrection response: I will resurrect Player3 because I believe they could provide crucial insights into who the werewolves are.\n",
      "2025-02-27 14:17:57 - INFO - Moderator: The witch has chosen to resurrect the player.\n",
      "2025-02-27 14:17:57 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "2025-02-27 14:17:57 - INFO - ===\n",
      "2025-02-27 14:17:57 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:17:57 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:17:57 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:58 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to identify the werewolves. I suspect Player1 might have a more strategic role based on their previous discussions.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:59 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:59 - INFO - Seer response: Player1\n",
      "2025-02-27 14:17:59 - INFO - Moderator: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:59 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night is peaceful, no player is eliminated.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2, Player3 and Player4. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:17:59 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:17:59 - INFO - ===\n",
      "2025-02-27 14:17:59 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:17:59 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"I suspect Player3 because of their suspicious behavior during the discussion. They seem to deflect accusations and haven't contributed much to the conversation.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.3\"}\n",
      "2025-02-27 14:17:59 - INFO - ===\n",
      "2025-02-27 14:18:01 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information and build suspicion towards Player2, Player3, and Player4 while keeping my role hidden. It's crucial to blend in with the villagers and create distrust among them.\", \"speak\": \"Hey everyone, I think we should discuss who we trust the least. I've been observing Player2's behavior, and they seem a bit off. What do you all think?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:02 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:02 - INFO - ===\n",
      "2025-02-27 14:18:02 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:02 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:18:02 - INFO - ===\n",
      "2025-02-27 14:18:03 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"Player1 is trying to cast suspicion on me, which might be a tactic to divert attention from themselves. I need to defend myself while keeping a close eye on Player1's behavior.\", \"speak\": \"I understand your concerns, Player1, but I think we should be careful about jumping to conclusions. We need to gather more information before pointing fingers.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:03 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:03 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:03 - INFO - ===\n",
      "2025-02-27 14:18:03 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:03 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to gather information to identify the werewolves. I suspect Player1 might have a more strategic role based on their previous discussions.\", \"speak\": \"Player1\"}\n",
      "2025-02-27 14:18:03 - INFO - ===\n",
      "2025-02-27 14:18:05 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious and not reveal my role as the seer yet. Player1 seems to be unsure, and I can use this to gather more insights.\", \"speak\": \"I agree, Player2. Let's keep an open mind and discuss our thoughts more thoroughly. Does anyone have any strong suspicions?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:06 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:06 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:06 - INFO - ===\n",
      "2025-02-27 14:18:06 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:06 - INFO - Retrieved similar documents: Player4: {\"thought\": \"Player3 could potentially have valuable information as the seer, and bringing them back might help us identify the werewolves more effectively. However, I need to consider the risks of resurrecting them in case they are not on our side.\", \"speak\": \"I will resurrect Player3 because I believe they could provide crucial insights into who the werewolves are.\", \"resurrect\": true}\n",
      "2025-02-27 14:18:06 - INFO - ===\n",
      "2025-02-27 14:18:07 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to blend in and gather more information without revealing my role. Discussing suspicions could lead to interesting insights about others' identities.\", \"speak\": \"I think it's important we all share our thoughts. I don't have a strong suspicion yet, but I'm listening carefully to what everyone says.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:08 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:08 - INFO - Survivors Discussion: I think it's important we all share our thoughts. I don't have a strong suspicion yet, but I'm listening carefully to what everyone says.\n",
      "2025-02-27 14:18:08 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:08 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:08 - INFO - ===\n",
      "2025-02-27 14:18:08 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:08 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to gather information and build suspicion towards Player2, Player3, and Player4 while keeping my role hidden. It's crucial to blend in with the villagers and create distrust among them.\", \"speak\": \"Hey everyone, I think we should discuss who we trust the least. I've been observing Player2's behavior, and they seem a bit off. What do you all think?\"}\n",
      "Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-27 14:18:08 - INFO - ===\n",
      "2025-02-27 14:18:09 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:09 - INFO - ===\n",
      "2025-02-27 14:18:09 - INFO - Fast Response: {\"thought\": \"I need to eliminate a player who seems suspicious based on the previous discussions, but I have to consider that I might bring some attention to myself as well if I vote out a player who isn't a werewolf.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.3\"}\n",
      "2025-02-27 14:18:09 - INFO - Fast Uncertainty: 0.3\n",
      "2025-02-27 14:18:09 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to eliminate a player who seems suspicious based on the previous discussions, but I have to consider that I might bring some attention to myself as well if I vote out a player who isn't a werewolf.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:10 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:10 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:10 - INFO - ===\n",
      "2025-02-27 14:18:10 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:10 - INFO - Retrieved similar documents: Player2: {\"thought\": \"Player1 is trying to cast suspicion on me, which might be a tactic to divert attention from themselves. I need to defend myself while keeping a close eye on Player1's behavior.\", \"speak\": \"I understand your concerns, Player1, but I think we should be careful about jumping to conclusions. We need to gather more information before pointing fingers.\"}\n",
      "Player2: {\"thought\": \"Player1 is trying to cast suspicion on me, which might be a tactic to divert attention from themselves. I need to defend myself while keeping a close eye on Player1's behavior.\", \"speak\": \"I understand your concerns, Player1, but I think we should be careful about jumping to conclusions. We need to gather more information before pointing fingers.\"}\n",
      "2025-02-27 14:18:10 - INFO - ===\n",
      "2025-02-27 14:18:12 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"We need to identify and eliminate a werewolf, but it's tough to trust anyone since we don’t have much evidence yet. I suspect Player3 because of their quietness in discussions, which could be strategic behavior for a werewolf. It's a risk, but we need to act.\",\"vote\": \"Player3\", \"uncertainty_score\": \"0.4\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:12 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:12 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:12 - INFO - ===\n",
      "2025-02-27 14:18:12 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:12 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to gather information to identify the werewolves. I suspect Player1 might have a more strategic role based on their previous discussions.\", \"speak\": \"Player1\"}\n",
      "2025-02-27 14:18:12 - INFO - ===\n",
      "2025-02-27 14:18:13 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to strategically analyze the discussions and behaviors of the players to decide who is likely a werewolf. Based on the conversation, Player1 seems to be acting suspiciously, and I think they could be a werewolf.\", \"vote\": \"Player1\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:14 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:14 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:14 - INFO - ===\n",
      "2025-02-27 14:18:14 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:14 - INFO - Retrieved similar documents: Player4: {\"thought\": \"Player3 could potentially have valuable information as the seer, and bringing them back might help us identify the werewolves more effectively. However, I need to consider the risks of resurrecting them in case they are not on our side.\", \"speak\": \"I will resurrect Player3 because I believe they could provide crucial insights into who the werewolves are.\", \"resurrect\": true}\n",
      "2025-02-27 14:18:14 - INFO - ===\n",
      "2025-02-27 14:18:15 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to choose a player to vote off, and since I am the witch, I have a unique position. I will watch my words carefully and consider which players seem to be acting suspicious as they could potentially be werewolves. I feel Player2 has been quieter than usual and could be hiding something. Therefore, I will vote to eliminate Player2.\",\"vote\": \"Player2\", \"uncertainty_score\": 0.3}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:16 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:16 - INFO - Moderator: Player3 has been voted out.\n",
      "2025-02-27 14:18:16 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-27 14:18:16 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player3 has been voted out.\n",
      "\u001b[93mModerator\u001b[0m: The game goes on.\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:16 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:16 - INFO - ===\n",
      "2025-02-27 14:18:16 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:16 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to gather information and build suspicion towards Player2, Player3, and Player4 while keeping my role hidden. It's crucial to blend in with the villagers and create distrust among them.\", \"speak\": \"Hey everyone, I think we should discuss who we trust the least. I've been observing Player2's behavior, and they seem a bit off. What do you all think?\"}\n",
      "Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-27 14:18:16 - INFO - ===\n",
      "2025-02-27 14:18:18 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to figure out who the werewolves are while also keeping my identity hidden. It's crucial to create doubt among the other players without revealing too much about my own role.\", \"speak\": \"I think we should pay attention to how everyone votes. Player3 seemed a bit hesitant during the last vote. What do you all think about that?\", \"finish_discussion\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:18 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:18 - INFO - Werewolves discussion: I think we should pay attention to how everyone votes. Player3 seemed a bit hesitant during the last vote. What do you all think about that?\n",
      "2025-02-27 14:18:18 - INFO - Moderator: Which player do you vote to kill?\n",
      "2025-02-27 14:18:18 - INFO - Moderator: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:18 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:18 - INFO - ===\n",
      "2025-02-27 14:18:18 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:18 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to gather information and build suspicion towards Player2, Player3, and Player4 while keeping my role hidden. It's crucial to blend in with the villagers and create distrust among them.\", \"speak\": \"Hey everyone, I think we should discuss who we trust the least. I've been observing Player2's behavior, and they seem a bit off. What do you all think?\"}\n",
      "Player1: {\"thought\": \"I need to carefully observe how the other players are acting. I suspect Player2 might be a werewolf due to their inconsistent statements about previous night events. I should bring this up to gain support for my accusation.\", \"speak\": \"I've been noticing some inconsistencies in what Player2 mentioned regarding last night\\u2019s events. It\\u2019s important we discuss this carefully since we\\u2019re down to just four players. What does everyone else think?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-27 14:18:18 - INFO - ===\n",
      "2025-02-27 14:18:20 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:20 - INFO - ===\n",
      "2025-02-27 14:18:20 - INFO - Fast Response: {\"thought\": \"I suspect Player2 might be a werewolf based on their behavior during our discussions. They seem to be avoiding direct accusations and are not contributing much to the conversation.\", \"vote\": \"Player2\", \"uncertainty_score\": \"0.3\"}\n",
      "2025-02-27 14:18:20 - INFO - Fast Uncertainty: 0.3\n",
      "2025-02-27 14:18:20 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player2 might be a werewolf based on their behavior during our discussions. They seem to be avoiding direct accusations and are not contributing much to the conversation.\", \"vote\": \"Player2\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:20 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:20 - INFO - Moderator: The player with the most votes is Player2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player2.\n",
      "\u001b[93mModerator\u001b[0m: Would you like to eliminate one player? If yes, specify the player_name.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:20 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:20 - INFO - ===\n",
      "2025-02-27 14:18:20 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:20 - INFO - Retrieved similar documents: Player4: {\"thought\": \"Player3 could potentially have valuable information as the seer, and bringing them back might help us identify the werewolves more effectively. However, I need to consider the risks of resurrecting them in case they are not on our side.\", \"speak\": \"I will resurrect Player3 because I believe they could provide crucial insights into who the werewolves are.\", \"resurrect\": true}\n",
      "2025-02-27 14:18:20 - INFO - ===\n",
      "2025-02-27 14:18:21 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"As the witch, I need to consider the dynamics of the remaining players. If I eliminate someone now, it could help or hinder the villagers depending on their role. I suspect Player2 might be a werewolf based on their previous behavior. I need to take a risk for the potential benefit of the villagers.\",\"speak\": \"I believe that Player2 could be a threat to us. I propose we consider eliminating them.\",\"eliminate\": \"true\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:22 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:22 - INFO - Witch Poison Response: I believe that Player2 could be a threat to us. I propose we consider eliminating them.\n",
      "2025-02-27 14:18:22 - INFO - The witch has chosen to poison Player2.\n",
      "2025-02-27 14:18:22 - INFO - Moderator: The werewolves have prevailed and taken over the village. Better luck next time!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\0\\werewolf.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\0\\witch.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\0\\seer.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\0\\villager.log\n",
      "Row added to logs/fastslow/noreflect/43/20250227_141751/fastslow_win_rate.csv: {'raw_log_filepath': 'logs/fastslow/noreflect/43/20250227_141751/fastslow_raw_game0.log', 'custom_agent_won': True}\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.manager._model:load_model_configs:108 - config_name [gpt-4o-mini] already exists.\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.manager._model:load_model_configs:108 - config_name [gpt-4o] already exists.\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.manager._model:load_model_configs:115 - Load configs for model wrapper: gpt-4o-mini, gpt-4o\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o]\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o: 'Model [gpt-4o] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-27 14:18:22 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-27 14:18:22 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:22 - INFO - Starting game 1\n",
      "2025-02-27 14:18:22 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters written to logs/fastslow/noreflect/43/20250227_141751/fastslow_raw_game1.log\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:22 - INFO - Moderator: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "2025-02-27 14:18:22 - INFO - ===\n",
      "2025-02-27 14:18:22 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:22 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:18:22 - INFO - ===\n",
      "2025-02-27 14:18:27 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"We need to be strategic in our discussions and votes. I want to gauge the suspicions of the others and figure out who they might suspect as werewolves. I should also consider any player behavior that stands out as suspicious.\", \"speak\": \"I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\", \"finish_discussion\": false}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:28 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:28 - INFO - Werewolves discussion: I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\n",
      "2025-02-27 14:18:28 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:28 - INFO - ===\n",
      "2025-02-27 14:18:28 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:28 - INFO - Retrieved similar documents: Player1: {\"thought\": \"We need to be strategic in our discussions and votes. I want to gauge the suspicions of the others and figure out who they might suspect as werewolves. I should also consider any player behavior that stands out as suspicious.\", \"speak\": \"I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\", \"finish_discussion\": false}\n",
      "Player1: {\"thought\": \"We need to be strategic in our discussions and votes. I want to gauge the suspicions of the others and figure out who they might suspect as werewolves. I should also consider any player behavior that stands out as suspicious.\", \"speak\": \"I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\", \"finish_discussion\": false}\n",
      "2025-02-27 14:18:28 - INFO - ===\n",
      "2025-02-27 14:18:30 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully navigate the discussion, suggesting possible suspects while avoiding drawing attention to myself as a werewolf. It's important to sow doubt among the villagers without revealing my role.\", \"speak\": \"I think we should consider Player2's behavior last night. They've been quite defensive since the game started, which is often a sign of being a werewolf. What do you all think about that?\", \"finish_discussion\": \"false\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:30 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:30 - INFO - Werewolves discussion: I think we should consider Player2's behavior last night. They've been quite defensive since the game started, which is often a sign of being a werewolf. What do you all think about that?\n",
      "2025-02-27 14:18:30 - INFO - Moderator: Which player do you vote to kill?\n",
      "2025-02-27 14:18:30 - INFO - Moderator: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:31 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:31 - INFO - ===\n",
      "2025-02-27 14:18:31 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:31 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to carefully navigate the discussion, suggesting possible suspects while avoiding drawing attention to myself as a werewolf. It's important to sow doubt among the villagers without revealing my role.\", \"speak\": \"I think we should consider Player2's behavior last night. They've been quite defensive since the game started, which is often a sign of being a werewolf. What do you all think about that?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"We need to be strategic in our discussions and votes. I want to gauge the suspicions of the others and figure out who they might suspect as werewolves. I should also consider any player behavior that stands out as suspicious.\", \"speak\": \"I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\", \"finish_discussion\": false}\n",
      "2025-02-27 14:18:31 - INFO - ===\n",
      "2025-02-27 14:18:32 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:32 - INFO - ===\n",
      "2025-02-27 14:18:32 - INFO - Fast Response: {\"thought\": \"I suspect Player3 based on their previous comments. They seem to be deflecting attention away from themselves and not contributing to identifying the werewolves effectively.\", \"vote\": \"Player3\", \"uncertainty_score\": 0.3}\n",
      "2025-02-27 14:18:32 - INFO - Fast Uncertainty: 0.3\n",
      "2025-02-27 14:18:32 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player3 based on their previous comments. They seem to be deflecting attention away from themselves and not contributing to identifying the werewolves effectively.\", \"vote\": \"Player3\", \"uncertainty_score\": 0.3}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:32 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:32 - INFO - Moderator: The player with the most votes is Player3.\n",
      "2025-02-27 14:18:32 - INFO - Moderator: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n",
      "2025-02-27 14:18:32 - INFO - ===\n",
      "2025-02-27 14:18:32 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:32 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:18:32 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player3.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:34 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Eliminating Player3 is a key decision. I need to consider if they were a threat or if there's potential value in keeping them alive. If I'm suspicious of Player3 being a werewolf, I might not choose to resurrect them, but if they could provide valuable information, it might be worth it.\",\"speak\": \"I'm going to resurrect Player3 because I believe they can provide valuable insights into who the werewolves might be, and we need their information to make a better decision moving forward.\",\"resurrect\": true}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:34 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:34 - INFO - Witch resurrection response: I'm going to resurrect Player3 because I believe they can provide valuable insights into who the werewolves might be, and we need their information to make a better decision moving forward.\n",
      "2025-02-27 14:18:34 - INFO - Moderator: The witch has chosen to resurrect the player.\n",
      "2025-02-27 14:18:34 - INFO - Moderator: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "2025-02-27 14:18:34 - INFO - ===\n",
      "2025-02-27 14:18:34 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:34 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:18:34 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:35 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to choose a player to check and gather information that could help the villagers. I suspect Player1 might be a werewolf due to vague claims during discussions.\", \"speak\": \"Player1\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:35 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:35 - INFO - Seer response: Player1\n",
      "2025-02-27 14:18:35 - INFO - Moderator: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night is peaceful, no player is eliminated.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2, Player3 and Player4. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:36 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:36 - INFO - ===\n",
      "2025-02-27 14:18:36 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:36 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to carefully navigate the discussion, suggesting possible suspects while avoiding drawing attention to myself as a werewolf. It's important to sow doubt among the villagers without revealing my role.\", \"speak\": \"I think we should consider Player2's behavior last night. They've been quite defensive since the game started, which is often a sign of being a werewolf. What do you all think about that?\", \"finish_discussion\": \"false\"}\n",
      "Player1: {\"thought\": \"We need to be strategic in our discussions and votes. I want to gauge the suspicions of the others and figure out who they might suspect as werewolves. I should also consider any player behavior that stands out as suspicious.\", \"speak\": \"I think we need to focus on who hasn't been contributing much to the discussion. We should also consider any players who seem to be deflecting suspicion onto others without a clear reason.\", \"finish_discussion\": false}\n",
      "2025-02-27 14:18:36 - INFO - ===\n",
      "2025-02-27 14:18:37 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully consider who to eliminate during the night phase. I should maintain a low profile and avoid drawing attention to myself while supporting my werewolf partner. It's crucial to sow doubt among the villagers and redirect suspicion away from us.\", \"speak\": \"I think we should discuss who seems most suspicious today. Let's focus on gathering more information before making a decision.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:38 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:38 - INFO - ===\n",
      "2025-02-27 14:18:38 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:38 - INFO - Retrieved similar documents: \n",
      "2025-02-27 14:18:38 - INFO - ===\n",
      "2025-02-27 14:18:41 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I agree with Player1's suggestion to discuss who seems suspicious. It's important to gather information among us villagers before casting any votes. We must be careful and avoid targeting each other without sufficient evidence.\", \"speak\": \"I think that's a good idea, Player1. Let's openly discuss our suspicions and see if we can gather clues about who the werewolves might be.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:41 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:41 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:41 - INFO - ===\n",
      "2025-02-27 14:18:41 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:41 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to choose a player to check and gather information that could help the villagers. I suspect Player1 might be a werewolf due to vague claims during discussions.\", \"speak\": \"Player1\"}\n",
      "2025-02-27 14:18:41 - INFO - ===\n",
      "2025-02-27 14:18:43 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to carefully navigate this discussion. Player1 and Player2 are already collaborating, and I must subtly guide them without revealing my role as the seer.\", \"speak\": \"I agree, Player2. Open discussions can really help us identify the werewolves. I’ve been observing Player4's actions too; they seem a bit off. What do you both think about Player4?\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:43 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:44 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:44 - INFO - ===\n",
      "2025-02-27 14:18:44 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:44 - INFO - Retrieved similar documents: Player4: {\"thought\": \"Eliminating Player3 is a key decision. I need to consider if they were a threat or if there's potential value in keeping them alive. If I'm suspicious of Player3 being a werewolf, I might not choose to resurrect them, but if they could provide valuable information, it might be worth it.\", \"speak\": \"I'm going to resurrect Player3 because I believe they can provide valuable insights into who the werewolves might be, and we need their information to make a better decision moving forward.\", \"resurrect\": true}\n",
      "2025-02-27 14:18:44 - INFO - ===\n",
      "2025-02-27 14:18:45 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 is suspicious of me, which suggests they could be trying to divert attention. I need to keep my cool and not reveal my role. I should also consider using my witch powers strategically if necessary.\", \"speak\": \"I understand your perspective, Player3. It’s essential to keep an open mind, but we should also analyze everyone’s actions thoroughly. Let's not jump to conclusions without solid evidence.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:46 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:46 - INFO - Survivors Discussion: I understand your perspective, Player3. It’s essential to keep an open mind, but we should also analyze everyone’s actions thoroughly. Let's not jump to conclusions without solid evidence.\n",
      "2025-02-27 14:18:46 - INFO - Moderator: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:46 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:46 - INFO - ===\n",
      "2025-02-27 14:18:46 - INFO - Query: Discussions related to villagers, seer, witch.\n",
      "2025-02-27 14:18:46 - INFO - Retrieved similar documents: Player1: {\"thought\": \"I need to carefully consider who to eliminate during the night phase. I should maintain a low profile and avoid drawing attention to myself while supporting my werewolf partner. It's crucial to sow doubt among the villagers and redirect suspicion away from us.\", \"speak\": \"I think we should discuss who seems most suspicious today. Let's focus on gathering more information before making a decision.\"}\n",
      "Player1: {\"thought\": \"I need to carefully navigate the discussion, suggesting possible suspects while avoiding drawing attention to myself as a werewolf. It's important to sow doubt among the villagers without revealing my role.\", \"speak\": \"I think we should consider Player2's behavior last night. They've been quite defensive since the game started, which is often a sign of being a werewolf. What do you all think about that?\", \"finish_discussion\": \"false\"}\n",
      "2025-02-27 14:18:46 - INFO - ===\n",
      "2025-02-27 14:18:47 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:47 - INFO - ===\n",
      "2025-02-27 14:18:47 - INFO - Fast Response: {\"thought\": \"I suspect Player2's behavior has been quite suspicious, together with a lack of engagement during discussions. I believe they might be a werewolf.\", \"vote\": \"Player2\", \"uncertainty_score\": 0.3}\n",
      "2025-02-27 14:18:47 - INFO - Fast Uncertainty: 0.3\n",
      "2025-02-27 14:18:47 - INFO - ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player2's behavior has been quite suspicious, together with a lack of engagement during discussions. I believe they might be a werewolf.\",\"vote\": \"Player2\", \"uncertainty_score\": 0.3}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:48 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:48 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:48 - INFO - ===\n",
      "2025-02-27 14:18:48 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:48 - INFO - Retrieved similar documents: Player2: {\"thought\": \"I agree with Player1's suggestion to discuss who seems suspicious. It's important to gather information among us villagers before casting any votes. We must be careful and avoid targeting each other without sufficient evidence.\", \"speak\": \"I think that's a good idea, Player1. Let's openly discuss our suspicions and see if we can gather clues about who the werewolves might be.\"}\n",
      "Player2: {\"thought\": \"I agree with Player1's suggestion to discuss who seems suspicious. It's important to gather information among us villagers before casting any votes. We must be careful and avoid targeting each other without sufficient evidence.\", \"speak\": \"I think that's a good idea, Player1. Let's openly discuss our suspicions and see if we can gather clues about who the werewolves might be.\"}\n",
      "2025-02-27 14:18:48 - INFO - ===\n",
      "2025-02-27 14:18:49 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I'm not entirely sure who the werewolves are, but I suspect Player1 based on their behavior during the discussion. They seemed evasive when questioned about their thoughts on others' suspicions.\", \"vote\": \"Player1\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:49 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:50 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:50 - INFO - ===\n",
      "2025-02-27 14:18:50 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:50 - INFO - Retrieved similar documents: Moderator: Okay, the role of Player1 is a werewolf.\n",
      "Player3: {\"thought\": \"I need to carefully navigate this discussion. Player1 and Player2 are already collaborating, and I must subtly guide them without revealing my role as the seer.\", \"speak\": \"I agree, Player2. Open discussions can really help us identify the werewolves. I\\u2019ve been observing Player4's actions too; they seem a bit off. What do you both think about Player4?\"}\n",
      "2025-02-27 14:18:50 - INFO - ===\n",
      "2025-02-27 14:18:51 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to analyze the discussions and behavior of the players to deduce who might be a werewolf. After considering the clues, my suspicion points towards Player2 as they seem to be deflecting blame.\", \"vote\": \"Player2\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:51 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:52 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:52 - INFO - ===\n",
      "2025-02-27 14:18:52 - INFO - Query: Discussions related to werewolves.\n",
      "2025-02-27 14:18:52 - INFO - Retrieved similar documents: Player4: {\"thought\": \"Eliminating Player3 is a key decision. I need to consider if they were a threat or if there's potential value in keeping them alive. If I'm suspicious of Player3 being a werewolf, I might not choose to resurrect them, but if they could provide valuable information, it might be worth it.\", \"speak\": \"I'm going to resurrect Player3 because I believe they can provide valuable insights into who the werewolves might be, and we need their information to make a better decision moving forward.\", \"resurrect\": true}\n",
      "2025-02-27 14:18:52 - INFO - ===\n",
      "2025-02-27 14:18:53 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I suspect Player1 is a werewolf based on their behavior during discussions. They seem to be deflecting attention away from themselves. I feel confident about this assessment.\", \"vote\": \"Player1\", \"uncertainty_score\": 0.2}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 14:18:53 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-02-27 14:18:53 - INFO - Moderator: Player1 has been voted out.\n",
      "2025-02-27 14:18:53 - INFO - Moderator: The game is over. The werewolves have been defeated, and the village is safe once again!\n",
      "2025-02-27 14:18:53 - INFO - Moderator: The game is over. The werewolves have been defeated, and the village is safe once again!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mModerator\u001b[0m: Player1 has been voted out.\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\1\\werewolf.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\1\\witch.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\1\\seer.log\n",
      "Messages saved to vectorstore\\fastslow\\20250227_141751\\1\\villager.log\n",
      "Row added to logs/fastslow/noreflect/43/20250227_141751/fastslow_win_rate.csv: {'raw_log_filepath': 'logs/fastslow/noreflect/43/20250227_141751/fastslow_raw_game1.log', 'custom_agent_won': False}\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    open_ai_key=openai_key,\n",
    "    fast_mind_model = \"gpt-4o-mini\",\n",
    "    slow_mind_model = \"gpt-4o\",\n",
    "    basefilepath=\"logs/fastslow\",\n",
    "    reflect_before_vectorstore=False,\n",
    "    max_days_per_game=2,\n",
    "    num_games=2,\n",
    "    similarity_top_k=2,\n",
    "    random_seed=43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
