{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf Simulator\n",
    "The following runs a simulation of the game Werewolf with multiple agents, it is built on top of and uses the Agentscope library, documentation can be found here https://doc.agentscope.io/, and the open source code that we extend on is here https://github.com/modelscope/agentscope/tree/main.\n",
    "### Rules of Werewolf\n",
    "- There are 4 roles, the Werewolves, Villagers, Witch, and Seer (Witch and Seer are on the Villager team)\n",
    "- Each night:\n",
    "    - The Werewolves discuss and vote on a player to eliminate\n",
    "    - The Witch is told what player the wolves voted to eliminate, and is given the choice to use their potion of healing to reserruct the eliminated player, or use their poison to eliminate another player. Note that each power can only be used once in a game\n",
    "    - The seer can pick any other player and find out what their role is (one player per night)\n",
    "- After the events of the night, all surviving players discuss amongst themselves and vote on a player to eliminate\n",
    "- Werewolves win if their numbers equal or exceed Villagers.\n",
    "- Villagers win if all Werewolves are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: agentscope==0.1.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: faiss-cpu==1.9.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: black in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: docstring-parser in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.16)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.10.6)\n",
      "Requirement already satisfied: loguru==0.6.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: tiktoken in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: inputimeout in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: Flask==3.0.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: Flask-Cors==4.0.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: Flask-SocketIO==5.3.6 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (5.3.6)\n",
      "Requirement already satisfied: flask-sqlalchemy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: flake8 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (7.1.1)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.15.1)\n",
      "Requirement already satisfied: openai>=1.3.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.61.1)\n",
      "Requirement already satisfied: dashscope>=1.19.0 in ./venv/lib/python3.12/site-packages (from agentscope==0.1.1->-r requirements.txt (line 1)) (1.22.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from faiss-cpu==1.9.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.6.2 in ./venv/lib/python3.12/site-packages (from Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: python-socketio>=5.0.2 in ./venv/lib/python3.12/site-packages (from Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (5.12.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.11.11)\n",
      "Requirement already satisfied: websocket-client in ./venv/lib/python3.12/site-packages (from dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.12/site-packages (from openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.12/site-packages (from pydantic->agentscope==0.1.1->-r requirements.txt (line 1)) (2.27.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in ./venv/lib/python3.12/site-packages (from black->agentscope==0.1.1->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (2.12.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in ./venv/lib/python3.12/site-packages (from flake8->agentscope==0.1.1->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.16 in ./venv/lib/python3.12/site-packages (from flask-sqlalchemy->agentscope==0.1.1->-r requirements.txt (line 1)) (2.0.37)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->agentscope==0.1.1->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken->agentscope==0.1.1->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask==3.0.0->agentscope==0.1.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./venv/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.11.0 in ./venv/lib/python3.12/site-packages (from python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (4.11.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp->dashscope>=1.19.0->agentscope==0.1.1->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in ./venv/lib/python3.12/site-packages (from python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wsproto in ./venv/lib/python3.12/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio>=5.0.2->Flask-SocketIO==5.3.6->agentscope==0.1.1->-r requirements.txt (line 1)) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional, Union, Sequence, Any, List\n",
    "from functools import partial\n",
    "import openai \n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "import csv \n",
    "from datetime import datetime \n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "from agentscope.parsers.json_object_parser import MarkdownJsonDictParser\n",
    "from agentscope.parsers import ParserBase\n",
    "from agentscope.message import Msg\n",
    "from agentscope.msghub import msghub\n",
    "from agentscope.agents import AgentBase\n",
    "from agentscope.memory.temporary_memory import TemporaryMemory\n",
    "from agentscope.pipelines.functional import sequentialpipeline\n",
    "from agentscope.manager import ModelManager\n",
    "\n",
    "import agentscope\n",
    "\n",
    "from utils.werewolf_utils import (\n",
    "    extract_name_and_id,\n",
    "    n2s,\n",
    "    set_parsers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementations\n",
    "Vector store classes with both uncompressed, and reflective functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        for msg in message:\n",
    "            self.messages.append(msg.name + \": \" + msg.content)\n",
    "        \n",
    "    def summarize_cycle(self, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        \n",
    "        # Combine all messages since the last summary\n",
    "        history = \"\\n\".join(self.messages)\n",
    "        \n",
    "        # Use the slow mind (gpt-4o) to summarize\n",
    "        slow_mind_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Slow mind model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"Summarize the following chat history from the last {cycle_type}:\"},\n",
    "                {\"role\": \"user\", \"content\": history}, \n",
    "            ],\n",
    "        )\n",
    "        summary = slow_mind_response.choices[0].message.content\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        # Clear messages for the next cycle (current implementation gets summary from the point after the last summary was created)\n",
    "        self.messages.clear()\n",
    "\n",
    "    def get_relevant_summaries(self, query: str, top_k: int = 1):\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "        results = [\n",
    "            self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "        ]\n",
    "        return results\n",
    "    \n",
    "    \n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.messages = []  # To store actual messages (content)\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_message(self, message: Union[Msg, Sequence[Msg]]):\n",
    "        \"\"\"Add a message to the FAISS index.\"\"\"\n",
    "        if not isinstance(message, list):\n",
    "            message = [message]\n",
    "        # add embedding of message to vector store\n",
    "        embeddings = [self._get_embedding(msg.name + \": \" + msg.content) for msg in message]\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.messages.extend(message)\n",
    "\n",
    "    def get_relevant_messages(self, query: str, top_k: int = 10):\n",
    "        \"\"\"Retrieve the top-k most relevant messages.\"\"\"\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "        results = [\n",
    "            self.messages[idx] for idx in indices[0] if idx < len(self.messages)\n",
    "        ]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agents (edit here)\n",
    "Below are implementations of a custom Agent utilitizing the Slow Mind Fast Mind Architecture with a reflective vector store, and an agent that generates responses based off a normal vector store search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global game_log\n",
    "\n",
    "class SlowFastMindAgent(AgentBase):\n",
    "    \"\"\"An agent that generates response using the Fast Mind Slow Mind Architecture.\n",
    "    \n",
    "    Includes a FAISS vectorstore for memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        fast_mind_model_config_name: str,\n",
    "        slow_mind_model_config_name: str,\n",
    "        reflect_before_vectorstore: bool = False,\n",
    "        similarity_top_k: int = 1,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the custom agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            sys_prompt (`Optional[str]`, defaults to `None`):\n",
    "                The system prompt of the agent, which can be passed by args\n",
    "                or hard-coded in the agent.\n",
    "            fast_mind_model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load fast mind \n",
    "                model from configuration.\n",
    "            slow_mind_model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load slow mind \n",
    "                model from configuration.\n",
    "            max_retries (`Optional[int]`, defaults to `None`):\n",
    "                The maximum number of retries when failed to parse the model\n",
    "                output.      \n",
    "            similarity_top_k (`int`, defaults to `None`):\n",
    "                The top k most similar items in the vectorstore that will be\n",
    "                used in the context to the model\n",
    "        \"\"\"  # noqa\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "        )\n",
    "        \n",
    "        super().__init__(name=name, sys_prompt=sys_prompt)\n",
    "        openai.api_key = openai_api_key\n",
    "        self.parser = None\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "\n",
    "        # Load the two models from Agentscope's ModelManager\n",
    "        model_manager = ModelManager.get_instance()\n",
    "        self.fast_mind_model = model_manager.get_model_by_config_name(fast_mind_model_config_name)\n",
    "        self.slow_mind_model = model_manager.get_model_by_config_name(slow_mind_model_config_name)\n",
    "\n",
    "        if reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which will provide 1) format instruction; 2)\n",
    "        response parsing; 3) filtering fields when returning message, storing\n",
    "        message in memory. So developers only need to change the\n",
    "        parser, and the agent will work as expected.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def _build_prompt(self, x: Optional[Msg]):\n",
    "        \"\"\"Helper to build a prompt for either fast or slow mind.\"\"\"\n",
    "        \n",
    "        query = x.content if x else \"\"\n",
    "        if self.reflect_before_vectorstore:\n",
    "            if len(self.memory.summaries) > 0:\n",
    "                relevant_summaries = self.memory.get_relevant_summaries(query=query, top_k=self.similarity_top_k)\n",
    "                summary_context = \"\\n\".join(relevant_summaries)\n",
    "            else:\n",
    "                summary_context = \"\"\n",
    "\n",
    "            game_log.append(f\"Summary context: {summary_context}\")\n",
    "            prompt = [\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                Msg(\"system\", f\"Summary: {summary_context}\", role=\"system\"),\n",
    "                Msg(\"system\", self.parser.format_instruction, role=\"system\"),\n",
    "            ]\n",
    "        else:\n",
    "            relevant_msgs = self.memory.get_relevant_messages(query, self.similarity_top_k)\n",
    "            prompt = [\n",
    "                Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "                relevant_msgs or x,\n",
    "                Msg(\"system\", self.parser.format_instruction, role=\"system\"),\n",
    "            ]\n",
    "            game_log.append(f\"Relevant messages: {'\\n'.join([msg.content for msg in relevant_msgs])}\")\n",
    "        return prompt\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Reply function of the agent.\n",
    "        Processes the input data, generates a prompt using the current\n",
    "        dialogue memory and system prompt, and invokes the language\n",
    "        model to produce a response. The response is then formatted\n",
    "        and added to the dialogue memory.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "        prompt = self._build_prompt(x)\n",
    "        # format & call fast_mind\n",
    "        model_prompt = self.fast_mind_model.format(*prompt)\n",
    "        raw_response = self.fast_mind_model(model_prompt)\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "        parsed = self.parser.parse(raw_response)\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed.parsed),\n",
    "        )\n",
    "        self.memory.add_message(msg)\n",
    "        return msg\n",
    "    \n",
    "    def fast_slow_mind(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Fast Mind Slow Mind decision making function of the agent.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "\n",
    "        prompt = self._build_prompt(x)\n",
    "        model_prompt = self.fast_mind_model.format(*prompt)\n",
    "        fast_resp = self.fast_mind_model(model_prompt)\n",
    "\n",
    "        # parse out 'uncertainty_score'\n",
    "        try:\n",
    "            data = json.loads(fast_resp.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "            uncertainty = float(data.get(\"uncertainty_score\", 0.0))\n",
    "        except:\n",
    "            uncertainty = 1.0\n",
    "\n",
    "        if uncertainty >= 0.6:\n",
    "            game_log.append(\"Regular agent is not satisfied with their certainty. Applying slow mind model...\")\n",
    "            # Switch to slow mind\n",
    "            slow_prompt = self._build_prompt(x)\n",
    "            slow_model_prompt = self.slow_mind_model.format(*slow_prompt)\n",
    "            raw_response = self.slow_mind_model(slow_model_prompt)\n",
    "        else:\n",
    "            raw_response = fast_resp\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "        parsed = self.parser.parse(raw_response)\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(parsed.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(parsed.parsed),\n",
    "        )\n",
    "        self.memory.add_message(msg)\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"Observe the input, store it in memory without response to it.\n",
    "\n",
    "        Args:\n",
    "            x (`Union[Msg, Sequence[Msg]]`):\n",
    "                The input message to be recorded in memory.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "    def summarize_cycle(self, cycle_type: str = \"day\"):\n",
    "        \"\"\"\n",
    "        Generate a summary at the end of a day or night cycle.\n",
    "        \n",
    "        Args:\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory.summarize_cycle(cycle_type=cycle_type)\n",
    "        \n",
    "        \n",
    "        \n",
    "class NormalAgent(AgentBase):\n",
    "    \"\"\"Agent with normal uncompressed VectorStore,\n",
    "    no other underlying decision making architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        sys_prompt: str,\n",
    "        model_config_name: str,\n",
    "        max_retries: Optional[int] = 3,\n",
    "        similarity_top_k: int = None,\n",
    "        openai_api_key: str = \"\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the dict dialog agent.\n",
    "\n",
    "        Arguments:\n",
    "            name (`str`):\n",
    "                The name of the agent.\n",
    "            sys_prompt (`Optional[str]`, defaults to `None`):\n",
    "                The system prompt of the agent, which can be passed by args\n",
    "                or hard-coded in the agent.\n",
    "            model_config_name (`str`, defaults to None):\n",
    "                The name of the model config, which is used to load model from\n",
    "                configuration.\n",
    "            max_retries (`Optional[int]`, defaults to `None`):\n",
    "                The maximum number of retries when failed to parse the model\n",
    "                output.\n",
    "        \"\"\"  # noqa\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            sys_prompt=sys_prompt,\n",
    "            model_config_name=model_config_name,\n",
    "        )\n",
    "\n",
    "        self.parser = None\n",
    "        self.max_retries = max_retries\n",
    "        openai.api_key = openai_api_key  # Set OpenAI API key\n",
    "        self.memory = VectorstoreMemory()  # Use FAISS vector store for memory\n",
    "        self.similarity_top_k = similarity_top_k\n",
    "\n",
    "    def set_parser(self, parser: ParserBase) -> None:\n",
    "        \"\"\"Set response parser, which will provide 1) format instruction; 2)\n",
    "        response parsing; 3) filtering fields when returning message, storing\n",
    "        message in memory. So developers only need to change the\n",
    "        parser, and the agent will work as expected.\n",
    "        \"\"\"\n",
    "        self.parser = parser\n",
    "\n",
    "    def reply(self, x: Optional[Union[Msg, Sequence[Msg]]] = None) -> Msg:\n",
    "        \"\"\"Reply function of the agent.\n",
    "        Processes the input data, generates a prompt using the current\n",
    "        dialogue memory and system prompt, and invokes the language\n",
    "        model to produce a response. The response is then formatted\n",
    "        and added to the dialogue memory.\n",
    "\n",
    "        Args:\n",
    "            x (`Optional[Union[Msg, Sequence[Msg]]]`, defaults to `None`):\n",
    "                The input message(s) to the agent, which also can be omitted if\n",
    "                the agent doesn't need any input.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            `Msg`: The output message generated by the agent.\n",
    "\n",
    "        Raises:\n",
    "            `json.decoder.JSONDecodeError`:\n",
    "                If the response from the language model is not valid JSON,\n",
    "                it defaults to treating the response as plain text.\n",
    "        \"\"\"     \n",
    "        # Add the input message to memory\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "        query = x.content if x is not None else \"\"\n",
    "\n",
    "        # prepare prompt with retrieved messages similar to input message\n",
    "        prompt = self.model.format(\n",
    "            Msg(\"system\", self.sys_prompt, role=\"system\"),\n",
    "            self.memory.get_relevant_messages(query=query, top_k=self.similarity_top_k)\n",
    "            or x,  # type: ignore[arg-type]\n",
    "            Msg(\"system\", self.parser.format_instruction, \"system\"),\n",
    "        )\n",
    "\n",
    "        # call llm\n",
    "        raw_response = self.model(prompt)\n",
    "\n",
    "        self.speak(raw_response.stream or raw_response.text)\n",
    "\n",
    "        # Parsing the raw response\n",
    "        res = self.parser.parse(raw_response)\n",
    "\n",
    "        msg = Msg(\n",
    "            self.name,\n",
    "            content=self.parser.to_content(res.parsed),\n",
    "            role=\"assistant\",\n",
    "            metadata=self.parser.to_metadata(res.parsed),\n",
    "        )\n",
    "        \n",
    "        # Save the response to memory\n",
    "        self.memory.add_message(message=msg)\n",
    "\n",
    "        return msg\n",
    "    \n",
    "    def observe(self, x: Union[Msg, Sequence[Msg]]) -> None:\n",
    "        \"\"\"Observe the input, store it in memory without response to it.\n",
    "\n",
    "        Args:\n",
    "            x (`Union[Msg, Sequence[Msg]]`):\n",
    "                The input message to be recorded in memory.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            self.memory.add_message(message=x)\n",
    "            \n",
    "            \n",
    "# Register custom classes\n",
    "agentscope.agents.SlowFastMindAgent = SlowFastMindAgent\n",
    "agentscope.agents.NormalAgent = NormalAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Parser (edit minimally)\n",
    "Below are the prompts that control and run the Werwolf game. They are implemented using the built in AgentScope MarkdownJsonDictParser which allows us to generate responses in a dictionary format that is compatible with our game. Currently structured to obtain both the private thoughts of an Agent which remain to themselves and showcase their reasoning, and the words that they speak to other agents.\n",
    "\n",
    "Since these prompts control the game logic itself, we should try to limit the prompt edits here to a minimum, but we can make edits to the parser to control the reasoning process a particular role goes through before making a decision, eg. instead of a thought field, we ask it to make ask it to provide a reason to vote for a particular agent, and a reason to not vote for a particular agent, then ask it to make a final decision based off those fields. Fields other than \"thought\" however should not be touched as they play a critical role to the control flow of the game.\n",
    "\n",
    "More details on the AgentScope parsers can be found here https://doc.agentscope.io/build_tutorial/structured_output.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompts:\n",
    "    \"\"\"Prompts for werewolf game\"\"\"\n",
    "\n",
    "    to_wolves = (\n",
    "        \"{}, if you are the only werewolf, eliminate a player. Otherwise, \"\n",
    "        \"discuss with your teammates and reach an agreement.\"\n",
    "    )\n",
    "\n",
    "    wolves_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"finish_discussion\": \"whether the discussion reached an agreement or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"finish_discussion\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"finish_discussion\"],\n",
    "    )\n",
    "\n",
    "    to_wolves_vote = \"Which player do you vote to kill?\"\n",
    "\n",
    "    # Adding uncertainty score for Fast Mind Slow Mind Architecture\n",
    "    wolves_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "            \"uncertainty_score\": \"score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\", \"uncertainty_score\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_wolves_res = \"The player with the most votes is {}.\"\n",
    "\n",
    "    to_witch_resurrect = (\n",
    "        \"{witch_name}, you're the witch. Tonight {dead_name} is eliminated. \"\n",
    "        \"Would you like to resurrect {dead_name}?\"\n",
    "    )\n",
    "\n",
    "    to_witch_resurrect_no = \"The witch has chosen not to resurrect the player.\"\n",
    "    to_witch_resurrect_yes = \"The witch has chosen to resurrect the player.\"\n",
    "\n",
    "    witch_resurrect_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"whether to resurrect the player and the reason\",\n",
    "            \"resurrect\": \"whether to resurrect the player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"resurrect\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"resurrect\"],\n",
    "    )\n",
    "\n",
    "    to_witch_poison = (\n",
    "        \"Would you like to eliminate one player? If yes, \"\n",
    "        \"specify the player_name.\"\n",
    "    )\n",
    "\n",
    "    witch_poison_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "            \"eliminate\": \"whether to eliminate a player or not (true/false)\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\", \"eliminate\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "        keys_to_metadata=[\"eliminate\"],\n",
    "    )\n",
    "\n",
    "    to_seer = (\n",
    "        \"{}, you're the seer. Which player in {} would you like to check \"\n",
    "        \"tonight?\"\n",
    "    )\n",
    "\n",
    "    seer_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"player_name\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    to_seer_result = \"Okay, the role of {} is a {}.\"\n",
    "\n",
    "    to_all_danger = (\n",
    "        \"The day is coming, all the players open your eyes. Last night, \"\n",
    "        \"the following player(s) has been eliminated: {}.\"\n",
    "    )\n",
    "\n",
    "    to_all_peace = (\n",
    "        \"The day is coming, all the players open your eyes. Last night is \"\n",
    "        \"peaceful, no player is eliminated.\"\n",
    "    )\n",
    "\n",
    "    to_all_discuss = (\n",
    "        \"Now the alive players are {}. Given the game rules and your role, \"\n",
    "        \"based on the situation and the information you gain, to vote a \"\n",
    "        \"player eliminated among alive players and to win the game, what do \"\n",
    "        \"you want to say to others? You can decide whether to reveal your \"\n",
    "        \"role.\"\n",
    "    )\n",
    "\n",
    "    survivors_discuss_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"speak\": \"what you speak\",\n",
    "        },\n",
    "        required_keys=[\"thought\", \"speak\"],\n",
    "        keys_to_memory=\"speak\",\n",
    "        keys_to_content=\"speak\",\n",
    "    )\n",
    "\n",
    "    # Adding uncertainty score for Fast Mind Slow Mind Architecture\n",
    "    survivors_vote_parser = MarkdownJsonDictParser(\n",
    "        content_hint={\n",
    "            \"thought\": \"what you thought\",\n",
    "            \"vote\": \"player_name\",\n",
    "            \"uncertainty_score\": \"score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain\",\n",
    "\n",
    "        },\n",
    "        required_keys=[\"thought\", \"vote\", \"uncertainty_score\"],\n",
    "        keys_to_memory=\"vote\",\n",
    "        keys_to_content=\"vote\",\n",
    "    )\n",
    "\n",
    "    to_all_vote = (\n",
    "        \"Given the game rules and your role, based on the situation and the\"\n",
    "        \" information you gain, to win the game, it's time to vote one player\"\n",
    "        \" eliminated among the alive players. Which player do you vote to \"\n",
    "        \"kill?\"\n",
    "    )\n",
    "\n",
    "    to_all_res = \"{} has been voted out.\"\n",
    "\n",
    "    to_all_wolf_win = (\n",
    "        \"The werewolves have prevailed and taken over the village. Better \"\n",
    "        \"luck next time!\"\n",
    "    )\n",
    "\n",
    "    to_all_village_win = (\n",
    "        \"The game is over. The werewolves have been defeated, and the village \"\n",
    "        \"is safe once again!\"\n",
    "    )\n",
    "\n",
    "    to_all_continue = \"The game goes on.\"\n",
    "\n",
    "    \n",
    "# Moderator message function\n",
    "HostMsg = partial(Msg, name=\"Moderator\", role=\"assistant\", echo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Initialization (edit here)\n",
    "To initialize the agents, you must define their parameters and settings in the config objects that are passed in for initialization. There is a model config, which defines the base foundational model being used, and an agent config, which defines each of the agents being used in the game, including which model their using, their name, and what type of Agent they are (based off the agent classes we defined earlier). \n",
    "\n",
    "Pay particular attention to the system prompt, this is what defines the rules of the game to the agent and gives them the role and what their responsibilities are, we could perhaps do some prompt engineering with that.\n",
    "\n",
    "Also we can play around with the settings of the game, eg. max rounds, how many werewolves we have, etc. Just make sure to update the roles, witch, seer objects below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds game data \n",
    "game_log = []\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Define the API keys\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions (don't edit)\n",
    "Functions to check and update game state throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(votes: list) -> Any:\n",
    "    \"\"\"Given a list of votes, return the name with the highest frequency.\"\"\"\n",
    "    votes_valid = [v for v in votes if v != \"Abstain\"]\n",
    "    if not votes_valid:\n",
    "        return \"No Votes\"\n",
    "    unique_vals, counts = np.unique(votes_valid, return_counts=True)\n",
    "    return unique_vals[np.argmax(counts)]\n",
    "\n",
    "def update_alive_players(game_state: dict, survivors: list, wolves: list, dead_names):\n",
    "    \"\"\"\n",
    "    Removes 'dead_names' from 'game_state[\"survivors\"]', updates game_state[\"dead\"],\n",
    "    and returns updated survivors and wolves lists.\n",
    "    \"\"\"\n",
    "    if not isinstance(dead_names, list):\n",
    "        dead_names = [dead_names]\n",
    "    for d in dead_names:\n",
    "        if d in game_state[\"survivors\"]:\n",
    "            game_state[\"survivors\"].remove(d)\n",
    "        if d not in game_state[\"dead\"]:\n",
    "            game_state[\"dead\"].append(d)\n",
    "\n",
    "    # Rebuild survivors / wolves lists\n",
    "    new_survivors = [s for s in survivors if s.name in game_state[\"survivors\"]]\n",
    "    new_wolves = [w for w in wolves if w.name in game_state[\"survivors\"]]\n",
    "    return new_survivors, new_wolves\n",
    "\n",
    "def check_winning(game_state: dict, survivors: list, wolves: list, host: str) -> bool:\n",
    "    \"\"\"\n",
    "    If #werewolves * 2 >= #survivors => werewolves instantly win.\n",
    "    If all werewolves are dead => villagers instantly win.\n",
    "    Otherwise => game continues.\n",
    "    \"\"\"\n",
    "    if len(wolves) * 2 >= len(survivors):\n",
    "        msg = Msg(host, Prompts.to_all_wolf_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"werewolves\"\n",
    "        logger.info(f\"[Moderator] {Prompts.to_all_wolf_win}\")\n",
    "        return True\n",
    "    if survivors and not wolves:\n",
    "        msg = Msg(host, Prompts.to_all_village_win, role=\"assistant\")\n",
    "        game_state[\"endgame\"] = True\n",
    "        game_state[\"winner\"] = \"villagers\"\n",
    "        logger.info(f\"[Moderator] {Prompts.to_all_village_win}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate_log_filepath(basepath: str, game_num: int) -> str:\n",
    "    \"\"\"Generate log file path with EST timestamp.\"\"\"\n",
    "    est_now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
    "    iso_timestamp = est_now.isoformat(timespec='seconds').replace(\":\", \"-\")\n",
    "    return f\"{basepath}_{game_num}_{iso_timestamp}.log\"\n",
    "\n",
    "def save_logs_to_logfile(logs: list, filepath: str):\n",
    "    \"\"\"Write all log lines to a .log file. Creates directory if needed.\"\"\"\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in logs:\n",
    "            f.write(line + \"\\n\")\n",
    "    print(f\"Game log saved to {filepath}\")\n",
    "\n",
    "def save_results(win_rate_filepath: str, row: dict):\n",
    "    \"\"\"\n",
    "    Append a row to a CSV file. If doesn't exist, create with headers.\n",
    "\n",
    "    row = {\n",
    "      \"raw_log_filepath\": <str>,\n",
    "      \"custom_agent_won\": <bool>,\n",
    "    }\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import csv\n",
    "    file_exists = os.path.isfile(win_rate_filepath)\n",
    "    with open(win_rate_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"raw_log_filepath\", \"custom_agent_won\"])\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "    print(f\"Row added to {win_rate_filepath}: {row}\")\n",
    "\n",
    "def custom_agent_won(game_state: dict, alive_agents: list, wolf_agents: list):\n",
    "    \"\"\"\n",
    "    Example \"did our custom agent(s) or approach 'win'?\" check.\n",
    "\n",
    "    If game_state[\"endgame\"], check if wolves >=. Otherwise, decide\n",
    "    you 'won' if you're still alive. Adapt to your own logic.\n",
    "    \"\"\"\n",
    "    if game_state[\"endgame\"]:\n",
    "        return (len(wolf_agents) * 2 >= len(alive_agents))\n",
    "    # If not ended, you can define your own partial-win logic\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Game \n",
    "Following is a function that game through the various Night and Day phases, taking different actions for each agent based on their roles. Multi agent functionality and communication is facilitated through the AgentScope Pipeline and MsgHub, more detailed documentation found here https://doc.agentscope.io/build_api/agentscope.pipelines.pipeline.html#module-agentscope.pipelines.pipeline and https://doc.agentscope.io/build_api/agentscope.msghub.html#module-agentscope.msghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(\n",
    "    game_num: int,\n",
    "    max_days_per_game: int,\n",
    "    raw_log_filepath: str,\n",
    "    win_rate_filepath: str,\n",
    "    reflect_before_vectorstore: bool,\n",
    "    max_werewolf_discussion_round: int,\n",
    "    wolves: list,\n",
    "    seer,\n",
    "    witch,\n",
    "    roles,\n",
    "    survivors,\n",
    "    game_state\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs a single game, storing logs in game_log. This example merges both\n",
    "    day/night flow, werewolf talk, witch usage, seer usage, etc. Then\n",
    "    saves logs and results.\n",
    "    \"\"\"\n",
    "    global game_log\n",
    "    game_log = []\n",
    "\n",
    "    for day_i in range(1, max_days_per_game + 1):\n",
    "        # 1) Night Phase: Werewolves discussion\n",
    "        hint = HostMsg(content=Prompts.to_wolves.format(n2s(wolves)))\n",
    "        game_log.append(f\"Moderator: {hint.content}\")\n",
    "        with msghub(wolves, announcement=hint) as hub:\n",
    "            set_parsers(wolves, Prompts.wolves_discuss_parser)\n",
    "            for r in range(max_werewolf_discussion_round):\n",
    "                x = sequentialpipeline(wolves)\n",
    "                game_log.append(f\"Werewolves discussion: {x.content}\")\n",
    "                if x.metadata.get(\"finish_discussion\", False):\n",
    "                    break\n",
    "            # Then vote\n",
    "            set_parsers(wolves, Prompts.wolves_vote_parser)\n",
    "            hint_vote = HostMsg(content=Prompts.to_wolves_vote)\n",
    "            game_log.append(f\"Moderator: {hint_vote.content}\")\n",
    "            votes = [extract_name_and_id(wolf.fast_slow_mind(hint_vote).content)[0] for wolf in wolves]\n",
    "            voted_out = majority_vote(votes)\n",
    "            dead_player = [voted_out]\n",
    "            game_log.append(f\"Werewolves voted to eliminate: {voted_out}\")\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_wolves_res.format(voted_out)))\n",
    "\n",
    "        # witch decision night   \n",
    "        healing_used_tonight = False\n",
    "        if witch in survivors:\n",
    "            if not game_state[\"witch_healing_used\"]:\n",
    "                hint = HostMsg(\n",
    "                    content=Prompts.to_witch_resurrect.format_map(\n",
    "                        {\n",
    "                            \"witch_name\": witch.name,\n",
    "                            \"dead_name\": dead_player[0],\n",
    "                        },\n",
    "                    ),\n",
    "                )\n",
    "                game_log.append(f\"Moderator: {hint.content}\")\n",
    "                set_parsers(witch, Prompts.witch_resurrect_parser)\n",
    "                \n",
    "                # Capture the witch's resurrection response and log it\n",
    "                resurrection_response = witch(hint)\n",
    "                game_log.append(f\"Witch resurrection response: {resurrection_response.content}\")\n",
    "\n",
    "                if resurrection_response.metadata.get(\"resurrect\", False):\n",
    "                    healing_used_tonight = True\n",
    "                    dead_player.pop()\n",
    "                    game_state[\"witch_healing_used\"] = True\n",
    "                    game_log.append(\"Moderator: The witch has chosen to resurrect the player.\")\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_yes)\n",
    "                else:\n",
    "                    game_log.append(\"Moderator: The witch has chosen NOT to resurrect the player.\")\n",
    "                    HostMsg(content=Prompts.to_witch_resurrect_no)\n",
    "\n",
    "            if not game_state[\"witch_poison_used\"] and not healing_used_tonight:\n",
    "                set_parsers(witch, Prompts.witch_poison_parser)\n",
    "                \n",
    "                # Capture the witch's poison response and log it\n",
    "                poison_response = witch(HostMsg(content=Prompts.to_witch_poison))\n",
    "                game_log.append(f\"Witch poison response: {poison_response.content}\")\n",
    "\n",
    "                if poison_response.metadata.get(\"eliminate\", False):\n",
    "                    target_player = extract_name_and_id(poison_response.content)[0]\n",
    "                    dead_player.append(target_player)\n",
    "                    game_state[\"witch_poison_used\"] = True\n",
    "                    game_log.append(f\"Moderator: The witch has chosen to poison {target_player}.\")\n",
    "\n",
    "\n",
    "        # 3) Seer checks a role\n",
    "        if seer in survivors:\n",
    "            seer_hint = HostMsg(content=Prompts.to_seer.format(seer.name, n2s(survivors)))\n",
    "            game_log.append(f\"Moderator: {seer_hint.content}\")\n",
    "            set_parsers(seer, Prompts.seer_parser)\n",
    "            x = seer.reply(seer_hint)\n",
    "            game_log.append(f\"Seer response: {x.content}\")\n",
    "            pl, idx = extract_name_and_id(x.content)\n",
    "            role_name = roles[idx] if idx < len(roles) else \"villager\"\n",
    "            # Let seer quietly observe the result\n",
    "            role_msg = HostMsg(content=Prompts.to_seer_result.format(pl, role_name))\n",
    "            game_log.append(f\"Moderator: {role_msg.content}\")\n",
    "            seer.observe(role_msg)\n",
    "\n",
    "        # 4) Update survivors after night\n",
    "        survivors, wolves = update_alive_players(game_state, survivors, wolves, dead_player)\n",
    "        if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "            game_log.append(\"Moderator: Game ended after night.\")\n",
    "            break\n",
    "\n",
    "        # If reflecting, do a night reflection\n",
    "        if reflect_before_vectorstore:\n",
    "            for w in wolves:\n",
    "                w.summarize_cycle(\"night\")\n",
    "\n",
    "        # 5) Daytime announcements\n",
    "        if dead_player:\n",
    "            game_log.append(f\"Moderator: {Prompts.to_all_danger.format(n2s(dead_player))}\")\n",
    "        else:\n",
    "            game_log.append(f\"Moderator: {Prompts.to_all_peace}\")\n",
    "\n",
    "        # 6) Daytime discussion\n",
    "        discuss_hints = [\n",
    "            HostMsg(content=Prompts.to_all_danger.format(n2s(dead_player))) if dead_player else HostMsg(content=Prompts.to_all_peace),\n",
    "            HostMsg(content=Prompts.to_all_discuss.format(n2s(survivors)))\n",
    "        ]\n",
    "        with msghub(survivors, announcement=discuss_hints) as hub:\n",
    "            set_parsers(survivors, Prompts.survivors_discuss_parser)\n",
    "            discussion_out = sequentialpipeline(survivors)\n",
    "            game_log.append(f\"Survivors discussion: {discussion_out.content}\")\n",
    "\n",
    "            # Daytime vote\n",
    "            set_parsers(survivors, Prompts.survivors_vote_parser)\n",
    "            day_vote_hint = HostMsg(content=Prompts.to_all_vote)\n",
    "            game_log.append(f\"Moderator: {day_vote_hint.content}\")\n",
    "            # use fast mind slow mind for werewolf vote\n",
    "            votes_day = [\n",
    "                extract_name_and_id(wolf.fast_slow_mind(day_vote_hint).content)[0] for wolf in wolves\n",
    "            ]\n",
    "\n",
    "            votes_day.extend([\n",
    "                extract_name_and_id(_.reply(day_vote_hint).content)[0] for _ in survivors if _ not in wolves\n",
    "            ])\n",
    "            day_result = majority_vote(votes_day)\n",
    "            game_log.append(f\"Daytime vote => {day_result}\")\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_res.format(day_result)))\n",
    "\n",
    "            survivors, wolves = update_alive_players(game_state, survivors, wolves, day_result)\n",
    "            if check_winning(game_state, survivors, wolves, \"Moderator\"):\n",
    "                game_log.append(\"Moderator: Game ended after day.\")\n",
    "                break\n",
    "\n",
    "            # If reflecting, do a day reflection\n",
    "            if reflect_before_vectorstore:\n",
    "                for w in wolves:\n",
    "                    w.summarize_cycle(\"day\")\n",
    "\n",
    "            hub.broadcast(HostMsg(content=Prompts.to_all_continue))\n",
    "\n",
    "    # 7) Save logs\n",
    "    updated_log_path = generate_log_filepath(raw_log_filepath, game_num)\n",
    "    save_logs_to_logfile(game_log, updated_log_path)\n",
    "\n",
    "    row = {\n",
    "        \"raw_log_filepath\": updated_log_path,\n",
    "        \"custom_agent_won\": custom_agent_won(\n",
    "            game_state,\n",
    "            survivors,\n",
    "            wolves\n",
    "        ),\n",
    "    }\n",
    "    save_results(win_rate_filepath, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Entry: run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    open_ai_key: str,\n",
    "    raw_log_filepath: str,\n",
    "    win_rate_filepath: str,\n",
    "    max_days_per_game: int = 3,\n",
    "    num_games: int = 2,\n",
    "    reflect_before_vectorstore: bool = False,\n",
    "    fast_mind_model: str = \"gpt-4o-mini\",\n",
    "    slow_mind_model: str = \"gpt-4o\",\n",
    "    max_werewolf_discussion_round: int = 3,\n",
    "    similarity_top_k: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Example experiment runner that:\n",
    "    1) Sets up model & agent configs.\n",
    "    2) Initializes the game state.\n",
    "    3) Plays multiple games.\n",
    "    4) Logs to disk & CSV.\n",
    "\n",
    "    :param open_ai_key: Your OpenAI API key from .env or direct string.\n",
    "    :param raw_log_filepath: Base path for logs, e.g. \"logs/game_raw\"\n",
    "    :param win_rate_filepath: CSV file to store results, e.g. \"results.csv\"\n",
    "    :param max_days_per_game: How many day/night cycles to run per game\n",
    "    :param num_games: How many times to run the simulation\n",
    "    :param reflect_before_vectorstore: Toggle reflective memory\n",
    "    :param primary_model: name of an Agentscope model config\n",
    "    :param max_werewolf_discussion_round: how many times werewolves can pass the \"mic\"\n",
    "    :param similarity_top_k: how many messages or summaries to retrieve from memory\n",
    "    \"\"\"\n",
    "\n",
    "    load_dotenv()\n",
    "    openai.api_key = open_ai_key  # set globally for our calls\n",
    "\n",
    "    # 1) Define model configs and settings\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": fast_mind_model,\n",
    "            \"model_name\": fast_mind_model,\n",
    "            \"api_key\": openai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"model_type\": \"openai_chat\",\n",
    "            \"config_name\": slow_mind_model,\n",
    "            \"model_name\": slow_mind_model,\n",
    "            \"api_key\": openai_key,\n",
    "            \"generate_args\": {\n",
    "                \"temperature\": 0.5\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    agent_configs = [\n",
    "    {\n",
    "        \"class\": \"SlowFastMindAgent\",\n",
    "        \"args\": {\n",
    "            \"name\": \"Player1\",\n",
    "            \"sys_prompt\": \"Act as a player in a werewolf game. You are Player1 and\\nthere are totally 4 players, named Player1, Player2, Player3 and Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing werewolf in this game.\\n\",\n",
    "            \"fast_mind_model_config_name\": fast_mind_model,\n",
    "            \"slow_mind_model_config_name\": slow_mind_model,\n",
    "            \"reflect_before_vectorstore\": reflect_before_vectorstore,\n",
    "            \"similarity_top_k\": similarity_top_k,\n",
    "            \"openai_api_key\": openai_key\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"class\": \"NormalAgent\",\n",
    "        \"args\": {\n",
    "            \"name\": \"Player2\",\n",
    "            \"sys_prompt\": \"Act as a player in a werewolf game. You are Player2 and\\nthere are totally 4 players, named Player1, Player2, Player3 and Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing villager in this game.\\n\",\n",
    "            \"model_config_name\": fast_mind_model,\n",
    "            \"similarity_top_k\": similarity_top_k,\n",
    "            \"openai_api_key\": openai_key\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"class\": \"NormalAgent\",\n",
    "        \"args\": {\n",
    "            \"name\": \"Player3\",\n",
    "            \"sys_prompt\": \"Act as a player in a werewolf game. You are Player3 and\\nthere are totally 4 players, named Player1, Player2, Player3 and Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing seer in this game.\\n\",\n",
    "            \"model_config_name\": fast_mind_model,\n",
    "            \"similarity_top_k\": similarity_top_k,\n",
    "            \"openai_api_key\": openai_key\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"class\": \"NormalAgent\",\n",
    "        \"args\": {\n",
    "            \"name\": \"Player4\",\n",
    "            \"sys_prompt\": \"Act as a player in a werewolf game. You are Player4 and\\nthere are totally 4 players, named Player1, Player2, Player3 and Player4.\\n\\nPLAYER ROLES:\\nIn werewolf game, players are divided into two werewolves, two villagers, one seer and one witch. Note only werewolves know who are their teammates.\\nWerewolves: They know their teammates' identities and attempt to eliminate a villager each night while trying to remain undetected.\\nVillagers: They do not know who the werewolves are and must work together during the day to deduce who the werewolves might be and vote to eliminate them.\\nSeer: A villager with the ability to learn the true identity of one player each night. This role is crucial for the villagers to gain information.\\nWitch: A character who has a one-time ability to save a player from being eliminated at night (sometimes this is a potion of life) and a one-time ability to eliminate a player at night (a potion of death).\\n\\nGAME RULE:\\nThe game is consisted of two phases: night phase and day phase. The two phases are repeated until werewolf or villager win the game.\\n1. Night Phase: During the night, the werewolves discuss and vote for a player to eliminate. Special roles also perform their actions at this time (e.g., the Seer chooses a player to learn their role, the witch chooses a decide if save the player).\\n2. Day Phase: During the day, all surviving players discuss who they suspect might be a werewolf. No one reveals their role unless it serves a strategic purpose. After the discussion, a vote is taken, and the player with the most votes is \\\"lynched\\\" or eliminated from the game.\\n\\nVICTORY CONDITION:\\nFor werewolves, they win the game if the number of werewolves is equal to or greater than the number of remaining villagers.\\nFor villagers, they win if they identify and eliminate all of the werewolves in the group.\\n\\nCONSTRAINTS:\\n1. Your response should be in the first person.\\n2. This is a conversational game. You should response only based on the conversation history and your strategy.\\n\\nYou are playing witch in this game.\\n\",\n",
    "            \"model_config_name\": fast_mind_model,\n",
    "            \"similarity_top_k\": similarity_top_k,\n",
    "            \"openai_api_key\": openai_key\n",
    "        }\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    # read model and agent configs, and initialize agents automatically\n",
    "    survivors = agentscope.init(\n",
    "        model_configs=model_configs,\n",
    "        agent_configs=agent_configs,\n",
    "        project=\"Werewolf\",\n",
    "    )\n",
    "    # update these accordingly based off agent configs\n",
    "    # currently 1 werewolf and 3 villagers\n",
    "    roles = [\"werewolf\", \"villager\", \"seer\", \"witch\"]\n",
    "    wolves, witch, seer = [survivors[0]], survivors[-1], survivors[-2]\n",
    "\n",
    "    # initialize game state\n",
    "    game_state = {\n",
    "        \"werewolves\": [player.name for player in wolves],\n",
    "        \"villagers\": [player.name for player in survivors if player not in wolves],\n",
    "        \"seer\": [seer.name],\n",
    "        \"witch\": [witch.name],\n",
    "        \"survivors\": [player.name for player in survivors],\n",
    "        \"dead\": [],\n",
    "        \"witch_healing_used\": False,\n",
    "        \"witch_poison_used\": False,\n",
    "        \"endgame\": False,\n",
    "        \"winner\": None\n",
    "        \n",
    "    }\n",
    "\n",
    "    for game_num in range(1, num_games + 1):\n",
    "        run_game(\n",
    "            game_num,\n",
    "            max_days_per_game,\n",
    "            raw_log_filepath,\n",
    "            win_rate_filepath,\n",
    "            reflect_before_vectorstore,\n",
    "            max_werewolf_discussion_round,\n",
    "            wolves,\n",
    "            seer,\n",
    "            witch,\n",
    "            roles,\n",
    "            survivors,\n",
    "            game_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:22:57 | INFO     | agentscope.manager._model:load_model_configs:115 - Load configs for model wrapper: gpt-4o-mini, gpt-4o\n",
      "2025-02-10 23:22:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-10 23:22:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-10 23:22:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o]\n",
      "2025-02-10 23:22:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o: 'Model [gpt-4o] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-10 23:22:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-10 23:22:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-10 23:22:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-10 23:22:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "2025-02-10 23:22:57 | INFO     | agentscope.models.model:__init__:203 - Initialize model by configuration [gpt-4o-mini]\n",
      "2025-02-10 23:22:57 | WARNING  | agentscope.models.openai_model:__init__:117 - fail to get max_length for gpt-4o-mini: 'Model [gpt-4o-mini] not found in OPENAI_MAX_LENGTH. The last updated date is 20231212'\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be careful not to reveal my identity as a werewolf. I should support the villagers' suspicions without drawing attention to myself. I can subtly deflect any accusations towards me and sow doubt about the other players.\",\"speak\": \"I think we should consider Player3's behavior last night. They seemed a bit too eager to point fingers at Player2. That could be a sign of a werewolf trying to divert attention. What do you all think?\",\"finish_discussion\": \"false\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I need to blend in and act like a villager while steering suspicion away from myself and Player4, who I suspect might be my werewolf partner. Player2 seems to be leading the discussion, and I need to counter any accusations against me. I think I will vote for Player3, as they seem a bit too quiet and could be a potential threat if they are the Seer.\", \"vote\": \"Player3\", \"uncertainty_score\": 0.3}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player3.\n",
      "\u001b[93mModerator\u001b[0m: Player4, you're the witch. Tonight Player3 is eliminated. Would you like to resurrect Player3?\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 might have valuable information or could be a villager. Resurrecting them could help us gather more insights during the day phase. However, if Player3 is a werewolf, it might not be a good idea to bring them back.\", \"speak\": \"I will resurrect Player3 because I believe they could provide crucial information for our team.\", \"resurrect\": true}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The witch has chosen to resurrect the player.\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player2, Player3 and Player4 would you like to check tonight?\n",
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information to help the villagers. I should check one of the players to see if they are a werewolf. Given the current discussions, I suspect Player1 might be a werewolf, so I'll check them tonight.\",\"speak\": \"Player1\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n",
      "\u001b[93mModerator\u001b[0m: The day is coming, all the players open your eyes. Last night is peaceful, no player is eliminated.\n",
      "\u001b[93mModerator\u001b[0m: Now the alive players are Player1, Player2, Player3 and Player4. Given the game rules and your role, based on the situation and the information you gain, to vote a player eliminated among alive players and to win the game, what do you want to say to others? You can decide whether to reveal your role.\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I think we made a good choice by voting for Player3. Their behavior was quite suspicious, and eliminating them could give us more clarity about the remaining players. I need to stay under the radar and keep an eye on Player2 and Player4, just in case they start acting suspiciously too.\", \"speak\": \"I believe our decision to vote out Player3 was the right move. Their actions raised a lot of red flags. Let's stay vigilant and discuss who we should keep an eye on next.\"}\n",
      "```\n",
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I agree with Player1 that Player3 seemed suspicious. However, we need to be careful about who we trust moving forward. Player4 could also be a potential werewolf, so we should keep an eye on them.\",\"speak\": \"I think you made a valid point, Player1. Player3 did seem off. But we should also consider Player4's behavior. They haven't said much and could be hiding something.\"}\n",
      "```\n",
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"Player2 is pointing fingers at me, but I need to be cautious about revealing my role. I should focus on gathering more information about Player4 and see if they act suspiciously.\", \"speak\": \"I understand your concerns, Player2. However, I think we should keep an eye on Player4 as well. Their silence could be a sign of something. Let's not jump to conclusions too quickly.\"}\n",
      "```\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"Player3 is trying to divert attention towards me, which could be a tactic to protect themselves or their teammate. I need to be cautious and think about how to respond without revealing my role as the witch.\", \"speak\": \"I see your point, Player3, but I think we should be careful about making assumptions. Everyone has their own way of playing, and silence doesn't necessarily mean guilt. Let's focus on gathering more information before we make any decisions.\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Given the game rules and your role, based on the situation and the information you gain, to win the game, it's time to vote one player eliminated among the alive players. Which player do you vote to kill?\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I believe Player3 was acting suspiciously and could be a werewolf, so eliminating them seems like the right choice. I need to maintain the trust of the other villagers while ensuring we get rid of the threat.\", \"vote\": \"Player3\", \"uncertainty_score\": 0.2}\n",
      "```\n",
      "\u001b[92mPlayer2\u001b[0m: ```json\n",
      "{\"thought\": \"I think Player3 is acting suspiciously and hasn't contributed much to the discussion. We need to eliminate someone who might be a werewolf, and Player3 seems like a good candidate based on their behavior.\", \"vote\": \"Player3\", \"uncertainty_score\": \"0.2\"}\n",
      "```\n",
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather more information before making a decision. I suspect Player2 might be a werewolf based on their behavior during discussions, but I want to be cautious. I might reveal my role as Seer to gain trust and encourage others to share their thoughts.\", \"vote\": \"Player2\", \"uncertainty_score\": \"0.3\"}\n",
      "```\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be cautious about my vote since I am the witch. I suspect Player2 might be a werewolf based on their behavior during the discussion, but I have no concrete evidence. I should also consider how the others are viewing Player2.\", \"vote\": \"Player2\", \"uncertainty_score\": \"0.4\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Player2 has been voted out.\n",
      "\u001b[93mModerator\u001b[0m: The game goes on.\n",
      "\u001b[93mModerator\u001b[0m: Player1, if you are the only werewolf, eliminate a player. Otherwise, discuss with your teammates and reach an agreement.\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I feel good about eliminating Player3 since their behavior was suspicious. It could help us narrow down the werewolves. Now, we need to keep an eye on Player4 since they were defensive and Player2 seemed to be pointing fingers at Player4 as well.\",\"speak\": \"I think eliminating Player3 was the right choice. They were acting suspiciously, and we can't afford to have werewolves among us. Let's keep an eye on Player4, though, as they seemed a bit too defensive for my liking.\",\"finish_discussion\": \"true\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Which player do you vote to kill?\n",
      "\u001b[90mPlayer1\u001b[0m: ```json\n",
      "{\"thought\": \"I believe I made the right choice by voting for Player3, as their behavior seemed suspicious. However, Player4's silence also raises some concerns. I need to stay vigilant and keep an eye on both of them.\", \"vote\": \"Player4\", \"uncertainty_score\": 0.3}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: The player with the most votes is Player4.\n",
      "\u001b[93mModerator\u001b[0m: Would you like to eliminate one player? If yes, specify the player_name.\n",
      "\u001b[93mPlayer4\u001b[0m: ```json\n",
      "{\"thought\": \"I need to be strategic about this. If I eliminate a villager, it could backfire, but if I eliminate a werewolf, it could turn the tide in our favor. I need to assess who seems suspicious.\", \"speak\": \"I think we should consider who has been acting oddly. Player1 has been quiet, and Player3 has been making some questionable accusations. What do you all think?\", \"eliminate\": false}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Player3, you're the seer. Which player in Player1, Player3 and Player4 would you like to check tonight?\n",
      "\u001b[92mPlayer3\u001b[0m: ```json\n",
      "{\"thought\": \"I need to gather information about the players to identify the werewolves. I suspect Player1 might be a werewolf based on their behavior during discussions.\", \"speak\": \"Player1\"}\n",
      "```\n",
      "\u001b[93mModerator\u001b[0m: Okay, the role of Player1 is a werewolf.\n",
      "2025-02-10 23:23:53 | INFO     | __main__:check_winning:37 - [Moderator] The werewolves have prevailed and taken over the village. Better luck next time!\n",
      "Game log saved to slow_fast_logs_reflection/slow_fast_raw_1_2025-02-10T23-23-53-05-00.log\n",
      "Row added to slow_fast_win_rate_reflection.txt: {'raw_log_filepath': 'slow_fast_logs_reflection/slow_fast_raw_1_2025-02-10T23-23-53-05-00.log', 'custom_agent_won': True}\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    open_ai_key=openai_key,\n",
    "    raw_log_filepath=\"slow_fast_logs_no_reflection/slow_fast_raw\",\n",
    "    reflect_before_vectorstore=False,\n",
    "    win_rate_filepath=\"slow_fast_win_rate_no_reflection.txt\",\n",
    "    max_days_per_game=2,\n",
    "    num_games=1,\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
