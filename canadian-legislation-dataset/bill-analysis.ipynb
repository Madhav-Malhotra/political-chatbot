{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political Chatbot Analysis: Canadian Bill Importance Evaluation\n",
    "\n",
    "This notebook performs an analysis of Canadian bills using a **text-based** approach. The goal is to evaluate the importance of each bill using **LLMs** and divide the text into smaller chunks to assess their significance in deciding whether to pass the bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs==24.3.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (24.3.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: certifi==2024.12.14 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2024.12.14)\n",
      "Requirement already satisfied: h11==0.14.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: idna==3.10 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: outcome==1.3.0.post0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.3.0.post0)\n",
      "Requirement already satisfied: PySocks==1.7.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (1.7.1)\n",
      "Requirement already satisfied: selenium==4.28.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.28.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: soupsieve==2.6 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (2.6)\n",
      "Requirement already satisfied: trio==0.28.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket==0.11.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.3.0)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.8.0)\n",
      "Requirement already satisfied: wsproto==1.2.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import EnumOutputParser\n",
    "from enum import Enum\n",
    "from typing import Optional, Union, Sequence, Any, List, Dict\n",
    "\n",
    "# load the .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "PASSED_BILL_ID = 'C-242'\n",
    "REJECTED_BILL_ID = 'C-230'\n",
    "\n",
    "# load the bill data\n",
    "with open(\"detailed_bills_with_full_text.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    bills_data = json.load(file)\n",
    "\n",
    "passed_bill_text = [bill for bill in bills_data if bill['id'] == PASSED_BILL_ID][0]['full_text']\n",
    "rejected_bill_text = [bill for bill in bills_data if bill['id'] == REJECTED_BILL_ID][0]['full_text']\n",
    "\n",
    "# split bill into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=200,  \n",
    "    length_function=len,  \n",
    "    is_separator_regex=False,  \n",
    ")\n",
    "\n",
    "passed_bill_text_chunks = text_splitter.split_text(passed_bill_text)\n",
    "rejected_bill_text_chunks = text_splitter.split_text(rejected_bill_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSV Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging utilities\n",
    "class StructuredLogger:\n",
    "    \"\"\"\n",
    "    A logger that outputs to TSV files for structured logging of LLM interactions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define all possible message types for documentation\n",
    "    MESSAGE_TYPES = [\n",
    "        \"vectorstore_add\",           # Adding a case to vectorstore\n",
    "        \"vectorstore_retrieve\",      # Retrieving similar cases\n",
    "        \"vectorstore_summarize\",     # Summarizing cached decisions\n",
    "        \"fast_prompt\",               # Input prompt for fast LLM\n",
    "        \"fast_response\",             # Fast LLM respnose\n",
    "        \"slow_prompt\",               # Input prompt for slow LLM\n",
    "        \"slow_response\",             # Slow LLM response\n",
    "        \n",
    "        # Legislation-specific messages\n",
    "        \"game_init\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, tsv_filepath: str):\n",
    "        \"\"\"\n",
    "        Initialize the logger with a base filepath.\n",
    "\n",
    "        Args:\n",
    "            tsv_filepath: Filepath for TSV log files (includes timestamp)\n",
    "        \"\"\"\n",
    "        self.tsv_filepath = tsv_filepath\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(self.tsv_filepath), exist_ok=True)\n",
    "\n",
    "        # Initialize TSV file with headers if it doesn't exist\n",
    "        if not os.path.exists(self.tsv_filepath):\n",
    "            with open(self.tsv_filepath, \"w\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "                writer = csv.writer(tsvfile, delimiter=\"\\t\")\n",
    "                writer.writerow([\"timestamp\", \"message_type\", \"message\"])\n",
    "\n",
    "    def log(self, message_type: str, message_data: Union[str, Dict[str, Any], list[Any], BaseModel]) -> None:\n",
    "        \"\"\"\n",
    "        Log a message to the TSV file.\n",
    "\n",
    "        Args:\n",
    "            message_type: Type of message (one of MESSAGE_TYPES)\n",
    "            message_data: Data to be logged (will be converted to JSON if not already a string)\n",
    "        \"\"\"\n",
    "        if message_type not in self.MESSAGE_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"Invalid message_type: {message_type}. Must be one of {self.MESSAGE_TYPES}\"\n",
    "            )\n",
    "\n",
    "        # Convert message_data to string if it's not already\n",
    "        if isinstance(message_data, BaseModel):\n",
    "            message = json.dumps(message_data.model_dump(), ensure_ascii=False)\n",
    "        elif isinstance(message_data, (dict, list)):\n",
    "            message = json.dumps(message_data, ensure_ascii=False)\n",
    "        else:\n",
    "            message = str(message_data)\n",
    "\n",
    "        timestamp = datetime.now().isoformat()\n",
    "\n",
    "        # Append to TSV file\n",
    "        with open(self.tsv_filepath, \"a\", newline=\"\", encoding=\"utf-8\") as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter=\"\\t\", quotechar='`', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow([timestamp, message_type, message])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveVectorstoreMemory:\n",
    "    \"\"\"Reflective Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        # Vector Store\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.analyses = []  # To store actual analyses (content)\n",
    "        self.summaries = []  # To store summaries\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        \n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_analysis(self, analysis: str):\n",
    "        \"\"\"Add an analysis to the FAISS index.\"\"\"\n",
    "        self.analyses.append(analysis)\n",
    "        \n",
    "    def summarize_cycle(self, slow_mind_model_name: str, logger: StructuredLogger):\n",
    "        \"\"\"\n",
    "        Generate a summary of the conversation after a day/night cycle,\n",
    "        and add it to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            survivors (list): List of current alive players.\n",
    "            secondary_model (str): Name of secondary model.\n",
    "            cycle_type (str): The type of cycle (\"day\" or \"night\").\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "\n",
    "        # Combine all analyses since the last summary\n",
    "        past_analyses = \"\\n\".join(self.analyses)\n",
    "    \n",
    "        response = client.chat.completions.create(\n",
    "            model=slow_mind_model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\\n\".join([ \n",
    "                    f\"You are a strategic policymaker, evaluating in-depth analyses of bill sections.\",\n",
    "                    f\"Summarize the decisions and reflect on the most important implications for future strategic decisions.\"\n",
    "                ])},\n",
    "                {\"role\": \"user\", \"content\": past_analyses},\n",
    "            ],\n",
    "            max_tokens=4096\n",
    "        )\n",
    "    \n",
    "        summary = response.choices[0].message.content\n",
    "\n",
    "        # Embed the summary and add it to the vector store\n",
    "        embedding = self._get_embedding(summary)\n",
    "        self.index.add(np.array([embedding]))\n",
    "        self.summaries.append(summary)\n",
    "\n",
    "        # add summarized vectorstore entry to tsv logger\n",
    "        logger.log(\"vectorstore_summarize\", {\n",
    "            \"input_decisions\": past_analyses, \n",
    "            \"summary\": summary\n",
    "        })\n",
    "        \n",
    "        # Clear analyses for the next cycle\n",
    "        self.analyses.clear()\n",
    "\n",
    "    def get_relevant_summaries(self, query: str, top_k: int = 2) -> str:\n",
    "        \"\"\"Retrieve the top-k most relevant summaries.\n",
    "        \n",
    "        Args:\n",
    "            query (str): query to find similar summaries to.\n",
    "            top_k (int): number of relevant summaries to retrieve.\n",
    "        \"\"\"\n",
    "        if not self.summaries:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "        results = [\n",
    "            self.summaries[idx] for idx in indices[0] if idx < len(self.summaries)\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "class VectorstoreMemory:\n",
    "    \"\"\"Vectorstore-based memory using FAISS and OpenAI embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Initialize the vectorstore with FAISS and OpenAI embeddings.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): The OpenAI embedding model to use.\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = faiss.IndexFlatL2(1536)  # 1536 is the dimensionality of 'text-embedding-ada-002'\n",
    "        self.analyses = []\n",
    "\n",
    "    def _get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate an embedding for the given text using OpenAI.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to embed.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The embedding vector as a NumPy array.\n",
    "        \"\"\"\n",
    "        client = openai.OpenAI()\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=self.embedding_model\n",
    "        )\n",
    "        return np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "\n",
    "    def add_analysis(self, analysis: str):\n",
    "        \"\"\"Add an analysis to the FAISS index.\"\"\"\n",
    "        embeddings = [self._get_embedding(analysis)]\n",
    "        self.index.add(np.array(embeddings)) \n",
    "        self.analyses.extend(analysis)\n",
    "\n",
    "    def get_relevant_analyses(self, query: str, top_k: int = 2) -> list:\n",
    "        \"\"\"Retrieve the top-k most relevant analyses.\"\"\"\n",
    "        if not self.analyses:\n",
    "            return []\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        distances, indices = self.index.search(np.array([query_embedding]), top_k)\n",
    "\n",
    "        results = [\n",
    "            self.analyses[idx] for idx in indices[0] if idx < len(self.analyses)\n",
    "        ]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_chunk_prompt_template = \"\"\"\n",
    "You are a legislative assistant analyzing sections of a bill to determine its impact and importance in the overall decision to support or oppose the bill.\n",
    "\n",
    "### **Task**\n",
    "Analyze the following section of the bill and:\n",
    "1. Summarize the key points in **neutral terms**.\n",
    "2. Assign an **importance score (0-100%)** to reflect how crucial this section is in making a decision on the bill.\n",
    "    - **0%**: No impact on decision-making.\n",
    "    - **100%**: Crucial in determining the billâ€™s outcome.\n",
    "3. Assign a score between 0-1 on how uncertain you are about your response, 0 if your completely certain, 1 if your completely uncertain.\n",
    "\n",
    "### **Bill Section:**\n",
    "{text}\n",
    "\n",
    "Relevant past analyses:\n",
    "{relevant_analyses}\n",
    "\n",
    "Format Instructions:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "final_decision_prompt_template = \"\"\"\n",
    "You are reviewing the key analyses from important sections of a bill. Your task is to make a final decision on whether the bill should be **approved or rejected**.\n",
    "\n",
    "### **Input Data**\n",
    "Here are the key analyses from important sections of the bill:\n",
    "{important_analyses}\n",
    "\n",
    "### **Task**\n",
    "1. Summarize the major **arguments for** and **against** the bill.\n",
    "2. Justify your decision to **pass** or **reject** the bill using **evidence from the provided analyses**.\n",
    "3. Based on the analyses, provide a **final decision**: Should the bill be **passed** or **rejected**?\n",
    "4. Assign a score between 0-1 on how uncertain you are about your response, 0 if your completely certain, 1 if your completely uncertain.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BillChunkAnalysis(BaseModel):\n",
    "    summary: str = Field(..., description=\"Neutral summary of the bill section.\")\n",
    "    importance_score: float = Field(..., ge=0, le=100, description=\"Quantitative importance score (0-100%) to determine how crucial this section is for making a decision on the bill.\")\n",
    "    uncertainty_score: float = Field(..., ge=0, le=1, description=\"Score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain.\")\n",
    "\n",
    "class FinalBillDecision(BaseModel):\n",
    "    arguments_for: str = Field(..., description=\"Summary of key arguments supporting the bill.\")\n",
    "    arguments_against: str = Field(..., description=\"Summary of key arguments against the bill.\")\n",
    "    justification: str = Field(..., description=\"Rationale for the final decision based on the provided analyses.\")\n",
    "    final_decision: str = Field(..., pattern=\"^(pass|reject)$\", description=\"Final decision on whether to pass or reject the bill.\")\n",
    "    uncertainty_score: float = Field(..., ge=0, le=1, description=\"Score between 0-1 on how uncertain you are about your vote, 0 if your completely certain, 1 if your completely uncertain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegislativeAgent():\n",
    "    def __init__(\n",
    "        self,\n",
    "        reflect_before_vectorstore: bool,\n",
    "        fast_mind_model: ChatOpenAI,\n",
    "        slow_mind_model: ChatOpenAI,\n",
    "        parser: BillChunkAnalysis,\n",
    "        logger: StructuredLogger\n",
    "    ):\n",
    "        self.reflect_before_vectorstore = reflect_before_vectorstore\n",
    "        self.fast_mind_model = fast_mind_model\n",
    "        self.slow_mind_model = slow_mind_model\n",
    "        self.parser = parser\n",
    "        self.logger = logger\n",
    "        if self.reflect_before_vectorstore:\n",
    "            self.memory = ReflectiveVectorstoreMemory()\n",
    "        else:\n",
    "            self.memory = VectorstoreMemory()\n",
    "\n",
    "    def analyze_bill_chunk(self, bill_chunk):\n",
    "        prompt = PromptTemplate(input_variables=[\"text\"], template=bill_chunk_prompt_template, partial_variables={\"format_instructions\": self.parser.get_format_instructions()})\n",
    "\n",
    "        if self.reflect_before_vectorstore:\n",
    "            relevant_analyses = self.memory.get_relevant_summaries(bill_chunk)\n",
    "        else: \n",
    "            relevant_analyses = self.memory.get_relevant_analyses(bill_chunk)\n",
    "        formatted_prompt = prompt.format(text=bill_chunk, relevant_analyses='\\n'.join(relevant_analyses))\n",
    "\n",
    "        self.logger.log('vectorstore_retrieve', '\\n'.join(relevant_analyses))\n",
    "        self.logger.log('fast_prompt', formatted_prompt)\n",
    "\n",
    "        response = self.fast_mind_model.predict(formatted_prompt)\n",
    "\n",
    "        # Parse the output using our parser\n",
    "        parsed_response = self.parser.parse(response)\n",
    "\n",
    "        self.logger.log('fast_response', parsed_response)\n",
    "\n",
    "        summary = parsed_response.summary\n",
    "        importance_score = parsed_response.importance_score\n",
    "        uncertainty_score = parsed_response.uncertainty_score\n",
    "\n",
    "        if uncertainty_score >= 0.2:\n",
    "            self.logger.log('slow_prompt', formatted_prompt)\n",
    "\n",
    "            response = self.slow_mind_model.predict(formatted_prompt)\n",
    "            parsed_response = self.parser.parse(response)\n",
    "\n",
    "            self.logger.log('slow_response', parsed_response)\n",
    "\n",
    "            summary = parsed_response.summary\n",
    "            importance_score = parsed_response.importance_score\n",
    "            uncertainty_score = parsed_response.uncertainty_score\n",
    "\n",
    "        if importance_score >= 80:\n",
    "            self.memory.add_analysis(summary)\n",
    "            self.logger.log('vectorstore_add', summary)\n",
    "\n",
    "    def summarize_cycle(self, slow_mind_model_name: str, logger: StructuredLogger):\n",
    "        self.memory.summarize_cycle(slow_mind_model_name, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    reflect_before_vectorstore: bool,\n",
    "    basefilepath: str,\n",
    "    bill_name: str,\n",
    "    bill_text_chunks: list\n",
    "):\n",
    "    print(\"length of bill text chunks is \" + str(len(bill_text_chunks)))\n",
    "\n",
    "    log_filepath = f'{basefilepath}/{'reflect' if reflect_before_vectorstore else 'noreflect'}/political_raw_{bill_name}.tsv'\n",
    "    logger = StructuredLogger(log_filepath)\n",
    "\n",
    "    fast_mind_model = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key, temperature=1, max_tokens=4096)\n",
    "    slow_mind_model = ChatOpenAI(model='gpt-4o', openai_api_key=openai_api_key, temperature=1, max_tokens=4096)\n",
    "\n",
    "    bill_chunk_parser = PydanticOutputParser(pydantic_object=BillChunkAnalysis)\n",
    "    agent = LegislativeAgent(reflect_before_vectorstore, fast_mind_model, slow_mind_model, bill_chunk_parser, logger)\n",
    "\n",
    "    for idx, bill_text_chunk in enumerate(bill_text_chunks):\n",
    "        agent.analyze_bill_chunk(bill_text_chunk)\n",
    "\n",
    "        if idx % 5 == 0 and idx > 0 and reflect_before_vectorstore:\n",
    "            agent.summarize_cycle('gpt-4o', logger)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of bill text chunks is 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[144]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogs/fastslow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPASSED_BILL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassed_bill_text_chunks\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(reflect_before_vectorstore, basefilepath, bill_name, bill_text_chunks)\u001b[39m\n\u001b[32m     16\u001b[39m agent = LegislativeAgent(reflect_before_vectorstore, fast_mind_model, slow_mind_model, bill_chunk_parser, logger)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, bill_text_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bill_text_chunks):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_bill_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbill_text_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m idx % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m reflect_before_vectorstore:\n\u001b[32m     22\u001b[39m         agent.summarize_cycle(\u001b[33m'\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m'\u001b[39m, logger)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mLegislativeAgent.analyze_bill_chunk\u001b[39m\u001b[34m(self, bill_chunk)\u001b[39m\n\u001b[32m     21\u001b[39m prompt = PromptTemplate(input_variables=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], template=bill_chunk_prompt_template, partial_variables={\u001b[33m\"\u001b[39m\u001b[33mformat_instructions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.parser.get_format_instructions()})\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reflect_before_vectorstore:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     relevant_analyses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_relevant_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbill_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[32m     26\u001b[39m     relevant_analyses = \u001b[38;5;28mself\u001b[39m.memory.get_relevant_analyses(bill_chunk)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mReflectiveVectorstoreMemory.get_relevant_summaries\u001b[39m\u001b[34m(self, query, top_k)\u001b[39m\n\u001b[32m     93\u001b[39m query_embedding = \u001b[38;5;28mself\u001b[39m._get_embedding(query)\n\u001b[32m     94\u001b[39m distances, indices = \u001b[38;5;28mself\u001b[39m.index.search(np.array([query_embedding]), top_k)\n\u001b[32m     95\u001b[39m results = [\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msummaries\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.summaries)\n\u001b[32m     97\u001b[39m ]\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    True,\n",
    "    'logs/fastslow',\n",
    "    PASSED_BILL_ID,\n",
    "    passed_bill_text_chunks \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
